{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project: Query-Driven Retrieval-Augmented Graph Exploration Tool\n",
    "By Karl Simon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the dataset into PyG (PyTorch Geometric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes and Properties:\n",
      "\n",
      "Node Type: Dataset\n",
      "Number of Nodes: 6390\n",
      "  - temporalExtentStart: 6375 items (non-numeric)\n",
      "  - seCorner: 5330 items (non-numeric)\n",
      "  - cmrId: 6390 items (non-numeric)\n",
      "  - globalId: 6390 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([6390, 512])\n",
      "  - abstract: 6390 items (non-numeric)\n",
      "  - daac: 6131 items (non-numeric)\n",
      "  - nwCorner: 5330 items (non-numeric)\n",
      "  - temporalFrequency: 6390 items (non-numeric)\n",
      "  - pagerank_global: torch.Size([6390, 1])\n",
      "  - temporalExtentEnd: 3765 items (non-numeric)\n",
      "  - shortName: 6390 items (non-numeric)\n",
      "  - landingPageUrl: 3037 items (non-numeric)\n",
      "  - doi: 6390 items (non-numeric)\n",
      "  - longName: 6390 items (non-numeric)\n",
      "\n",
      "Node Type: DataCenter\n",
      "Number of Nodes: 184\n",
      "  - pagerank_global: torch.Size([184, 1])\n",
      "  - globalId: 184 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([184, 512])\n",
      "  - shortName: 184 items (non-numeric)\n",
      "  - url: 184 items (non-numeric)\n",
      "  - longName: 184 items (non-numeric)\n",
      "\n",
      "Node Type: Project\n",
      "Number of Nodes: 333\n",
      "  - pagerank_global: torch.Size([333, 1])\n",
      "  - globalId: 333 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([333, 512])\n",
      "  - shortName: 333 items (non-numeric)\n",
      "  - longName: 333 items (non-numeric)\n",
      "\n",
      "Node Type: Platform\n",
      "Number of Nodes: 442\n",
      "  - Type: 442 items (non-numeric)\n",
      "  - pagerank_global: torch.Size([442, 1])\n",
      "  - globalId: 442 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([442, 512])\n",
      "  - shortName: 442 items (non-numeric)\n",
      "  - longName: 442 items (non-numeric)\n",
      "\n",
      "Node Type: Instrument\n",
      "Number of Nodes: 867\n",
      "  - pagerank_global: torch.Size([867, 1])\n",
      "  - globalId: 867 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([867, 512])\n",
      "  - shortName: 867 items (non-numeric)\n",
      "  - longName: 867 items (non-numeric)\n",
      "\n",
      "Node Type: ScienceKeyword\n",
      "Number of Nodes: 1609\n",
      "  - pagerank_global: torch.Size([1609, 1])\n",
      "  - globalId: 1609 items (non-numeric)\n",
      "  - name: 1609 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([1609, 512])\n",
      "  - pagerank_publication_dataset: torch.Size([1609, 1])\n",
      "\n",
      "Node Type: Publication\n",
      "Number of Nodes: 125939\n",
      "  - year: 125939 items (non-numeric)\n",
      "  - pagerank_global: torch.Size([125939, 1])\n",
      "  - globalId: 125939 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([125939, 512])\n",
      "  - title: 125937 items (non-numeric)\n",
      "  - DOI: 125939 items (non-numeric)\n",
      "  - abstract: 99859 items (non-numeric)\n",
      "  - authors: 109975 items (non-numeric)\n",
      "\n",
      "Edges and Types:\n",
      "Edge Type: ('DataCenter', 'HAS_DATASET', 'Dataset') - Number of Edges: 9017 - Shape: torch.Size([2, 9017])\n",
      "Edge Type: ('Dataset', 'OF_PROJECT', 'Project') - Number of Edges: 6049 - Shape: torch.Size([2, 6049])\n",
      "Edge Type: ('Dataset', 'HAS_PLATFORM', 'Platform') - Number of Edges: 9884 - Shape: torch.Size([2, 9884])\n",
      "Edge Type: ('Platform', 'HAS_INSTRUMENT', 'Instrument') - Number of Edges: 2469 - Shape: torch.Size([2, 2469])\n",
      "Edge Type: ('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword') - Number of Edges: 1823 - Shape: torch.Size([2, 1823])\n",
      "Edge Type: ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword') - Number of Edges: 20436 - Shape: torch.Size([2, 20436])\n",
      "Edge Type: ('Publication', 'CITES', 'Publication') - Number of Edges: 208670 - Shape: torch.Size([2, 208670])\n",
      "Edge Type: ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword') - Number of Edges: 89039 - Shape: torch.Size([2, 89039])\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports for entire notebook\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from torch_geometric.data import HeteroData\n",
    "from collections import defaultdict\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "# from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import random\n",
    "from scholarly import scholarly\n",
    "\n",
    "\n",
    "MAX_VAL_LEN = 1000 # max text length for input to LLM from graph_results for each node\n",
    "\n",
    "\n",
    "# Load JSON data from file\n",
    "file_path = \"/home/karlsimon/CSCI6365/final/graph.json\"\n",
    "graph_data = []\n",
    "\n",
    "# Load data line by line to prevent memory overload\n",
    "with open(file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            graph_data.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON line: {e}\")\n",
    "            continue\n",
    "\n",
    "# Initialize HeteroData object\n",
    "data = HeteroData()\n",
    "\n",
    "# Mapping for node indices per node type\n",
    "node_mappings = defaultdict(dict)\n",
    "\n",
    "# Temporary storage for properties\n",
    "node_properties = defaultdict(lambda: defaultdict(list))\n",
    "edge_indices = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# # Define limits for node subsets based on type\n",
    "# node_limits = {\n",
    "#     'Publication': 1000,\n",
    "#     'Dataset': 500,\n",
    "#     'ScienceKeyword': 300,\n",
    "#     'Instrument': 200,\n",
    "#     'Platform': 150,\n",
    "#     'Project': 100,\n",
    "#     'DataCenter': 50\n",
    "# }\n",
    "\n",
    "# Track the number of nodes added per type\n",
    "node_counts = defaultdict(int)\n",
    "\n",
    "# Process nodes with limits\n",
    "for item in graph_data:\n",
    "    if item['type'] == 'node':\n",
    "        node_type = item['labels'][0]\n",
    "        # if node_counts[node_type] >= node_limits.get(node_type, 50):\n",
    "        #     continue  # Skip nodes once the limit is reached\n",
    "\n",
    "        node_id = item['id']\n",
    "        properties = item['properties']\n",
    "\n",
    "        # Store the node index mapping\n",
    "        node_index = len(node_mappings[node_type])\n",
    "        node_mappings[node_type][node_id] = node_index\n",
    "        node_counts[node_type] += 1\n",
    "\n",
    "        # Store properties temporarily by type\n",
    "        for key, value in properties.items():\n",
    "            if isinstance(value, list) and all(isinstance(v, (int, float)) for v in value):\n",
    "                node_properties[node_type][key].append(torch.tensor(value, dtype=torch.float))\n",
    "            elif isinstance(value, (int, float)):\n",
    "                node_properties[node_type][key].append(torch.tensor([value], dtype=torch.float))\n",
    "            else:\n",
    "                node_properties[node_type][key].append(value)  # non-numeric properties as lists\n",
    "\n",
    "# # Define limits for relationships based on type\n",
    "# relationship_limits = {\n",
    "#     'CITES': 2000,\n",
    "#     'HAS_APPLIED_RESEARCH_AREA': 1000,\n",
    "#     'HAS_SCIENCEKEYWORD': 500,\n",
    "#     'HAS_PLATFORM': 500,\n",
    "#     'HAS_DATASET': 500,\n",
    "#     'OF_PROJECT': 300,\n",
    "#     'HAS_INSTRUMENT': 200\n",
    "# }\n",
    "\n",
    "# Track the number of relationships added per type\n",
    "relationship_counts = defaultdict(int)\n",
    "\n",
    "# Filter relationships to only include sampled nodes\n",
    "for item in graph_data:\n",
    "    if item['type'] == 'relationship':\n",
    "        start_type = item['start']['labels'][0]\n",
    "        end_type = item['end']['labels'][0]\n",
    "        start_id = item['start']['id']\n",
    "        end_id = item['end']['id']\n",
    "        edge_type = item['label']\n",
    "\n",
    "        # # Skip if relationship limit reached\n",
    "        # if relationship_counts[edge_type] >= relationship_limits.get(edge_type, 100):\n",
    "        #     continue\n",
    "\n",
    "        # Check if start and end nodes exist in the sampled nodes\n",
    "        if start_id in node_mappings[start_type] and end_id in node_mappings[end_type]:\n",
    "            start_idx = node_mappings[start_type][start_id]\n",
    "            end_idx = node_mappings[end_type][end_id]\n",
    "\n",
    "            # Append to edge list\n",
    "            edge_indices[(start_type, edge_type, end_type)]['start'].append(start_idx)\n",
    "            edge_indices[(start_type, edge_type, end_type)]['end'].append(end_idx)\n",
    "            relationship_counts[edge_type] += 1\n",
    "\n",
    "# Finalize node properties by batch processing\n",
    "for node_type, properties in node_properties.items():\n",
    "    data[node_type].num_nodes = len(node_mappings[node_type])\n",
    "    for key, values in properties.items():\n",
    "        if isinstance(values[0], torch.Tensor):\n",
    "            data[node_type][key] = torch.stack(values)\n",
    "        else:\n",
    "            data[node_type][key] = values  # Keep non-tensor properties as lists\n",
    "\n",
    "# Finalize edge indices in bulk\n",
    "for (start_type, edge_type, end_type), indices in edge_indices.items():\n",
    "    edge_index = torch.tensor([indices['start'], indices['end']], dtype=torch.long)\n",
    "    data[start_type, edge_type, end_type].edge_index = edge_index\n",
    "\n",
    "# Display statistics for verification\n",
    "print(\"Nodes and Properties:\")\n",
    "for node_type in data.node_types:\n",
    "    print(f\"\\nNode Type: {node_type}\")\n",
    "    print(f\"Number of Nodes: {data[node_type].num_nodes}\")\n",
    "    for key, value in data[node_type].items():\n",
    "        if key != 'num_nodes':\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"  - {key}: {value.shape}\")\n",
    "            else:\n",
    "                print(f\"  - {key}: {len(value)} items (non-numeric)\")\n",
    "\n",
    "print(\"\\nEdges and Types:\")\n",
    "for edge_type in data.edge_types:\n",
    "    edge_index = data[edge_type].edge_index\n",
    "    print(f\"Edge Type: {edge_type} - Number of Edges: {edge_index.size(1)} - Shape: {edge_index.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 : Search Graph for nodes based on user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Steps:\n",
    "# 1. improve graph search and rank results.\n",
    "# 2. improve subgraph exploration.\n",
    "# 3. improve external context retrieval (NASA API).\n",
    "\n",
    "# Functions definitions for keywords, search and display used in next cell\n",
    "def extract_keywords(query):\n",
    "    keywords = re.findall(r'\\b\\w+\\b', query)\n",
    "    return [kw.lower() for kw in keywords]\n",
    "\n",
    "# Updated search_graph function with TF-IDF scoring\n",
    "# TODO: make max_per_type specific to each node type\n",
    "def search_graph(data, keywords, node_types=['Dataset', 'Project', 'ScienceKeyword', 'Instrument', 'Platform', 'Publication'], max_results=50, max_per_type=10):\n",
    "    results = []\n",
    "    texts = []  # Collect text data for TF-IDF processing\n",
    "    metadata = []  # To store corresponding metadata (node type, index, key, value)\n",
    "\n",
    "    # Step 1: Collect all matching nodes and their text data\n",
    "    for node_type in node_types:\n",
    "        for key in data[node_type]:\n",
    "            if key == 'num_nodes':\n",
    "                continue\n",
    "            \n",
    "            values = data[node_type][key]\n",
    "            if isinstance(values, list):\n",
    "                for idx, value in enumerate(values):\n",
    "                    value_str = str(value).lower()\n",
    "                    if any(kw in value_str for kw in keywords):\n",
    "                        texts.append(value_str)\n",
    "                        metadata.append((node_type, idx, key, value))\n",
    "\n",
    "    if not texts:\n",
    "        return []\n",
    "\n",
    "    # Step 2: Compute TF-IDF scores for the collected texts\n",
    "    # NOTE: texts stores the properties of the nodes which contain the keywords\n",
    "    vectorizer = TfidfVectorizer(vocabulary=keywords)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    scores = tfidf_matrix.sum(axis=1).A1  # Sum the TF-IDF scores for each text\n",
    "\n",
    "    # Step 3: Sort the results by TF-IDF score in descending order\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    sorted_results = [metadata[i] for i in sorted_indices]\n",
    "    # with open(\"sorted_results.txt\", \"w\") as file:\n",
    "    #     for result in sorted_results:\n",
    "    #         file.write(f\"{result}\\n\")\n",
    "\n",
    "    # Step 4: Limit the number of results overall and per node type\n",
    "    final_results = []\n",
    "    counts_per_type = {node_type: 0 for node_type in node_types}\n",
    "\n",
    "    for result in sorted_results:\n",
    "        node_type = result[0]\n",
    "        if len(final_results) >= max_results:\n",
    "            break\n",
    "        if counts_per_type[node_type] < max_per_type:\n",
    "            final_results.append(result)\n",
    "            counts_per_type[node_type] += 1\n",
    "\n",
    "    # write the 50 final_results to a file\n",
    "    print(\"Writing 50 final_results to file\")\n",
    "    with open(\"final_results.txt\", \"w\") as file:\n",
    "        for result in final_results:\n",
    "            file.write(f\"{result}\\n\")\n",
    "\n",
    "    return final_results\n",
    "\n",
    "# Updated display_results function to trim long values\n",
    "def display_results(results, max_value_length=MAX_VAL_LEN):\n",
    "    if not results:\n",
    "        print(\"No relevant nodes found.\")\n",
    "        return\n",
    "\n",
    "    with open(\"query_results.txt\", \"w\") as file:\n",
    "        print(f\"\\nFound {len(results)} relevant nodes:\\n\")\n",
    "        for node_type, idx, key, value in results:\n",
    "            value_str = str(value)\n",
    "            if len(value_str) > max_value_length:\n",
    "                value_str = value_str[:max_value_length] + \"...\"\n",
    "            output_line = f\"Node Type: {node_type} | Index: {idx} | Property: {key} | Value: {value_str}\\n\"\n",
    "            file.write(output_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given query, extract keywords, search the graph for relevant nodes, and display the results\n",
    "# NOTE: currently only searches for exact keyword matches in node properties\n",
    "\n",
    "def get_subgraph(data, node_type, node_indices, num_hops=2):\n",
    "    # Find all edge types where the node_type is either the source or target\n",
    "    relevant_edges = [\n",
    "        (src, rel, dst) for (src, rel, dst) in data.edge_types if src == node_type or dst == node_type\n",
    "    ]\n",
    "    \n",
    "    print(\"relevant_edges = \", relevant_edges)\n",
    "\n",
    "    if not relevant_edges:\n",
    "        print(f\"No edges found for node type '{node_type}'\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Combine edge indices from all relevant edge types\n",
    "    combined_edge_index = []\n",
    "    combined_edge_types = []\n",
    "\n",
    "    for edge_type in relevant_edges:\n",
    "        edge_index = data[edge_type].edge_index\n",
    "        combined_edge_index.append(edge_index)\n",
    "        combined_edge_types.append(edge_type)\n",
    "\n",
    "    # Stack all edge indices into a single tensor\n",
    "    combined_edge_index = torch.cat(combined_edge_index, dim=1)\n",
    "\n",
    "    # Extract the subgraph using the combined edge index\n",
    "    subset, edge_index, _, _ = k_hop_subgraph(node_idx=node_indices, num_hops=num_hops, edge_index=combined_edge_index)\n",
    "    return subset, edge_index, combined_edge_types\n",
    "\n",
    "\n",
    "# Explore subgraphs based on the search results.\n",
    "def explore_subgraphs(data, results, num_hops=2):\n",
    "    if not results:\n",
    "        print(\"No nodes to explore for subgraphs.\")\n",
    "        return\n",
    "\n",
    "    # Group the results by node type\n",
    "    nodes_by_type = defaultdict(list)\n",
    "    for node_type, idx, _, _ in results:\n",
    "        nodes_by_type[node_type].append(idx)\n",
    "\n",
    "    # Extract and display subgraphs for each node type\n",
    "    for node_type, indices in nodes_by_type.items():\n",
    "        print(f\"\\nExploring subgraph for node type: {node_type}\")\n",
    "        # print(f\"Number of nodes: {len(indices)}\") #10 nodes\n",
    "        # Get the valid range for node indices\n",
    "        num_nodes = data[node_type].num_nodes\n",
    "        valid_indices = [idx for idx in indices if idx < num_nodes]\n",
    "\n",
    "        if not valid_indices:\n",
    "            print(f\"No valid indices for node type '{node_type}'.\")\n",
    "            continue\n",
    "\n",
    "        node_indices = torch.tensor(valid_indices[:10])  # Limit to 10 nodes (only using 10 per node_type anyways for now)\n",
    "        print(f\"Exploring subgraph for node indices: {node_indices}\") # may not be sequential due to search results ordering \n",
    "        subset, edge_index, edge_type = get_subgraph(data, node_type, node_indices, num_hops=num_hops)\n",
    "\n",
    "        if subset is not None and edge_index is not None:\n",
    "            print(f\"Extracted subgraph with {len(subset)} nodes and {edge_index.size(1)} edges.\")\n",
    "            print(f\"Edge Type: {edge_type}\")\n",
    "        else:\n",
    "            print(f\"Could not extract subgraph for node type: {node_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 : Use APIs Wikipedia for external information based on user query\n",
    "- Question: should API be queries based on keywords or the extracted graph nodes from keywords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get external context from Wikipedia using the REST API\n",
    "def fetch_wikipedia_context(keywords):\n",
    "    search_term = \" \".join(keywords)\n",
    "    \n",
    "    # Step 1: Use the Action API to get the top 5 search results\n",
    "    search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    search_params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": search_term,\n",
    "        \"srlimit\": 5,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"GraphExplorationTool/1.0 (ksimon24@gwu.edu)\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        search_response = requests.get(search_url, params=search_params, headers=headers)\n",
    "        search_response.raise_for_status()\n",
    "        search_data = search_response.json()\n",
    "        \n",
    "        search_results = search_data.get(\"query\", {}).get(\"search\", [])\n",
    "        \n",
    "        if not search_results:\n",
    "            return None\n",
    "\n",
    "        # Step 2: Fetch summaries using the REST API for each search result\n",
    "        context_list = []\n",
    "        for result in search_results:\n",
    "            print(\"result = \", result)\n",
    "            page_title = result.get(\"title\")\n",
    "            rest_url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{page_title.replace(' ', '_')}\"\n",
    "            \n",
    "            rest_response = requests.get(rest_url, headers=headers)\n",
    "            rest_response.raise_for_status()\n",
    "            rest_data = rest_response.json()\n",
    "            \n",
    "            # Extract relevant information\n",
    "            title = rest_data.get(\"title\", \"No Title\")\n",
    "            description = rest_data.get(\"description\", \"No Description Available.\")\n",
    "            summary = rest_data.get(\"extract\", \"No Summary Available.\")\n",
    "            link = rest_data.get(\"content_urls\", {}).get(\"desktop\", {}).get(\"page\", \"No Link Available.\")\n",
    "            thumbnail = rest_data.get(\"thumbnail\", {}).get(\"source\", None)\n",
    "            \n",
    "            context_entry = {\n",
    "                \"title\": title,\n",
    "                \"description\": description,\n",
    "                \"summary\": summary,\n",
    "                \"link\": link,\n",
    "                \"thumbnail\": thumbnail\n",
    "            }\n",
    "            context_list.append(context_entry)\n",
    "        \n",
    "        return context_list\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching Wikipedia context: {e}\")\n",
    "        return None\n",
    "    \n",
    "def display_wikipedia_context(context_list):\n",
    "    if not context_list:\n",
    "        print(\"\\nNo external context available from Wikipedia.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nWikipedia Context:\")\n",
    "    for i, context in enumerate(context_list, start=1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"Title: {context['title']}\")\n",
    "        print(f\"Description: {context['description']}\")\n",
    "        print(f\"Summary: {context['summary']}\")\n",
    "        print(f\"Link: {context['link']}\")\n",
    "        if context['thumbnail']:\n",
    "            print(f\"Thumbnail: {context['thumbnail']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Keywords: ['climate', 'change']\n",
      "Writing 50 final_results to file\n",
      "\n",
      "Found 50 relevant nodes:\n",
      "\n",
      "result =  {'ns': 0, 'title': 'Climate change', 'pageid': 5042951, 'size': 317341, 'wordcount': 27919, 'snippet': 'Present-day <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span> includes both global warming—the ongoing increase in global average temperature—and its wider effects on Earth\\'s <span class=\"searchmatch\">climate</span>. <span class=\"searchmatch\">Climate</span> change', 'timestamp': '2024-12-10T03:05:32Z'}\n",
      "result =  {'ns': 0, 'title': 'Climate change denial', 'pageid': 12474403, 'size': 237127, 'wordcount': 22133, 'snippet': '<span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">change</span> denial (also global warming denial) is a form of science denial characterized by rejecting, refusing to acknowledge, disputing, or fighting', 'timestamp': '2024-12-04T00:54:58Z'}\n",
      "result =  {'ns': 0, 'title': 'Climate change mitigation', 'pageid': 2119179, 'size': 229408, 'wordcount': 22636, 'snippet': '<span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">change</span> mitigation (or decarbonisation) is action to limit the greenhouse gases in the atmosphere that cause <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span>. <span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">change</span> mitigation', 'timestamp': '2024-12-09T04:31:50Z'}\n",
      "result =  {'ns': 0, 'title': 'Effects of climate change', 'pageid': 2119174, 'size': 180605, 'wordcount': 20335, 'snippet': 'Effects of <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span> are well documented and growing for Earth\\'s natural environment and human societies. <span class=\"searchmatch\">Changes</span> to the <span class=\"searchmatch\">climate</span> system include an', 'timestamp': '2024-12-08T12:02:41Z'}\n",
      "result =  {'ns': 0, 'title': 'United Nations Framework Convention on Climate Change', 'pageid': 31898, 'size': 95299, 'wordcount': 9711, 'snippet': 'Framework Convention on <span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">Change</span> (UNFCCC) is the UN process for negotiating an agreement to limit dangerous <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span>. It is an international', 'timestamp': '2024-11-18T10:15:56Z'}\n",
      "\n",
      "Wikipedia Context:\n",
      "\n",
      "Result 1:\n",
      "Title: Climate change\n",
      "Description: Human-caused changes to climate on Earth\n",
      "Summary: Present-day climate change includes both global warming—the ongoing increase in global average temperature—and its wider effects on Earth's climate. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The current rise in global temperatures is driven by human activities, especially fossil fuel burning since the Industrial Revolution. Fossil fuel use, deforestation, and some agricultural and industrial practices release greenhouse gases. These gases absorb some of the heat that the Earth radiates after it warms from sunlight, warming the lower atmosphere. Carbon dioxide, the primary greenhouse gas driving global warming, has grown by about 50% and is at levels not seen for millions of years.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Change_in_Average_Temperature_With_Fahrenheit.svg/320px-Change_in_Average_Temperature_With_Fahrenheit.svg.png\n",
      "\n",
      "Result 2:\n",
      "Title: Climate change denial\n",
      "Description: Denial of the scientific consensus on climate change\n",
      "Summary: Climate change denial is a form of science denial characterized by rejecting, refusing to acknowledge, disputing, or fighting the scientific consensus on climate change. Those promoting denial commonly use rhetorical tactics to give the appearance of a scientific controversy where there is none. Climate change denial includes unreasonable doubts about the extent to which climate change is caused by humans, its effects on nature and human society, and the potential of adaptation to global warming by human actions. To a lesser extent, climate change denial can also be implicit when people accept the science but fail to reconcile it with their belief or action. Several studies have analyzed these positions as forms of denialism, pseudoscience, or propaganda.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change_denial\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Inhofe_holding_snowball.jpg/320px-Inhofe_holding_snowball.jpg\n",
      "\n",
      "Result 3:\n",
      "Title: Climate change mitigation\n",
      "Description: Actions to reduce net greenhouse gas emissions to limit climate change\n",
      "Summary: Climate change mitigation (or decarbonisation) is action to limit the greenhouse gases in the atmosphere that cause climate change. Climate change mitigation actions include conserving energy and replacing fossil fuels with clean energy sources. Secondary mitigation strategies include changes to land use and removing carbon dioxide (CO2) from the atmosphere. Current climate change mitigation policies are insufficient as they would still result in global warming of about 2.7 °C by 2100, significantly above the 2015 Paris Agreement's goal of limiting global warming to below 2 °C.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change_mitigation\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Westmill_Solar_2.jpg/320px-Westmill_Solar_2.jpg\n",
      "\n",
      "Result 4:\n",
      "Title: Effects of climate change\n",
      "Description: No Description Available.\n",
      "Summary: Effects of climate change are well documented and growing for Earth's natural environment and human societies. Changes to the climate system include an overall warming trend, changes to precipitation patterns, and more extreme weather. As the climate changes it impacts the natural environment with effects such as more intense forest fires, thawing permafrost, and desertification. These changes impact ecosystems and societies, and can become irreversible once tipping points are crossed. Climate activists are engaged in a range of activities around the world that seek to ameliorate these issues or prevent them from happening.\n",
      "Link: https://en.wikipedia.org/wiki/Effects_of_climate_change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/062821Yreka_Fire_CalFire_-2wiki.jpg/320px-062821Yreka_Fire_CalFire_-2wiki.jpg\n",
      "\n",
      "Result 5:\n",
      "Title: United Nations Framework Convention on Climate Change\n",
      "Description: International environmental treaty\n",
      "Summary: The United Nations Framework Convention on Climate Change (UNFCCC) is the UN process for negotiating an agreement to limit dangerous climate change. It is an international treaty among countries to combat \"dangerous human interference with the climate system\". The main way to do this is limiting the increase in greenhouse gases in the atmosphere. It was signed in 1992 by 154 states at the United Nations Conference on Environment and Development (UNCED), informally known as the Earth Summit, held in Rio de Janeiro. The treaty entered into force on 21 March 1994. \"UNFCCC\" is also the name of the Secretariat charged with supporting the operation of the convention, with offices on the UN Campus in Bonn, Germany.\n",
      "Link: https://en.wikipedia.org/wiki/United_Nations_Framework_Convention_on_Climate_Change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/UNFCCC_Annex_I_Parties%2C_OECD%2C_EU.svg/320px-UNFCCC_Annex_I_Parties%2C_OECD%2C_EU.svg.png\n",
      "\n",
      "Exploring subgraph for node type: Publication\n",
      "Exploring subgraph for node indices: tensor([76606, 94077, 50763, 55702, 88548, 42689, 82757, 12919, 60848, 35890])\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 18 nodes and 8 edges.\n",
      "Edge Type: [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Dataset\n",
      "Exploring subgraph for node indices: tensor([4329, 2323,   97,   61,   65, 2131, 4180, 3752,  293, 4253])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Extracted subgraph with 2263 nodes and 16366 edges.\n",
      "Edge Type: [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: ScienceKeyword\n",
      "Exploring subgraph for node indices: tensor([ 94, 802, 888, 989, 139,  70,  72,  96, 729, 229])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 374 nodes and 738 edges.\n",
      "Edge Type: [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Project\n",
      "Exploring subgraph for node indices: tensor([109, 117,  26,  59,  89, 105, 108, 121,   0, 137])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Extracted subgraph with 197 nodes and 206 edges.\n",
      "Edge Type: [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "\n",
      "Exploring subgraph for node type: Platform\n",
      "Exploring subgraph for node indices: tensor([165, 166, 173, 178, 344,  68, 248, 307, 344, 409])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Extracted subgraph with 1953 nodes and 4553 edges.\n",
      "Edge Type: [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n"
     ]
    }
   ],
   "source": [
    "# ################ Pre-LLM Steps ################\n",
    "# query = input(\"Enter your query (e.g., 'Find datasets related to climate change projects'): \")\n",
    "query = \"climate change\" #TODO: remove hardcoded query\n",
    "keywords = extract_keywords(query)\n",
    "print(f\"\\nExtracted Keywords: {keywords}\")\n",
    "\n",
    "# Search the graph with TF-IDF ranking\n",
    "graph_results = search_graph(data, keywords)\n",
    "display_results(graph_results) # 50 results\n",
    "\n",
    "# Fetch Wikipedia context\n",
    "wikipedia_context = fetch_wikipedia_context(keywords)\n",
    "display_wikipedia_context(wikipedia_context)\n",
    "\n",
    "# Explore subgraphs based on the results\n",
    "# TODO: save these subgraphs\n",
    "explore_subgraphs(data, graph_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin the RAG Pipeline with LLM Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Gemini API ############\n",
    "# Define the path to the text file containing the API key\n",
    "file_path = \"/home/karlsimon/CSCI6365/final/gemini_api_key.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    api_key = file.read().strip()\n",
    "# print(api_key)\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Create a model instance (using Gemini 1.5 Flash in this case)\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM-Generated Summary:\n",
      "The provided text consists of abstracts from several research papers and Wikipedia summaries on climate change.  The research papers investigate various aspects of climate change impacts and modeling:\n",
      "\n",
      "* **Impacts on Evapotranspiration and Water Resources:** Studies analyze the spatial and temporal distribution of evapotranspiration (ET) in the Narmada River basin (India), focusing on the effects of land use and climate change on water resources and agriculture.\n",
      "\n",
      "* **Climate Change and Forest Fires:** Research examines the relationship between climate change, terrestrial water storage, and forest fire frequency in Yunnan province, China.\n",
      "\n",
      "* **Climate Change Effects on Marine Ecosystems:** A study investigates the impact of climate change-induced changes in prey quality on the growth and survival of juvenile Chinook salmon.\n",
      "\n",
      "* **Ozone Depletion and Climate Change Interactions:** Research explores the complex interplay between stratospheric ozone recovery (due to the Montreal Protocol), climate change, and UV radiation levels.\n",
      "\n",
      "* **Climate Model Sensitivity:** A paper analyzes the increased equilibrium climate sensitivity in a newer climate model (CNRM-CM6-1), attributing the increase to changes in atmospheric components, particularly cloud radiative responses.\n",
      "\n",
      "* **Climate Change and Human Migration:**  Research focuses on individual decision-making processes regarding migration from climate-vulnerable regions, emphasizing the role of economic and social factors.\n",
      "\n",
      "* **Climate Change Impacts on Groundwater:** A study investigates the effects of climate change on groundwater table fluctuations in the Great Hungarian Plain, using hydrological modeling and climate projections.\n",
      "\n",
      "* **Climate Change Impacts on Vegetation Cycles:** Research uses a land surface model to simulate the impacts of climate change on the biomass and phenology of various vegetation types in France.\n",
      "\n",
      "* **Climate Change and Flood Frequency:** A study assesses the implications of climate change for future bivariate quantiles of flood peak and volume in the Ganjiang River basin, China.\n",
      "\n",
      "* **Agricultural Practices and Climate Change:**  A systematic review examines the evidence for synergistic outcomes (mitigation and adaptation) from various agricultural practices in response to climate change.\n",
      "\n",
      "The Wikipedia entries provide background information on climate change, its denial, mitigation efforts, its various effects, and the UNFCCC's role in international climate agreements.  Overall, the combined information highlights the multifaceted nature of climate change, its diverse impacts across various ecosystems and human societies, and the ongoing research efforts to understand and address this complex challenge.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to summarize combined results using the LLM\n",
    "def summarize_results_with_llm(graph_results, wikipedia_context):\n",
    "    prompt = \"Summarize the following search results and Wikipedia context:\\n\\n\"\n",
    "\n",
    "    # Add graph results to the prompt\n",
    "    prompt += \"Graph Search Results:\\n\"\n",
    "    for node_type, idx, key, value in graph_results[:10]:  # Limit to top 10 results\n",
    "        prompt += f\"- Node Type: {node_type}, Property: {key}, Value: {str(value)[:MAX_VAL_LEN]}...\\n\"\n",
    "\n",
    "    # Add Wikipedia context to the prompt\n",
    "    prompt += \"\\nWikipedia Context:\\n\"\n",
    "    for i, context in enumerate(wikipedia_context, start=1):\n",
    "        prompt += f\"{i}. Title: {context['title']}\\n\"\n",
    "        prompt += f\"   Summary: {context['summary'][:MAX_VAL_LEN]}...\\n\"\n",
    "    \n",
    "    with open(\"prompt_file.txt\", \"w\") as file:\n",
    "        file.write(f\"{prompt}\\n\")\n",
    "\n",
    "    # Call the Gemini model to generate the summary\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Generate an LLM summary of the combined results\n",
    "summary = summarize_results_with_llm(graph_results, wikipedia_context)\n",
    "print(\"\\nLLM-Generated Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Use subgraph for additional information and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploring subgraph for node type: Publication\n",
      "Exploring subgraph for node indices: tensor([76606, 94077, 50763, 55702, 88548, 42689, 82757, 12919, 60848, 35890])\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 18 nodes and 8 edges.\n",
      "Edge Type: [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Dataset\n",
      "Exploring subgraph for node indices: tensor([4329, 2323,   97,   61,   65, 2131, 4180, 3752,  293, 4253])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Extracted subgraph with 2263 nodes and 16366 edges.\n",
      "Edge Type: [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: ScienceKeyword\n",
      "Exploring subgraph for node indices: tensor([ 94, 802, 888, 989, 139,  70,  72,  96, 729, 229])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 374 nodes and 738 edges.\n",
      "Edge Type: [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Project\n",
      "Exploring subgraph for node indices: tensor([109, 117,  26,  59,  89, 105, 108, 121,   0, 137])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Extracted subgraph with 197 nodes and 206 edges.\n",
      "Edge Type: [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "\n",
      "Exploring subgraph for node type: Platform\n",
      "Exploring subgraph for node indices: tensor([165, 166, 173, 178, 344,  68, 248, 307, 344, 409])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Extracted subgraph with 1953 nodes and 4553 edges.\n",
      "Edge Type: [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "subset =  tensor([   6,    8,   46,  478,  482,  486,  629,  716,  739,  871,  881,  886,\n",
      "         897,  924,  936,  939,  993, 1000, 1005, 1032, 1102, 2323]) , edge_index =  tensor([[   6,    8,   46,   46,  629,  716,  739,  871,  881,  886,  897,  924,\n",
      "          936,  939,  993, 1000, 1005, 1032, 1102,    6,  478,  482,  486,  886,\n",
      "          886],\n",
      "        [  46,   46, 2323,    6,   46,   46,   46,   46,   46,   46,   46,   46,\n",
      "           46,   46,   46,   46,   46,   46,   46,    6,   46,   46,   46,    8,\n",
      "            6]]) , edge_types =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')] and node_type =  Dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Explore subgraphs based on the results (NOT LLM dependent, just for output to terminal)\n",
    "explore_subgraphs(data, graph_results)\n",
    "\n",
    "# TODO: change metrics to evaluate the LLM generated explanations\n",
    "# Generate LLM explanations for each explored subgraph\n",
    "\n",
    "# print(\"length of graph_results[:3]: \", len(graph_results)) # len(graph_results) = 50, and len(graph_results[:3]) = 3\n",
    "# print(\"graph_results[:3]: \", graph_results[:3])\n",
    "\n",
    "for node_type, idx, key, value in graph_results[11:12]:  # Limit to 3 nodes for brevity\n",
    "    # Get subgraph for the current node type and indices\n",
    "    subset, edge_index, edge_types = get_subgraph(data, node_type, torch.tensor([idx]))\n",
    "    print(f\"subset = \", subset, \", edge_index = \", edge_index, \", edge_types = \", edge_types, \"and node_type = \", node_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# specify the priorities to use in value selection\n",
    "def get_priority_properties():\n",
    "    priority_properties = {\n",
    "        'Dataset': ['longName', 'abstract', 'shortName'],\n",
    "        'Publication': ['title', 'abstract'],\n",
    "        'ScienceKeyword': ['name'],\n",
    "        'Instrument': ['longName', 'shortName'],\n",
    "        'Platform': ['longName', 'shortName'],\n",
    "        'Project': ['longName', 'shortName'],\n",
    "        'DataCenter': ['longName', 'shortName']\n",
    "    }\n",
    "    return priority_properties\n",
    "\n",
    "def create_nodes_of_interest(graph_results, max_per_type=3):\n",
    "    nodes_by_type = defaultdict(list)\n",
    "    for node_type, idx, key, value in graph_results:\n",
    "        nodes_by_type[node_type].append((idx, node_type, key, value))\n",
    "\n",
    "    # Select up to max_per_type nodes for each type\n",
    "    nodes_of_interest = []\n",
    "    for node_type, nodes in nodes_by_type.items():\n",
    "        nodes_of_interest.extend(random.sample(nodes, min(max_per_type, len(nodes))))\n",
    "\n",
    "    return nodes_of_interest\n",
    "\n",
    "def explore_subgraph_nodes(data, node_type, node_id, num_hops=2, max_per_type=3):\n",
    "    priority_properties = get_priority_properties()\n",
    "    subset, edge_index, edge_types = get_subgraph(data, node_type, torch.tensor([node_id]), num_hops=num_hops)\n",
    "\n",
    "    if subset is None:\n",
    "        return []\n",
    "\n",
    "    # Map node indices to their types and values\n",
    "    subgraph_nodes = []\n",
    "    print(f\"Number of nodes in subgraph for node_id: {node_id} = {len(subset)} | subset = {subset}\")\n",
    "\n",
    "    for sub_id in subset.tolist():\n",
    "        for sub_node_type in data.node_types:\n",
    "            num_nodes = data[sub_node_type].num_nodes\n",
    "            if sub_id < num_nodes:\n",
    "                # Attempt to find a meaningful property\n",
    "                value = None\n",
    "                for prop in priority_properties.get(sub_node_type, []):\n",
    "                    if prop in data[sub_node_type] and len(data[sub_node_type][prop]) > sub_id:\n",
    "                        value = data[sub_node_type][prop][sub_id]\n",
    "                        break\n",
    "                if value is None:  # Fallback to globalId or indicate no value\n",
    "                    value = data[sub_node_type].get('globalId', ['No value'])[sub_id] if 'globalId' in data[sub_node_type] else 'No value'\n",
    "                \n",
    "                subgraph_nodes.append((sub_id, sub_node_type, value))\n",
    "\n",
    "    # Group by node type and select a random subset of up to max_per_type nodes\n",
    "    nodes_by_type = defaultdict(list)\n",
    "    for node_id, node_type, value in subgraph_nodes:\n",
    "        nodes_by_type[node_type].append((node_id, node_type, value))\n",
    "\n",
    "    exploration_list = []\n",
    "    for node_type, nodes in nodes_by_type.items():\n",
    "        exploration_list.extend(random.sample(nodes, min(max_per_type, len(nodes))))\n",
    "\n",
    "    return exploration_list\n",
    "\n",
    "def write_exploration_to_file(data, graph_results, filename=\"graph_exploration.txt\"):\n",
    "    nodes_of_interest = create_nodes_of_interest(graph_results)\n",
    "\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(\"=== Nodes of Interest ===\\n\")\n",
    "        for idx, node_type, key, value in nodes_of_interest:\n",
    "            value_str = str(value)\n",
    "            display_value = value_str[:MAX_VAL_LEN] + (\"...\" if len(value_str) > MAX_VAL_LEN else \"\")\n",
    "            file.write(f\"ID: {idx}, Type: {node_type}, Key: {key}, Value: {display_value}\\n\")\n",
    "\n",
    "        file.write(\"\\n=== Subgraph Exploration ===\\n\")\n",
    "        for idx, node_type, key, value in nodes_of_interest:\n",
    "            file.write(f\"\\nExploring Subgraph for Node ID: {idx} (Type: {node_type})\\n\")\n",
    "            subgraph_nodes = explore_subgraph_nodes(data, node_type, idx)\n",
    "            for sub_id, sub_node_type, sub_value in subgraph_nodes:\n",
    "                sub_value_str = str(sub_value)\n",
    "                display_sub_value = sub_value_str[:MAX_VAL_LEN] + (\"...\" if len(sub_value_str) > MAX_VAL_LEN else \"\")\n",
    "                file.write(f\"  - ID: {sub_id}, Type: {sub_node_type}, Value: {display_sub_value}\\n\")\n",
    "\n",
    "    print(f\"\\nExploration results written to '{filename}'.\")\n",
    "\n",
    "\n",
    "def interactive_exploration(data):\n",
    "    priority_properties = get_priority_properties()\n",
    "    while True:\n",
    "        choice = input(\"\\nEnter a Node ID to explore further (or 'q' to quit): \")\n",
    "        if choice.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            node_id = int(choice)\n",
    "            node_type = input(\"Enter the Node Type (e.g., Dataset, ScienceKeyword, Instrument): \").strip()\n",
    "\n",
    "            # Validate the node type\n",
    "            if node_type not in data.node_types:\n",
    "                print(f\"Invalid node type '{node_type}'. Available types: {data.node_types}\")\n",
    "                continue\n",
    "\n",
    "            num_nodes = data[node_type].num_nodes\n",
    "            if node_id >= num_nodes:\n",
    "                print(f\"No node with ID: {node_id} in type '{node_type}'.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nSelected Node ID: {node_id} (Type: {node_type})\")\n",
    "            action = input(\"Enter 'wiki' to fetch Wikipedia context or 'subgraph' to explore subgraph of node: \").lower()\n",
    "\n",
    "            if action == 'wiki':\n",
    "                # Select a meaningful property using priority_properties\n",
    "                value = None\n",
    "                for prop in priority_properties.get(node_type, []):\n",
    "                    if prop in data[node_type] and len(data[node_type][prop]) > node_id:\n",
    "                        value = data[node_type][prop][node_id]\n",
    "                        break\n",
    "\n",
    "                if value is None:\n",
    "                    value = 'No value'\n",
    "\n",
    "                wikipedia_context = fetch_wikipedia_context([str(value)])\n",
    "                print(\"The prompt used for the Wikipedia context =\", str(value))\n",
    "                display_wikipedia_context(wikipedia_context)\n",
    "\n",
    "            elif action == 'subgraph':\n",
    "                subgraph_nodes = explore_subgraph_nodes(data, node_type, node_id)\n",
    "                print(f\"\\nSubgraph for Node ID: {node_id} (Type: {node_type})\")\n",
    "                for sub_id, sub_node_type, sub_value in subgraph_nodes:\n",
    "                    sub_value_str = str(sub_value)\n",
    "                    display_sub_value = sub_value_str[:MAX_VAL_LEN] + (\"...\" if len(sub_value_str) > MAX_VAL_LEN else \"\")\n",
    "                    print(f\"  - ID: {sub_id}, Type: {sub_node_type}, Value: {display_sub_value}\")\n",
    "\n",
    "            else:\n",
    "                print(\"Invalid action. Please enter 'wiki' or 'subgraph'.\")\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Invalid Node ID. Please enter a valid number.\")\n",
    "\n",
    "def run_exploration_tool(data, graph_results):\n",
    "    # Write initial exploration to file\n",
    "    write_exploration_to_file(data, graph_results)\n",
    "\n",
    "    # Start interactive exploration\n",
    "    interactive_exploration(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 88548 = 1 | subset = tensor([88548])\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 35890 = 1 | subset = tensor([35890])\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 76606 = 1 | subset = tensor([76606])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 2323 = 22 | subset = tensor([   6,    8,   46,  478,  482,  486,  629,  716,  739,  871,  881,  886,\n",
      "         897,  924,  936,  939,  993, 1000, 1005, 1032, 1102, 2323])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 293 = 512 | subset = tensor([   1,    3,    6,    9,   10,   11,   12,   13,   14,   15,   17,   20,\n",
      "          22,   23,   26,   27,   28,   36,   42,   43,   46,   51,   54,   55,\n",
      "          61,   67,   69,   72,   73,   76,   78,   79,   83,   84,   88,   93,\n",
      "          94,   96,   98,  100,  102,  106,  109,  111,  113,  116,  118,  122,\n",
      "         125,  128,  133,  140,  141,  144,  148,  151,  153,  154,  160,  164,\n",
      "         169,  170,  171,  173,  174,  179,  181,  187,  189,  191,  193,  196,\n",
      "         202,  205,  206,  208,  214,  217,  220,  223,  228,  234,  237,  238,\n",
      "         239,  240,  241,  245,  248,  249,  251,  253,  254,  259,  262,  264,\n",
      "         265,  266,  274,  290,  292,  293,  295,  298,  299,  308,  311,  313,\n",
      "         314,  317,  319,  321,  332,  344,  345,  346,  347,  348,  349,  351,\n",
      "         352,  354,  361,  363,  365,  372,  373,  374,  375,  376,  381,  385,\n",
      "         388,  389,  390,  394,  396,  398,  400,  401,  408,  412,  415,  421,\n",
      "         424,  427,  428,  433,  434,  435,  436,  439,  440,  447,  450,  459,\n",
      "         460,  466,  469,  472,  580,  645,  698,  800,  885,  886, 1069, 1141,\n",
      "        1142, 1143, 1145, 1146, 1152, 1153, 1154, 1156, 1157, 1161, 1162, 1163,\n",
      "        1164, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1174, 1177, 1179, 1184,\n",
      "        1185, 1186, 1187, 1188, 1189, 1190, 1193, 1195, 1196, 1197, 1203, 1208,\n",
      "        1233, 1236, 1238, 1239, 1242, 1243, 1246, 1248, 1249, 1250, 1253, 1255,\n",
      "        1256, 1259, 1262, 1266, 1267, 1270, 1271, 1272, 1277, 1278, 1283, 1284,\n",
      "        1287, 1291, 1293, 1296, 1297, 1301, 1306, 1310, 1315, 1316, 1317, 1318,\n",
      "        1319, 1320, 1324, 1325, 1329, 1373, 1640, 1694, 1702, 1711, 1720, 1741,\n",
      "        1744, 1753, 1770, 1792, 1813, 1820, 1827, 1842, 1850, 1865, 1866, 1885,\n",
      "        1889, 1892, 1895, 1917, 1918, 1928, 1929, 1931, 1935, 1959, 1967, 1972,\n",
      "        1992, 2000, 2004, 2021, 2039, 2040, 2054, 2059, 2062, 2063, 2075, 2091,\n",
      "        2106, 2127, 2143, 2148, 2152, 2164, 2189, 2209, 2216, 2231, 2254, 2261,\n",
      "        2277, 2287, 2290, 2291, 2298, 2311, 2314, 2335, 2340, 2348, 2369, 2384,\n",
      "        2396, 2397, 2402, 2406, 2421, 2434, 2446, 2454, 2470, 2474, 2484, 2496,\n",
      "        2503, 2516, 2534, 2535, 2565, 2576, 2578, 2583, 2593, 2598, 2603, 2606,\n",
      "        2610, 2611, 2623, 2643, 2649, 2671, 2682, 2690, 2696, 2715, 2722, 2734,\n",
      "        2737, 2739, 2743, 2766, 2774, 2775, 2784, 2786, 2789, 2791, 2797, 2817,\n",
      "        2827, 2836, 2838, 2840, 2847, 2850, 2878, 2882, 2891, 2919, 2938, 2948,\n",
      "        2966, 2983, 2989, 2993, 2996, 2997, 3003, 3020, 3032, 3044, 3045, 3056,\n",
      "        3058, 3060, 3066, 3072, 3075, 3076, 3078, 3082, 3084, 3087, 3095, 3098,\n",
      "        3100, 3102, 3103, 3104, 3105, 3109, 3112, 3118, 3126, 3133, 3135, 3141,\n",
      "        3145, 3146, 3147, 3149, 3152, 3154, 3157, 3160, 3162, 3163, 3165, 3166,\n",
      "        3167, 3168, 3169, 3172, 3174, 3175, 3179, 3180, 3285, 3294, 3302, 3314,\n",
      "        3320, 3346, 3364, 3398, 3411, 3434, 3437, 3461, 3466, 3475, 3479, 3480,\n",
      "        3497, 3509, 3516, 3530, 3545, 3556, 3557, 3571, 3581, 3657, 3669, 3681,\n",
      "        3774, 3838, 3921, 3944, 3957, 3965, 4042, 4057, 4110, 4140, 4156, 4270,\n",
      "        4282, 4324, 4473, 4526, 4849, 5065, 5068, 5070, 5075, 5076, 5155, 5176,\n",
      "        5357, 5369, 5408, 5464, 5555, 5663, 5697, 5738, 5794, 5807, 6094, 6097,\n",
      "        6100, 6106, 6135, 6144, 6158, 6167, 6180, 6194, 6197, 6212, 6223, 6231,\n",
      "        6246, 6261, 6278, 6310, 6311, 6314, 6349, 6350])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 97 = 554 | subset = tensor([   1,    3,    6,    9,   10,   11,   12,   13,   14,   15,   17,   20,\n",
      "          22,   23,   26,   27,   28,   35,   36,   42,   43,   46,   51,   54,\n",
      "          55,   61,   67,   69,   71,   72,   73,   76,   78,   79,   83,   84,\n",
      "          88,   90,   94,   96,   97,   98,  100,  102,  104,  106,  109,  111,\n",
      "         113,  115,  116,  118,  122,  125,  128,  133,  138,  140,  141,  144,\n",
      "         148,  151,  153,  154,  160,  164,  169,  170,  171,  173,  174,  179,\n",
      "         181,  187,  189,  191,  193,  196,  202,  205,  206,  208,  214,  217,\n",
      "         220,  223,  228,  234,  237,  238,  239,  240,  241,  245,  248,  249,\n",
      "         251,  253,  254,  259,  262,  264,  265,  266,  274,  290,  292,  295,\n",
      "         298,  299,  308,  311,  313,  314,  317,  319,  321,  332,  344,  345,\n",
      "         346,  347,  348,  349,  351,  352,  354,  361,  363,  365,  372,  373,\n",
      "         374,  375,  376,  381,  385,  388,  389,  390,  394,  396,  398,  400,\n",
      "         401,  408,  412,  415,  421,  424,  427,  428,  433,  434,  435,  436,\n",
      "         439,  440,  447,  450,  459,  460,  466,  469,  472,  580,  601,  645,\n",
      "         698,  757,  800,  831,  885,  886,  919,  984, 1069, 1120, 1141, 1142,\n",
      "        1143, 1145, 1146, 1152, 1153, 1154, 1156, 1157, 1161, 1162, 1163, 1164,\n",
      "        1166, 1167, 1168, 1169, 1170, 1171, 1172, 1174, 1177, 1179, 1184, 1185,\n",
      "        1186, 1187, 1188, 1189, 1190, 1193, 1195, 1196, 1197, 1203, 1208, 1233,\n",
      "        1236, 1238, 1239, 1242, 1243, 1246, 1248, 1249, 1250, 1253, 1255, 1256,\n",
      "        1259, 1262, 1266, 1267, 1270, 1271, 1272, 1277, 1278, 1283, 1284, 1287,\n",
      "        1291, 1293, 1296, 1297, 1301, 1306, 1310, 1315, 1316, 1317, 1318, 1319,\n",
      "        1320, 1324, 1325, 1329, 1373, 1640, 1694, 1702, 1711, 1720, 1741, 1743,\n",
      "        1744, 1753, 1770, 1792, 1813, 1820, 1827, 1842, 1850, 1865, 1866, 1885,\n",
      "        1889, 1892, 1895, 1917, 1918, 1928, 1929, 1931, 1935, 1936, 1959, 1960,\n",
      "        1967, 1972, 1981, 1992, 2000, 2001, 2004, 2021, 2039, 2040, 2054, 2059,\n",
      "        2062, 2063, 2075, 2091, 2106, 2127, 2143, 2148, 2152, 2164, 2178, 2189,\n",
      "        2209, 2211, 2216, 2231, 2254, 2261, 2277, 2287, 2290, 2291, 2298, 2311,\n",
      "        2314, 2335, 2340, 2348, 2369, 2384, 2396, 2397, 2402, 2406, 2421, 2434,\n",
      "        2446, 2454, 2470, 2474, 2484, 2496, 2503, 2516, 2534, 2535, 2565, 2576,\n",
      "        2578, 2583, 2592, 2593, 2598, 2603, 2606, 2610, 2611, 2623, 2643, 2649,\n",
      "        2671, 2682, 2690, 2696, 2715, 2722, 2734, 2737, 2739, 2743, 2766, 2774,\n",
      "        2775, 2784, 2786, 2789, 2791, 2797, 2817, 2827, 2834, 2836, 2838, 2840,\n",
      "        2847, 2850, 2852, 2866, 2878, 2879, 2882, 2891, 2914, 2919, 2938, 2948,\n",
      "        2966, 2983, 2989, 2993, 2996, 2997, 3003, 3020, 3032, 3039, 3044, 3045,\n",
      "        3056, 3058, 3060, 3066, 3072, 3075, 3076, 3078, 3082, 3084, 3087, 3095,\n",
      "        3098, 3100, 3102, 3103, 3104, 3105, 3109, 3112, 3118, 3126, 3133, 3135,\n",
      "        3141, 3145, 3146, 3147, 3149, 3152, 3154, 3157, 3160, 3162, 3163, 3165,\n",
      "        3166, 3167, 3168, 3169, 3172, 3174, 3175, 3179, 3180, 3285, 3294, 3302,\n",
      "        3314, 3320, 3346, 3364, 3398, 3411, 3434, 3437, 3461, 3466, 3475, 3479,\n",
      "        3480, 3497, 3509, 3516, 3530, 3545, 3556, 3557, 3571, 3581, 3614, 3669,\n",
      "        3681, 3774, 3838, 3921, 3944, 3957, 4042, 4057, 4110, 4136, 4140, 4153,\n",
      "        4156, 4270, 4282, 4324, 4473, 4526, 4849, 5065, 5068, 5070, 5075, 5076,\n",
      "        5081, 5085, 5136, 5155, 5159, 5176, 5220, 5229, 5290, 5323, 5331, 5408,\n",
      "        5474, 5504, 5515, 5519, 5555, 5605, 5663, 5697, 5714, 5738, 5739, 5785,\n",
      "        5788, 5794, 5807, 5894, 6094, 6097, 6100, 6106, 6135, 6144, 6158, 6167,\n",
      "        6180, 6194, 6197, 6212, 6223, 6231, 6246, 6261, 6278, 6310, 6311, 6314,\n",
      "        6349, 6350])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 989 = 3 | subset = tensor([945, 987, 989])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 96 = 82 | subset = tensor([  90,   96, 3220, 3222, 3224, 3225, 3227, 3230, 3231, 3246, 3248, 3262,\n",
      "        3264, 3270, 3286, 3301, 3305, 3307, 3310, 3312, 3313, 3315, 3318, 3327,\n",
      "        3330, 3347, 3350, 3358, 3372, 3375, 3377, 3379, 3382, 3389, 3393, 3397,\n",
      "        3399, 3400, 3410, 3414, 3416, 3418, 3422, 3428, 3432, 3433, 3440, 3442,\n",
      "        3449, 3452, 3462, 3476, 3481, 3485, 3492, 3493, 3494, 3496, 3504, 3511,\n",
      "        3512, 3517, 3522, 3529, 3531, 3532, 3546, 3561, 3564, 3568, 3575, 3576,\n",
      "        3577, 3578, 5102, 5118, 5188, 5347, 5405, 5410, 5712, 5789])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 94 = 82 | subset = tensor([  90,   94, 3220, 3222, 3224, 3225, 3227, 3230, 3231, 3246, 3248, 3262,\n",
      "        3264, 3270, 3286, 3301, 3305, 3307, 3310, 3312, 3313, 3315, 3318, 3327,\n",
      "        3330, 3347, 3350, 3358, 3372, 3375, 3377, 3379, 3382, 3389, 3393, 3397,\n",
      "        3399, 3400, 3410, 3414, 3416, 3418, 3422, 3428, 3432, 3433, 3440, 3442,\n",
      "        3449, 3452, 3462, 3476, 3481, 3485, 3492, 3493, 3494, 3496, 3504, 3511,\n",
      "        3512, 3517, 3522, 3529, 3531, 3532, 3546, 3561, 3564, 3568, 3575, 3576,\n",
      "        3577, 3578, 5102, 5118, 5188, 5347, 5405, 5410, 5712, 5789])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Number of nodes in subgraph for node_id: 108 = 3 | subset = tensor([ 108, 2323, 2424])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Number of nodes in subgraph for node_id: 117 = 11 | subset = tensor([ 117, 3181, 3184, 3197, 3362, 3374, 3409, 3415, 3514, 3521, 5919])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Number of nodes in subgraph for node_id: 26 = 39 | subset = tensor([  26,  534,  539,  545,  555,  556,  572,  576,  594,  601,  610,  611,\n",
      "         624,  636,  676,  695,  711,  727,  757,  786,  831,  849,  862,  865,\n",
      "         873,  892,  910,  919,  945,  969,  984, 1033, 1037, 1050, 1057, 1066,\n",
      "        1119, 1120, 1127])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Number of nodes in subgraph for node_id: 166 = 28 | subset = tensor([  69,   81,   85,  166,  335,  548,  554,  607,  673,  690,  697,  721,\n",
      "         735,  751,  752,  819,  834,  904,  946,  979,  994, 1095, 1723, 1742,\n",
      "        1779, 2029, 2467, 2760])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Number of nodes in subgraph for node_id: 248 = 544 | subset = tensor([  11,   12,   13,   14,   15,   16,   17,   20,   23,   24,   26,   29,\n",
      "          42,   49,   50,   54,   65,   67,   69,   73,   75,   76,   80,   87,\n",
      "          92,   93,   95,   96,   98,   99,  100,  102,  106,  108,  109,  111,\n",
      "         112,  114,  116,  117,  118,  120,  121,  122,  128,  129,  133,  136,\n",
      "         137,  140,  141,  148,  149,  150,  151,  153,  154,  160,  164,  168,\n",
      "         169,  170,  171,  175,  178,  179,  183,  185,  189,  191,  193,  196,\n",
      "         200,  205,  207,  208,  210,  211,  214,  215,  220,  222,  223,  226,\n",
      "         230,  231,  235,  237,  238,  240,  241,  244,  248,  249,  250,  251,\n",
      "         252,  253,  254,  258,  260,  261,  262,  263,  264,  265,  267,  271,\n",
      "         274,  276,  279,  290,  292,  297,  298,  299,  306,  308,  312,  313,\n",
      "         314,  321,  323,  325,  327,  333,  335,  337,  340,  342,  344,  345,\n",
      "         346,  348,  349,  351,  352,  358,  361,  362,  366,  367,  371,  373,\n",
      "         374,  375,  380,  382,  386,  388,  390,  391,  394,  396,  397,  398,\n",
      "         403,  405,  410,  411,  412,  413,  415,  418,  419,  421,  422,  428,\n",
      "         433,  434,  435,  436,  438,  439,  441,  442,  445,  447,  453,  454,\n",
      "         456,  458,  459,  460,  466,  468,  515,  648,  885,  963,  991, 1035,\n",
      "        1039, 1140, 1144, 1147, 1155, 1158, 1159, 1160, 1163, 1165, 1167, 1168,\n",
      "        1169, 1170, 1174, 1182, 1185, 1188, 1190, 1192, 1194, 1196, 1198, 1201,\n",
      "        1237, 1243, 1328, 1705, 1863, 2268, 2381, 2497, 2744, 2830, 2999, 3059,\n",
      "        3064, 3068, 3069, 3077, 3082, 3090, 3092, 3093, 3106, 3110, 3111, 3112,\n",
      "        3114, 3116, 3119, 3120, 3122, 3131, 3134, 3136, 3137, 3140, 3143, 3148,\n",
      "        3151, 3153, 3156, 3164, 3167, 3170, 3171, 3173, 3176, 3177, 3181, 3184,\n",
      "        3188, 3192, 3193, 3194, 3208, 3211, 3218, 3221, 3251, 3275, 3294, 3319,\n",
      "        3320, 3338, 3362, 3371, 3373, 3374, 3378, 3385, 3395, 3398, 3408, 3409,\n",
      "        3415, 3417, 3427, 3434, 3439, 3447, 3474, 3515, 3521, 3530, 3536, 3557,\n",
      "        3558, 3603, 3624, 3625, 3626, 3634, 3643, 3647, 3665, 3683, 3686, 3688,\n",
      "        3689, 3690, 3691, 3708, 3711, 3714, 3718, 3738, 3741, 3749, 3761, 3765,\n",
      "        3769, 3774, 3790, 3796, 3805, 3807, 3808, 3813, 3826, 3838, 3839, 3857,\n",
      "        3861, 3880, 3905, 3909, 3913, 3921, 3936, 3939, 3941, 3944, 3945, 3951,\n",
      "        3957, 3958, 3959, 3969, 3986, 3996, 4013, 4020, 4021, 4029, 4041, 4044,\n",
      "        4047, 4084, 4085, 4095, 4098, 4110, 4112, 4114, 4116, 4121, 4128, 4135,\n",
      "        4143, 4158, 4176, 4196, 4209, 4212, 4218, 4224, 4228, 4232, 4236, 4250,\n",
      "        4277, 4281, 4285, 4291, 4299, 4310, 4332, 4333, 4377, 4386, 4393, 4396,\n",
      "        4408, 4413, 4428, 4432, 4437, 4441, 4443, 4453, 4458, 4466, 4469, 4482,\n",
      "        4497, 4502, 4507, 4526, 4536, 4538, 4544, 4546, 4565, 4569, 4578, 4580,\n",
      "        4588, 4606, 4618, 4623, 4641, 4647, 4653, 4657, 4660, 4662, 4683, 4700,\n",
      "        4701, 4704, 4710, 4715, 4719, 4720, 4734, 4755, 4759, 4761, 4762, 4764,\n",
      "        4769, 4777, 4782, 4786, 4795, 4815, 4824, 4825, 4836, 4839, 4841, 4849,\n",
      "        4853, 5138, 5150, 5157, 5158, 5187, 5196, 5267, 5277, 5281, 5298, 5307,\n",
      "        5342, 5345, 5357, 5369, 5375, 5412, 5415, 5422, 5446, 5447, 5464, 5467,\n",
      "        5482, 5490, 5498, 5501, 5515, 5522, 5525, 5536, 5546, 5553, 5571, 5581,\n",
      "        5583, 5595, 5619, 5642, 5698, 5720, 5731, 5739, 5752, 5756, 5793, 5795,\n",
      "        5803, 5828, 5839, 5862, 5896, 6081, 6103, 6110, 6114, 6117, 6121, 6130,\n",
      "        6139, 6154, 6159, 6170, 6182, 6186, 6189, 6191, 6199, 6202, 6203, 6209,\n",
      "        6224, 6227, 6229, 6236, 6237, 6238, 6240, 6242, 6243, 6247, 6248, 6251,\n",
      "        6256, 6265, 6266, 6350])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Number of nodes in subgraph for node_id: 344 = 22 | subset = tensor([  69,   81,   85,   98,  139,  192,  214,  215,  251,  285,  292,  344,\n",
      "         359,  360,  362,  363,  373,  386, 3189, 4247, 4539, 4573])\n",
      "\n",
      "Exploration results written to 'graph_exploration.txt'.\n",
      "\n",
      "Selected Node ID: 8 (Type: Project)\n",
      "result =  {'ns': 0, 'title': 'Joint Polar Satellite System', 'pageid': 31362604, 'size': 21941, 'wordcount': 2481, 'snippet': 'The <span class=\"searchmatch\">Joint</span> <span class=\"searchmatch\">Polar</span> <span class=\"searchmatch\">Satellite</span> <span class=\"searchmatch\">System</span> (JPSS) is the latest generation of U.S. <span class=\"searchmatch\">polar</span>-<span class=\"searchmatch\">orbiting</span>, non-geosynchronous, environmental <span class=\"searchmatch\">satellites</span>. JPSS will provide', 'timestamp': '2024-12-06T16:23:55Z'}\n",
      "result =  {'ns': 0, 'title': 'Suomi NPP', 'pageid': 34540265, 'size': 18364, 'wordcount': 1780, 'snippet': 'Suomi <span class=\"searchmatch\">National</span> <span class=\"searchmatch\">Polar</span>-<span class=\"searchmatch\">orbiting</span> <span class=\"searchmatch\">Partnership</span> (Suomi NPP), previously known as the <span class=\"searchmatch\">National</span> <span class=\"searchmatch\">Polar</span>-<span class=\"searchmatch\">orbiting</span> Operational Environmental <span class=\"searchmatch\">Satellite</span> <span class=\"searchmatch\">System</span> Preparatory', 'timestamp': '2024-11-05T19:47:01Z'}\n",
      "result =  {'ns': 0, 'title': 'National Oceanic and Atmospheric Administration', 'pageid': 37876, 'size': 45583, 'wordcount': 4358, 'snippet': 'generations of <span class=\"searchmatch\">satellites</span> are developed to succeed the current <span class=\"searchmatch\">polar</span> <span class=\"searchmatch\">orbiting</span> and geosynchronous <span class=\"searchmatch\">satellites</span>, the <span class=\"searchmatch\">Joint</span> <span class=\"searchmatch\">Polar</span> <span class=\"searchmatch\">Satellite</span> <span class=\"searchmatch\">System</span>, and GOES-R', 'timestamp': '2024-11-29T14:52:00Z'}\n",
      "result =  {'ns': 0, 'title': 'Geostationary Operational Environmental Satellite', 'pageid': 82915, 'size': 40884, 'wordcount': 4218, 'snippet': 'Environmental <span class=\"searchmatch\">Satellite</span> (GOES), operated by the United States\\' <span class=\"searchmatch\">National</span> Oceanic and Atmospheric Administration (NOAA)\\'s <span class=\"searchmatch\">National</span> Environmental <span class=\"searchmatch\">Satellite</span>, Data', 'timestamp': '2024-06-30T10:52:52Z'}\n",
      "result =  {'ns': 0, 'title': 'NOAA-20', 'pageid': 56087100, 'size': 16830, 'wordcount': 1538, 'snippet': 'called the <span class=\"searchmatch\">Joint</span> <span class=\"searchmatch\">Polar</span> <span class=\"searchmatch\">Satellite</span> <span class=\"searchmatch\">System</span>. NOAA-20 was launched on 18 November 2017 and joined the Suomi <span class=\"searchmatch\">National</span> <span class=\"searchmatch\">Polar</span>-<span class=\"searchmatch\">orbiting</span> <span class=\"searchmatch\">Partnership</span> <span class=\"searchmatch\">satellite</span> in the', 'timestamp': '2024-10-20T18:28:47Z'}\n",
      "The prompt used for the Wikipedia context = National Polar Orbiting Partnership-Joint Polar Satellite System\n",
      "\n",
      "Wikipedia Context:\n",
      "\n",
      "Result 1:\n",
      "Title: Joint Polar Satellite System\n",
      "Description: Constellation of American meteorology satellites\n",
      "Summary: The Joint Polar Satellite System (JPSS) is the latest generation of U.S. polar-orbiting, non-geosynchronous, environmental satellites. JPSS will provide the global environmental data used in numerical weather prediction models for forecasts, and scientific data used for climate monitoring. JPSS will aid in fulfilling the mission of the U.S. National Oceanic and Atmospheric Administration (NOAA), an agency of the Department of Commerce. Data and imagery obtained from the JPSS will increase timeliness and accuracy of public warnings and forecasts of climate and weather events, thus reducing the potential loss of human life and property and advancing the national economy. The JPSS is developed by the National Aeronautics and Space Administration (NASA) for the National Oceanic and Atmospheric Administration (NOAA), who is responsible for operation of JPSS. Three to five satellites are planned for the JPSS constellation of satellites. JPSS satellites will be flown, and the scientific data from JPSS will be processed, by the JPSS – Common Ground System (JPSS-CGS).\n",
      "Link: https://en.wikipedia.org/wiki/Joint_Polar_Satellite_System\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/JPSS-1.jpg/320px-JPSS-1.jpg\n",
      "\n",
      "Result 2:\n",
      "Title: Suomi NPP\n",
      "Description: NASA/NOAA Earth weather satellite (2011–Present)\n",
      "Summary: The Suomi National Polar-orbiting Partnership, previously known as the National Polar-orbiting Operational Environmental Satellite System Preparatory Project (NPP) and NPP-Bridge, is a weather satellite operated by the United States National Oceanic and Atmospheric Administration (NOAA). It was launched in 2011 and is currently in operation.\n",
      "Link: https://en.wikipedia.org/wiki/Suomi_NPP\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/5/55/Suomi_NPP_satellite.jpg/320px-Suomi_NPP_satellite.jpg\n",
      "\n",
      "Result 3:\n",
      "Title: National Oceanic and Atmospheric Administration\n",
      "Description: US government scientific agency\n",
      "Summary: The National Oceanic and Atmospheric Administration is an American scientific and regulatory agency charged with forecasting weather, monitoring oceanic and atmospheric conditions, charting the seas, conducting deep-sea exploration, and managing fishing and protection of marine mammals and endangered species in the US exclusive economic zone. The agency is part of the United States Department of Commerce and is headquartered in Silver Spring, Maryland.\n",
      "Link: https://en.wikipedia.org/wiki/National_Oceanic_and_Atmospheric_Administration\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/c/c9/NOAA_Flag.svg/320px-NOAA_Flag.svg.png\n",
      "\n",
      "Result 4:\n",
      "Title: Geostationary Operational Environmental Satellite\n",
      "Description: US weather satellite series\n",
      "Summary: The Geostationary Operational Environmental Satellite (GOES), operated by the United States' National Oceanic and Atmospheric Administration (NOAA)'s National Environmental Satellite, Data, and Information Service division, supports weather forecasting, severe storm tracking, and meteorology research. Spacecraft and ground-based elements of the system work together to provide a continuous stream of environmental data. The National Weather Service (NWS) and the Meteorological Service of Canada use the GOES system for their North American weather monitoring and forecasting operations, and scientific researchers use the data to better understand land, atmosphere, ocean, and climate dynamics.\n",
      "Link: https://en.wikipedia.org/wiki/Geostationary_Operational_Environmental_Satellite\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/GOES_8_Spac0255.jpg/320px-GOES_8_Spac0255.jpg\n",
      "\n",
      "Result 5:\n",
      "Title: NOAA-20\n",
      "Description: NASA/NOAA weather satellite (2017–Present)\n",
      "Summary: NOAA-20, designated JPSS-1 prior to launch, is the first of the United States National Oceanic and Atmospheric Administration's latest generation of U.S. polar-orbiting, non-geosynchronous, environmental satellites called the Joint Polar Satellite System. NOAA-20 was launched on 18 November 2017 and joined the Suomi National Polar-orbiting Partnership satellite in the same orbit. NOAA-20 operates about 50 minutes behind Suomi NPP, allowing important overlap in observational coverage. Circling the Earth from pole-to-pole, it crosses the equator about 14 times daily, providing full global coverage twice a day. This gives meteorologists information on \"atmospheric temperature and moisture, clouds, sea-surface temperature, ocean color, sea ice cover, volcanic ash, and fire detection\" so as to enhance weather forecasting including hurricane tracking, post-hurricane recovery by detailing storm damage and mapping of power outages.\n",
      "Link: https://en.wikipedia.org/wiki/NOAA-20\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/6/6a/JPSS-1.jpg/320px-JPSS-1.jpg\n"
     ]
    }
   ],
   "source": [
    "# Assumes `graph_results` contains the 50 search results\n",
    "run_exploration_tool(data, graph_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connections for node 76606 of type 'Publication': [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n"
     ]
    }
   ],
   "source": [
    "# See what kinds of edge types exist for a given node\n",
    "def check_node_connections(data, node_type, node_idx):\n",
    "    connections = []\n",
    "    for edge_type in data.edge_types:\n",
    "        edge_index = data[edge_type].edge_index\n",
    "        if node_idx in edge_index[0] or node_idx in edge_index[1]:\n",
    "            connections.append(edge_type)\n",
    "    return connections\n",
    "\n",
    "node_idx = 76606\n",
    "node_type = 'Publication'\n",
    "connections = check_node_connections(data, node_type, node_idx)\n",
    "print(f\"Connections for node {node_idx} of type '{node_type}': {connections}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scholarly PiPy package\n",
    "from scholarly import scholarly\n",
    "\n",
    "def fetch_scholar_info(query):\n",
    "    try:\n",
    "        search_query = scholarly.search_pubs(query)\n",
    "        for i in range(3):  # Get top 3 results\n",
    "            paper = next(search_query)\n",
    "            print(f\"Title: {paper['bib']['title']}\")\n",
    "            print(f\"Abstract: {paper['bib'].get('abstract', 'No abstract available')}\")\n",
    "            print(f\"URL: {paper.get('pub_url', 'No URL available')}\\n\")\n",
    "    except StopIteration:\n",
    "        print(\"No results found on Google Scholar.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from Google Scholar: {e}\")\n",
    "\n",
    "# Example usage\n",
    "query = \"AIRS Aqua CO2 free troposphere\"\n",
    "fetch_scholar_info(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "#### Known Issues:\n",
    "- Duplicates in the dataset are not manually removed\n",
    "- Using WIKIPEDIA as external resource. NASA APIs are very specific and not generalizeable to the specifc user queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
