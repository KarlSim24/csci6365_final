{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project: Query-Driven Retrieval-Augmented Graph Exploration Tool\n",
    "By Karl Simon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the dataset into PyG (PyTorch Geometric)\n",
    "- Syntax based on the huggingface dataset loading instructions: https://huggingface.co/datasets/nasa-gesdisc/nasa-eo-knowledge-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes and Properties:\n",
      "\n",
      "Node Type: Dataset\n",
      "Number of Nodes: 6390\n",
      "  - temporalExtentStart: 6375 items (non-numeric)\n",
      "  - seCorner: 5330 items (non-numeric)\n",
      "  - cmrId: 6390 items (non-numeric)\n",
      "  - globalId: 6390 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([6390, 512])\n",
      "  - abstract: 6390 items (non-numeric)\n",
      "  - daac: 6131 items (non-numeric)\n",
      "  - nwCorner: 5330 items (non-numeric)\n",
      "  - temporalFrequency: 6390 items (non-numeric)\n",
      "  - pagerank_global: torch.Size([6390, 1])\n",
      "  - temporalExtentEnd: 3765 items (non-numeric)\n",
      "  - shortName: 6390 items (non-numeric)\n",
      "  - landingPageUrl: 3037 items (non-numeric)\n",
      "  - doi: 6390 items (non-numeric)\n",
      "  - longName: 6390 items (non-numeric)\n",
      "\n",
      "Node Type: DataCenter\n",
      "Number of Nodes: 184\n",
      "  - pagerank_global: torch.Size([184, 1])\n",
      "  - globalId: 184 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([184, 512])\n",
      "  - shortName: 184 items (non-numeric)\n",
      "  - url: 184 items (non-numeric)\n",
      "  - longName: 184 items (non-numeric)\n",
      "\n",
      "Node Type: Project\n",
      "Number of Nodes: 333\n",
      "  - pagerank_global: torch.Size([333, 1])\n",
      "  - globalId: 333 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([333, 512])\n",
      "  - shortName: 333 items (non-numeric)\n",
      "  - longName: 333 items (non-numeric)\n",
      "\n",
      "Node Type: Platform\n",
      "Number of Nodes: 442\n",
      "  - Type: 442 items (non-numeric)\n",
      "  - pagerank_global: torch.Size([442, 1])\n",
      "  - globalId: 442 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([442, 512])\n",
      "  - shortName: 442 items (non-numeric)\n",
      "  - longName: 442 items (non-numeric)\n",
      "\n",
      "Node Type: Instrument\n",
      "Number of Nodes: 867\n",
      "  - pagerank_global: torch.Size([867, 1])\n",
      "  - globalId: 867 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([867, 512])\n",
      "  - shortName: 867 items (non-numeric)\n",
      "  - longName: 867 items (non-numeric)\n",
      "\n",
      "Node Type: ScienceKeyword\n",
      "Number of Nodes: 1609\n",
      "  - pagerank_global: torch.Size([1609, 1])\n",
      "  - globalId: 1609 items (non-numeric)\n",
      "  - name: 1609 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([1609, 512])\n",
      "  - pagerank_publication_dataset: torch.Size([1609, 1])\n",
      "\n",
      "Node Type: Publication\n",
      "Number of Nodes: 125939\n",
      "  - year: 125939 items (non-numeric)\n",
      "  - pagerank_global: torch.Size([125939, 1])\n",
      "  - globalId: 125939 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([125939, 512])\n",
      "  - title: 125937 items (non-numeric)\n",
      "  - DOI: 125939 items (non-numeric)\n",
      "  - abstract: 99859 items (non-numeric)\n",
      "  - authors: 109975 items (non-numeric)\n",
      "\n",
      "Edges and Types:\n",
      "Edge Type: ('DataCenter', 'HAS_DATASET', 'Dataset') - Number of Edges: 9017 - Shape: torch.Size([2, 9017])\n",
      "Edge Type: ('Dataset', 'OF_PROJECT', 'Project') - Number of Edges: 6049 - Shape: torch.Size([2, 6049])\n",
      "Edge Type: ('Dataset', 'HAS_PLATFORM', 'Platform') - Number of Edges: 9884 - Shape: torch.Size([2, 9884])\n",
      "Edge Type: ('Platform', 'HAS_INSTRUMENT', 'Instrument') - Number of Edges: 2469 - Shape: torch.Size([2, 2469])\n",
      "Edge Type: ('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword') - Number of Edges: 1823 - Shape: torch.Size([2, 1823])\n",
      "Edge Type: ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword') - Number of Edges: 20436 - Shape: torch.Size([2, 20436])\n",
      "Edge Type: ('Publication', 'CITES', 'Publication') - Number of Edges: 208670 - Shape: torch.Size([2, 208670])\n",
      "Edge Type: ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword') - Number of Edges: 89039 - Shape: torch.Size([2, 89039])\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports for entire notebook\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from torch_geometric.data import HeteroData\n",
    "from collections import defaultdict\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "# from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import random\n",
    "from scholarly import scholarly\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "MAX_VAL_LEN = 1000 # max text length for input to LLM from graph_results for each node\n",
    "\n",
    "\n",
    "# Load JSON data from file\n",
    "file_path = \"/home/karlsimon/CSCI6365/final/graph.json\"\n",
    "graph_data = []\n",
    "\n",
    "# Load data line by line to prevent memory overload\n",
    "with open(file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            graph_data.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON line: {e}\")\n",
    "            continue\n",
    "\n",
    "# Initialize HeteroData object\n",
    "data = HeteroData()\n",
    "\n",
    "# Mapping for node indices per node type\n",
    "node_mappings = defaultdict(dict)\n",
    "\n",
    "# Temporary storage for properties\n",
    "node_properties = defaultdict(lambda: defaultdict(list))\n",
    "edge_indices = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# # Define limits for node subsets based on type\n",
    "# node_limits = {\n",
    "#     'Publication': 1000,\n",
    "#     'Dataset': 500,\n",
    "#     'ScienceKeyword': 300,\n",
    "#     'Instrument': 200,\n",
    "#     'Platform': 150,\n",
    "#     'Project': 100,\n",
    "#     'DataCenter': 50\n",
    "# }\n",
    "\n",
    "# Track the number of nodes added per type\n",
    "node_counts = defaultdict(int)\n",
    "\n",
    "# Process nodes with limits\n",
    "for item in graph_data:\n",
    "    if item['type'] == 'node':\n",
    "        node_type = item['labels'][0]\n",
    "        # if node_counts[node_type] >= node_limits.get(node_type, 50):\n",
    "        #     continue  # Skip nodes once the limit is reached\n",
    "\n",
    "        node_id = item['id']\n",
    "        properties = item['properties']\n",
    "\n",
    "        # Store the node index mapping\n",
    "        node_index = len(node_mappings[node_type])\n",
    "        node_mappings[node_type][node_id] = node_index\n",
    "        node_counts[node_type] += 1\n",
    "\n",
    "        # Store properties temporarily by type\n",
    "        for key, value in properties.items():\n",
    "            if isinstance(value, list) and all(isinstance(v, (int, float)) for v in value):\n",
    "                node_properties[node_type][key].append(torch.tensor(value, dtype=torch.float))\n",
    "            elif isinstance(value, (int, float)):\n",
    "                node_properties[node_type][key].append(torch.tensor([value], dtype=torch.float))\n",
    "            else:\n",
    "                node_properties[node_type][key].append(value)  # non-numeric properties as lists\n",
    "\n",
    "# # Define limits for relationships based on type\n",
    "# relationship_limits = {\n",
    "#     'CITES': 2000,\n",
    "#     'HAS_APPLIED_RESEARCH_AREA': 1000,\n",
    "#     'HAS_SCIENCEKEYWORD': 500,\n",
    "#     'HAS_PLATFORM': 500,\n",
    "#     'HAS_DATASET': 500,\n",
    "#     'OF_PROJECT': 300,\n",
    "#     'HAS_INSTRUMENT': 200\n",
    "# }\n",
    "\n",
    "# Track the number of relationships added per type\n",
    "relationship_counts = defaultdict(int)\n",
    "\n",
    "# Filter relationships to only include sampled nodes\n",
    "for item in graph_data:\n",
    "    if item['type'] == 'relationship':\n",
    "        start_type = item['start']['labels'][0]\n",
    "        end_type = item['end']['labels'][0]\n",
    "        start_id = item['start']['id']\n",
    "        end_id = item['end']['id']\n",
    "        edge_type = item['label']\n",
    "\n",
    "        # # Skip if relationship limit reached\n",
    "        # if relationship_counts[edge_type] >= relationship_limits.get(edge_type, 100):\n",
    "        #     continue\n",
    "\n",
    "        # Check if start and end nodes exist in the sampled nodes\n",
    "        if start_id in node_mappings[start_type] and end_id in node_mappings[end_type]:\n",
    "            start_idx = node_mappings[start_type][start_id]\n",
    "            end_idx = node_mappings[end_type][end_id]\n",
    "\n",
    "            # Append to edge list\n",
    "            edge_indices[(start_type, edge_type, end_type)]['start'].append(start_idx)\n",
    "            edge_indices[(start_type, edge_type, end_type)]['end'].append(end_idx)\n",
    "            relationship_counts[edge_type] += 1\n",
    "\n",
    "# Finalize node properties by batch processing\n",
    "for node_type, properties in node_properties.items():\n",
    "    data[node_type].num_nodes = len(node_mappings[node_type])\n",
    "    for key, values in properties.items():\n",
    "        if isinstance(values[0], torch.Tensor):\n",
    "            data[node_type][key] = torch.stack(values)\n",
    "        else:\n",
    "            data[node_type][key] = values  # Keep non-tensor properties as lists\n",
    "\n",
    "# Finalize edge indices in bulk\n",
    "for (start_type, edge_type, end_type), indices in edge_indices.items():\n",
    "    edge_index = torch.tensor([indices['start'], indices['end']], dtype=torch.long)\n",
    "    data[start_type, edge_type, end_type].edge_index = edge_index\n",
    "\n",
    "# Display statistics for verification\n",
    "print(\"Nodes and Properties:\")\n",
    "for node_type in data.node_types:\n",
    "    print(f\"\\nNode Type: {node_type}\")\n",
    "    print(f\"Number of Nodes: {data[node_type].num_nodes}\")\n",
    "    for key, value in data[node_type].items():\n",
    "        if key != 'num_nodes':\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"  - {key}: {value.shape}\")\n",
    "            else:\n",
    "                print(f\"  - {key}: {len(value)} items (non-numeric)\")\n",
    "\n",
    "print(\"\\nEdges and Types:\")\n",
    "for edge_type in data.edge_types:\n",
    "    edge_index = data[edge_type].edge_index\n",
    "    print(f\"Edge Type: {edge_type} - Number of Edges: {edge_index.size(1)} - Shape: {edge_index.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 : Search Graph for nodes based on user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Steps:\n",
    "# 1. improve graph search and rank results.\n",
    "# 2. improve subgraph exploration.\n",
    "# 3. improve external context retrieval (NASA API).\n",
    "\n",
    "# Functions definitions for keywords, search and display used in next cell\n",
    "def extract_keywords(query):\n",
    "    keywords = re.findall(r'\\b\\w+\\b', query)\n",
    "    return [kw.lower() for kw in keywords]\n",
    "\n",
    "# Updated search_graph function with TF-IDF scoring\n",
    "# TODO: make max_per_type specific to each node type\n",
    "def search_graph(data, keywords, node_types=['Dataset', 'Project', 'ScienceKeyword', 'Instrument', 'Platform', 'Publication'], max_results=50, max_per_type=10):\n",
    "    results = []\n",
    "    texts = []  # Collect text data for TF-IDF processing\n",
    "    metadata = []  # To store corresponding metadata (node type, index, key, value)\n",
    "\n",
    "    # Step 1: Collect all matching nodes and their text data\n",
    "    for node_type in node_types:\n",
    "        for key in data[node_type]:\n",
    "            if key == 'num_nodes':\n",
    "                continue\n",
    "            \n",
    "            values = data[node_type][key]\n",
    "            if isinstance(values, list):\n",
    "                for idx, value in enumerate(values):\n",
    "                    value_str = str(value).lower()\n",
    "                    if any(kw in value_str for kw in keywords):\n",
    "                        texts.append(value_str)\n",
    "                        metadata.append((node_type, idx, key, value))\n",
    "\n",
    "    if not texts:\n",
    "        return []\n",
    "\n",
    "    # Compute TF-IDF scores for the collected texts\n",
    "    # NOTE: texts stores the properties of the nodes which contain the keywords\n",
    "    vectorizer = TfidfVectorizer(vocabulary=keywords)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    scores = tfidf_matrix.sum(axis=1).A1  # Sum the TF-IDF scores for each text\n",
    "\n",
    "    # Sort the results by TF-IDF score in descending order\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    sorted_results = [metadata[i] for i in sorted_indices]\n",
    "    # with open(\"sorted_results.txt\", \"w\") as file:\n",
    "    #     for result in sorted_results:\n",
    "    #         file.write(f\"{result}\\n\")\n",
    "\n",
    "    # Limit the number of results overall and per node type\n",
    "    final_results = []\n",
    "    counts_per_type = {node_type: 0 for node_type in node_types}\n",
    "\n",
    "    for result in sorted_results:\n",
    "        node_type = result[0]\n",
    "        if len(final_results) >= max_results:\n",
    "            break\n",
    "        if counts_per_type[node_type] < max_per_type:\n",
    "            final_results.append(result)\n",
    "            counts_per_type[node_type] += 1\n",
    "\n",
    "    # write the 50 final_results to a file\n",
    "    print(\"Writing 50 final_results to file\")\n",
    "    with open(\"final_results.txt\", \"w\") as file:\n",
    "        for result in final_results:\n",
    "            file.write(f\"{result}\\n\")\n",
    "\n",
    "    return final_results\n",
    "\n",
    "# Updated display_results function to trim long values\n",
    "def display_results(results, max_value_length=MAX_VAL_LEN):\n",
    "    if not results:\n",
    "        print(\"No relevant nodes found.\")\n",
    "        return\n",
    "\n",
    "    with open(\"query_results.txt\", \"w\") as file:\n",
    "        print(f\"\\nFound {len(results)} relevant nodes:\\n\")\n",
    "        for node_type, idx, key, value in results:\n",
    "            value_str = str(value)\n",
    "            if len(value_str) > max_value_length:\n",
    "                value_str = value_str[:max_value_length] + \"...\"\n",
    "            output_line = f\"Node Type: {node_type} | Index: {idx} | Property: {key} | Value: {value_str}\\n\"\n",
    "            file.write(output_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given query, extract keywords, search the graph for relevant nodes, and display the results\n",
    "# NOTE: currently only searches for exact keyword matches in node properties\n",
    "def get_subgraph(data, node_type, node_indices, num_hops=2):\n",
    "    # Find all edge types where the node_type is either the source or target\n",
    "    relevant_edges = [\n",
    "        (src, rel, dst) for (src, rel, dst) in data.edge_types if src == node_type or dst == node_type\n",
    "    ]\n",
    "    \n",
    "    # print(\"relevant_edges = \", relevant_edges) # UNCOMMENT TO SEE RELEVANT EDGES\n",
    "\n",
    "    if not relevant_edges:\n",
    "        print(f\"No edges found for node type '{node_type}'\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Combine edge indices from all relevant edge types\n",
    "    combined_edge_index = []\n",
    "    combined_edge_types = []\n",
    "\n",
    "    for edge_type in relevant_edges:\n",
    "        edge_index = data[edge_type].edge_index\n",
    "        combined_edge_index.append(edge_index)\n",
    "        combined_edge_types.append(edge_type)\n",
    "\n",
    "    # Stack all edge indices into a single tensor\n",
    "    combined_edge_index = torch.cat(combined_edge_index, dim=1)\n",
    "\n",
    "    # Extract the subgraph using the combined edge index\n",
    "    subset, edge_index, _, _ = k_hop_subgraph(node_idx=node_indices, num_hops=num_hops, edge_index=combined_edge_index)\n",
    "    return subset, edge_index, combined_edge_types\n",
    "\n",
    "\n",
    "# Explore subgraphs based on the search results. Currently only writes to terminal, doesn't do anything analysis\n",
    "def explore_subgraphs(data, results, num_hops=2):\n",
    "    if not results:\n",
    "        print(\"No nodes to explore for subgraphs.\")\n",
    "        return\n",
    "\n",
    "    # Group the results by node type\n",
    "    nodes_by_type = defaultdict(list)\n",
    "    for node_type, idx, _, _ in results:\n",
    "        nodes_by_type[node_type].append(idx)\n",
    "\n",
    "    # Extract and display subgraphs for each node type\n",
    "    for node_type, indices in nodes_by_type.items():\n",
    "        print(f\"\\nExploring subgraph for node type: {node_type}\")\n",
    "        # print(f\"Number of nodes: {len(indices)}\") #10 nodes\n",
    "        # Get the valid range for node indices\n",
    "        num_nodes = data[node_type].num_nodes\n",
    "        valid_indices = [idx for idx in indices if idx < num_nodes]\n",
    "\n",
    "        if not valid_indices:\n",
    "            print(f\"No valid indices for node type '{node_type}'.\")\n",
    "            continue\n",
    "\n",
    "        node_indices = torch.tensor(valid_indices[:10])  # Limit to 10 nodes (only using 10 per node_type anyways for now)\n",
    "        print(f\"Exploring subgraph for node indices: {node_indices}\") # may not be sequential due to search results ordering \n",
    "        subset, edge_index, edge_type = get_subgraph(data, node_type, node_indices, num_hops=num_hops)\n",
    "\n",
    "        if subset is not None and edge_index is not None:\n",
    "            print(f\"Extracted subgraph with {len(subset)} nodes and {edge_index.size(1)} edges.\")\n",
    "            print(f\"Edge Type: {edge_type}\")\n",
    "        else:\n",
    "            print(f\"Could not extract subgraph for node type: {node_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 : Use APIs Wikipedia for external information based on user query\n",
    "- NOTE: API queries are based on keywords, and not the extracted graph nodes from keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get external context from Wikipedia using the REST API\n",
    "def fetch_wikipedia_context(keywords):\n",
    "    search_term = \" \".join(keywords)\n",
    "    \n",
    "    # Use the Action API to get the top 5 search results\n",
    "    search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    search_params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": search_term,\n",
    "        \"srlimit\": 5,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"GraphExplorationTool/1.0 (ksimon24@gwu.edu)\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        search_response = requests.get(search_url, params=search_params, headers=headers)\n",
    "        search_response.raise_for_status()\n",
    "        search_data = search_response.json()\n",
    "        \n",
    "        search_results = search_data.get(\"query\", {}).get(\"search\", [])\n",
    "        \n",
    "        if not search_results:\n",
    "            return None\n",
    "\n",
    "        # Fetch summaries using the REST API for each search result\n",
    "        context_list = []\n",
    "        for result in search_results:\n",
    "            print(\"result = \", result)\n",
    "            page_title = result.get(\"title\")\n",
    "            rest_url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{page_title.replace(' ', '_')}\"\n",
    "            \n",
    "            rest_response = requests.get(rest_url, headers=headers)\n",
    "            rest_response.raise_for_status()\n",
    "            rest_data = rest_response.json()\n",
    "            \n",
    "            # Extract relevant information\n",
    "            title = rest_data.get(\"title\", \"No Title\")\n",
    "            description = rest_data.get(\"description\", \"No Description Available.\")\n",
    "            summary = rest_data.get(\"extract\", \"No Summary Available.\")\n",
    "            link = rest_data.get(\"content_urls\", {}).get(\"desktop\", {}).get(\"page\", \"No Link Available.\")\n",
    "            thumbnail = rest_data.get(\"thumbnail\", {}).get(\"source\", None)\n",
    "            \n",
    "            context_entry = {\n",
    "                \"title\": title,\n",
    "                \"description\": description,\n",
    "                \"summary\": summary,\n",
    "                \"link\": link,\n",
    "                \"thumbnail\": thumbnail\n",
    "            }\n",
    "            context_list.append(context_entry)\n",
    "        \n",
    "        return context_list\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching Wikipedia context: {e}\")\n",
    "        return None\n",
    "    \n",
    "def display_wikipedia_context(context_list):\n",
    "    if not context_list:\n",
    "        print(\"\\nNo external context available from Wikipedia.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nWikipedia Context:\")\n",
    "    for i, context in enumerate(context_list, start=1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"Title: {context['title']}\")\n",
    "        print(f\"Description: {context['description']}\")\n",
    "        print(f\"Summary: {context['summary']}\")\n",
    "        print(f\"Link: {context['link']}\")\n",
    "        if context['thumbnail']:\n",
    "            print(f\"Thumbnail: {context['thumbnail']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: Run the Graph Query Tool (combining 2.1, 2.2 functionality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Keywords: ['climate', 'change']\n",
      "Writing 50 final_results to file\n",
      "\n",
      "Found 50 relevant nodes:\n",
      "\n",
      "result =  {'ns': 0, 'title': 'Climate change', 'pageid': 5042951, 'size': 317341, 'wordcount': 27919, 'snippet': 'Present-day <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span> includes both global warming—the ongoing increase in global average temperature—and its wider effects on Earth\\'s <span class=\"searchmatch\">climate</span>. <span class=\"searchmatch\">Climate</span> change', 'timestamp': '2024-12-10T03:05:32Z'}\n",
      "result =  {'ns': 0, 'title': 'Climate change denial', 'pageid': 12474403, 'size': 237127, 'wordcount': 22133, 'snippet': '<span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">change</span> denial (also global warming denial) is a form of science denial characterized by rejecting, refusing to acknowledge, disputing, or fighting', 'timestamp': '2024-12-04T00:54:58Z'}\n",
      "result =  {'ns': 0, 'title': 'Climate change mitigation', 'pageid': 2119179, 'size': 229408, 'wordcount': 22636, 'snippet': '<span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">change</span> mitigation (or decarbonisation) is action to limit the greenhouse gases in the atmosphere that cause <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span>. <span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">change</span> mitigation', 'timestamp': '2024-12-09T04:31:50Z'}\n",
      "result =  {'ns': 0, 'title': 'Effects of climate change', 'pageid': 2119174, 'size': 180605, 'wordcount': 20335, 'snippet': 'Effects of <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span> are well documented and growing for Earth\\'s natural environment and human societies. <span class=\"searchmatch\">Changes</span> to the <span class=\"searchmatch\">climate</span> system include an', 'timestamp': '2024-12-08T12:02:41Z'}\n",
      "result =  {'ns': 0, 'title': 'United Nations Framework Convention on Climate Change', 'pageid': 31898, 'size': 95299, 'wordcount': 9711, 'snippet': 'Framework Convention on <span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">Change</span> (UNFCCC) is the UN process for negotiating an agreement to limit dangerous <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span>. It is an international', 'timestamp': '2024-11-18T10:15:56Z'}\n",
      "\n",
      "Wikipedia Context:\n",
      "\n",
      "Result 1:\n",
      "Title: Climate change\n",
      "Description: Human-caused changes to climate on Earth\n",
      "Summary: Present-day climate change includes both global warming—the ongoing increase in global average temperature—and its wider effects on Earth's climate. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The current rise in global temperatures is driven by human activities, especially fossil fuel burning since the Industrial Revolution. Fossil fuel use, deforestation, and some agricultural and industrial practices release greenhouse gases. These gases absorb some of the heat that the Earth radiates after it warms from sunlight, warming the lower atmosphere. Carbon dioxide, the primary greenhouse gas driving global warming, has grown by about 50% and is at levels not seen for millions of years.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Change_in_Average_Temperature_With_Fahrenheit.svg/320px-Change_in_Average_Temperature_With_Fahrenheit.svg.png\n",
      "\n",
      "Result 2:\n",
      "Title: Climate change denial\n",
      "Description: Denial of the scientific consensus on climate change\n",
      "Summary: Climate change denial is a form of science denial characterized by rejecting, refusing to acknowledge, disputing, or fighting the scientific consensus on climate change. Those promoting denial commonly use rhetorical tactics to give the appearance of a scientific controversy where there is none. Climate change denial includes unreasonable doubts about the extent to which climate change is caused by humans, its effects on nature and human society, and the potential of adaptation to global warming by human actions. To a lesser extent, climate change denial can also be implicit when people accept the science but fail to reconcile it with their belief or action. Several studies have analyzed these positions as forms of denialism, pseudoscience, or propaganda.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change_denial\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Inhofe_holding_snowball.jpg/320px-Inhofe_holding_snowball.jpg\n",
      "\n",
      "Result 3:\n",
      "Title: Climate change mitigation\n",
      "Description: Actions to reduce net greenhouse gas emissions to limit climate change\n",
      "Summary: Climate change mitigation (or decarbonisation) is action to limit the greenhouse gases in the atmosphere that cause climate change. Climate change mitigation actions include conserving energy and replacing fossil fuels with clean energy sources. Secondary mitigation strategies include changes to land use and removing carbon dioxide (CO2) from the atmosphere. Current climate change mitigation policies are insufficient as they would still result in global warming of about 2.7 °C by 2100, significantly above the 2015 Paris Agreement's goal of limiting global warming to below 2 °C.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change_mitigation\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Westmill_Solar_2.jpg/320px-Westmill_Solar_2.jpg\n",
      "\n",
      "Result 4:\n",
      "Title: Effects of climate change\n",
      "Description: No Description Available.\n",
      "Summary: Effects of climate change are well documented and growing for Earth's natural environment and human societies. Changes to the climate system include an overall warming trend, changes to precipitation patterns, and more extreme weather. As the climate changes it impacts the natural environment with effects such as more intense forest fires, thawing permafrost, and desertification. These changes impact ecosystems and societies, and can become irreversible once tipping points are crossed. Climate activists are engaged in a range of activities around the world that seek to ameliorate these issues or prevent them from happening.\n",
      "Link: https://en.wikipedia.org/wiki/Effects_of_climate_change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/062821Yreka_Fire_CalFire_-2wiki.jpg/320px-062821Yreka_Fire_CalFire_-2wiki.jpg\n",
      "\n",
      "Result 5:\n",
      "Title: United Nations Framework Convention on Climate Change\n",
      "Description: International environmental treaty\n",
      "Summary: The United Nations Framework Convention on Climate Change (UNFCCC) is the UN process for negotiating an agreement to limit dangerous climate change. It is an international treaty among countries to combat \"dangerous human interference with the climate system\". The main way to do this is limiting the increase in greenhouse gases in the atmosphere. It was signed in 1992 by 154 states at the United Nations Conference on Environment and Development (UNCED), informally known as the Earth Summit, held in Rio de Janeiro. The treaty entered into force on 21 March 1994. \"UNFCCC\" is also the name of the Secretariat charged with supporting the operation of the convention, with offices on the UN Campus in Bonn, Germany.\n",
      "Link: https://en.wikipedia.org/wiki/United_Nations_Framework_Convention_on_Climate_Change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/UNFCCC_Annex_I_Parties%2C_OECD%2C_EU.svg/320px-UNFCCC_Annex_I_Parties%2C_OECD%2C_EU.svg.png\n",
      "\n",
      "Exploring subgraph for node type: Publication\n",
      "Exploring subgraph for node indices: tensor([76606, 94077, 50763, 55702, 88548, 42689, 82757, 12919, 60848, 35890])\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 18 nodes and 8 edges.\n",
      "Edge Type: [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Dataset\n",
      "Exploring subgraph for node indices: tensor([4329, 2323,   97,   61,   65, 2131, 4180, 3752,  293, 4253])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Extracted subgraph with 2263 nodes and 16366 edges.\n",
      "Edge Type: [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: ScienceKeyword\n",
      "Exploring subgraph for node indices: tensor([ 94, 802, 888, 989, 139,  70,  72,  96, 729, 229])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 374 nodes and 738 edges.\n",
      "Edge Type: [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Project\n",
      "Exploring subgraph for node indices: tensor([109, 117,  26,  59,  89, 105, 108, 121,   0, 137])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Extracted subgraph with 197 nodes and 206 edges.\n",
      "Edge Type: [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "\n",
      "Exploring subgraph for node type: Platform\n",
      "Exploring subgraph for node indices: tensor([165, 166, 173, 178, 344,  68, 248, 307, 344, 409])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Extracted subgraph with 1953 nodes and 4553 edges.\n",
      "Edge Type: [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Code to run the Graph Query Tool (pre-LLM summary)\n",
    "#   - Gets the user query, searches the graph, fetches Wikipedia context,\n",
    "#     and explores subgraphs for each of the nodes in the search results.\n",
    "#     The results are displayed in the terminal. \n",
    "# \n",
    "# Output: \n",
    "#   1. Found 50 relevant nodes\n",
    "#   2. Wikipedia Context (top 5 articles)\n",
    "#   3. Subgraph statistics (for each node type)\n",
    "# =======================================================================\n",
    "\n",
    "query = input(\"Enter your query (e.g., 'Find datasets related to climate change projects'): \")\n",
    "# query = \"climate change\" #TODO: remove hardcoded query\n",
    "keywords = extract_keywords(query)\n",
    "print(f\"\\nExtracted Keywords: {keywords}\")\n",
    "\n",
    "# Search the graph with TF-IDF ranking\n",
    "graph_results = search_graph(data, keywords)\n",
    "display_results(graph_results) # 50 results\n",
    "\n",
    "# Fetch Wikipedia context\n",
    "wikipedia_context = fetch_wikipedia_context(keywords)\n",
    "display_wikipedia_context(wikipedia_context)\n",
    "\n",
    "# Explore subgraphs based on the results\n",
    "# TODO: save these subgraphs\n",
    "explore_subgraphs(data, graph_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: LLM Summary using RAG for the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== Using Gemini's API ===============\n",
    "# Define the path to the text file containing the API key\n",
    "file_path = \"/home/karlsimon/CSCI6365/final/gemini_api_key.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    api_key = file.read().strip()\n",
    "# print(api_key)\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Create a model instance (using Gemini 1.5 Flash in this case)\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM-Generated Summary For Query:\n",
      "The provided text focuses on various research studies examining the impacts of climate change and its effects on different aspects of the environment and human society.  Several studies analyze the effects of climate change on:\n",
      "\n",
      "* **Evapotranspiration (ET):**  Research investigates spatial and temporal variations in ET across different land use types (forests, croplands) and projects future changes based on climate and land use modeling.\n",
      "* **Forest fires:** One study uses GRACE satellite data to analyze the relationship between climate change and forest fire frequency in Yunnan, China.\n",
      "* **Marine ecosystems:** An experiment assesses the impact of climate change-induced changes in prey nutritional quality on juvenile Chinook salmon.\n",
      "* **Stratospheric ozone and UV radiation:**  The Environmental Effects Assessment Panel's work is highlighted, focusing on the complex interplay between stratospheric ozone recovery (due to the Montreal Protocol) and climate change, and their combined effects on UV radiation levels.\n",
      "* **Climate model sensitivity:** A study investigates the increased equilibrium climate sensitivity in a specific climate model (CNRM-CM6-1) and attributes it to changes in atmospheric components, particularly cloud radiative responses.\n",
      "* **Human migration:** Research explores the individual-level factors influencing migration decisions in regions highly vulnerable to climate change, emphasizing the interplay of economic, social, and environmental considerations.\n",
      "* **Groundwater levels:**  A study examines the impact of climate change on groundwater table fluctuations in Hungary's Great Hungarian Plain, using hydrological modeling and climate projections.\n",
      "* **Vegetation cycles and agricultural adaptation:**  Research explores the potential of a Land Surface Model to simulate the impacts of climate change on vegetation and soil water content, informing adaptation strategies for agriculture and forestry.\n",
      "* **Flood frequency and design floods:** A study investigates the impact of climate change on the bivariate characteristics of flood peak and volume, using climate model outputs and a hydrological model.\n",
      "* **Agricultural practices:** A systematic review assesses the evidence for synergies between climate change mitigation and adaptation in various agricultural practices, finding that claims of synergies may be overstated.\n",
      "\n",
      "\n",
      "The Wikipedia context provides background information on climate change, its denial, mitigation and effects, and the role of the UNFCCC in international efforts to address it.  Overall, the search results and Wikipedia context paint a picture of the multifaceted nature of climate change, its impacts across various systems, and the ongoing scientific efforts to understand and address it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to summarize combined results using the LLM\n",
    "def summarize_results_with_llm(graph_results, wikipedia_context):\n",
    "    prompt = \"Summarize the following search results and Wikipedia context:\\n\\n\"\n",
    "\n",
    "    # Add graph results to the prompt\n",
    "    prompt += \"Graph Search Results:\\n\"\n",
    "    for node_type, idx, key, value in graph_results[:10]:  # Limit to top 10 results\n",
    "        prompt += f\"- Node Type: {node_type}, Property: {key}, Value: {str(value)[:MAX_VAL_LEN]}...\\n\"\n",
    "\n",
    "    # Add Wikipedia context to the prompt\n",
    "    prompt += \"\\nWikipedia Context:\\n\"\n",
    "    for i, context in enumerate(wikipedia_context, start=1):\n",
    "        prompt += f\"{i}. Title: {context['title']}\\n\"\n",
    "        prompt += f\"   Summary: {context['summary'][:MAX_VAL_LEN]}...\\n\"\n",
    "    \n",
    "    with open(\"prompt_file.txt\", \"w\") as file:\n",
    "        file.write(f\"{prompt}\\n\")\n",
    "\n",
    "    # Call the Gemini model to generate the summary\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Generate an LLM summary of the combined results\n",
    "summary = summarize_results_with_llm(graph_results, wikipedia_context)\n",
    "print(\"LLM-Generated Summary For Query:\")\n",
    "print(summary)\n",
    "\n",
    "with open(\"llm_summary.md\", \"w\") as file:\n",
    "    file.write(f\"{summary}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Node Inspection + Graph Exploration of query results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# specify the priorities to use in value selection\n",
    "def get_priority_properties():\n",
    "    priority_properties = {\n",
    "        'Dataset': ['longName', 'abstract', 'shortName'],\n",
    "        'Publication': ['title', 'abstract'],\n",
    "        'ScienceKeyword': ['name'],\n",
    "        'Instrument': ['longName', 'shortName'],\n",
    "        'Platform': ['longName', 'shortName'],\n",
    "        'Project': ['longName', 'shortName'],\n",
    "        'DataCenter': ['longName', 'shortName']\n",
    "    }\n",
    "    return priority_properties\n",
    "\n",
    "def create_nodes_of_interest(graph_results, max_per_type=3):\n",
    "    nodes_by_type = defaultdict(list)\n",
    "    for node_type, idx, key, value in graph_results:\n",
    "        nodes_by_type[node_type].append((idx, node_type, key, value))\n",
    "\n",
    "    # Select up to max_per_type nodes for each type\n",
    "    nodes_of_interest = []\n",
    "    for node_type, nodes in nodes_by_type.items():\n",
    "        nodes_of_interest.extend(random.sample(nodes, min(max_per_type, len(nodes))))\n",
    "\n",
    "    return nodes_of_interest\n",
    "\n",
    "def explore_subgraph_nodes(data, node_type, node_id, num_hops=2, max_per_type=3):\n",
    "    priority_properties = get_priority_properties()\n",
    "    subset, edge_index, edge_types = get_subgraph(data, node_type, torch.tensor([node_id]), num_hops=num_hops)\n",
    "\n",
    "    if subset is None:\n",
    "        return []\n",
    "\n",
    "    # Map node indices to their types and values\n",
    "    subgraph_nodes = []\n",
    "    # print(f\"Number of nodes in subgraph for node_id: {node_id} = {len(subset)} | subset = {subset}\") #UNCOMMENT FOR DEBUGGING\n",
    "\n",
    "    for sub_id in subset.tolist():\n",
    "        for sub_node_type in data.node_types:\n",
    "            num_nodes = data[sub_node_type].num_nodes\n",
    "            if sub_id < num_nodes:\n",
    "                # Attempt to find a meaningful property\n",
    "                value = None\n",
    "                for prop in priority_properties.get(sub_node_type, []):\n",
    "                    if prop in data[sub_node_type] and len(data[sub_node_type][prop]) > sub_id:\n",
    "                        value = data[sub_node_type][prop][sub_id]\n",
    "                        break\n",
    "                if value is None:  # Fallback to globalId or indicate no value\n",
    "                    value = data[sub_node_type].get('globalId', ['No value'])[sub_id] if 'globalId' in data[sub_node_type] else 'No value'\n",
    "                \n",
    "                subgraph_nodes.append((sub_id, sub_node_type, value))\n",
    "\n",
    "    # Group by node type and select a random subset of up to max_per_type nodes\n",
    "    nodes_by_type = defaultdict(list)\n",
    "    for node_id, node_type, value in subgraph_nodes:\n",
    "        nodes_by_type[node_type].append((node_id, node_type, value))\n",
    "\n",
    "    exploration_list = []\n",
    "    for node_type, nodes in nodes_by_type.items():\n",
    "        exploration_list.extend(random.sample(nodes, min(max_per_type, len(nodes))))\n",
    "\n",
    "    return exploration_list\n",
    "\n",
    "def write_exploration_to_file(data, graph_results, filename=\"graph_exploration.txt\"):\n",
    "    nodes_of_interest = create_nodes_of_interest(graph_results)\n",
    "\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(\"=== Nodes of Interest ===\\n\")\n",
    "        for idx, node_type, key, value in nodes_of_interest:\n",
    "            value_str = str(value)\n",
    "            display_value = value_str[:MAX_VAL_LEN] + (\"...\" if len(value_str) > MAX_VAL_LEN else \"\")\n",
    "            file.write(f\"ID: {idx}, Type: {node_type}, Key: {key}, Value: {display_value}\\n\")\n",
    "\n",
    "        file.write(\"\\n=== Subgraph Exploration ===\\n\")\n",
    "        for idx, node_type, key, value in nodes_of_interest:\n",
    "            file.write(f\"\\nExploring Subgraph for Node ID: {idx} (Type: {node_type})\\n\")\n",
    "            subgraph_nodes = explore_subgraph_nodes(data, node_type, idx)\n",
    "            for sub_id, sub_node_type, sub_value in subgraph_nodes:\n",
    "                sub_value_str = str(sub_value)\n",
    "                display_sub_value = sub_value_str[:MAX_VAL_LEN] + (\"...\" if len(sub_value_str) > MAX_VAL_LEN else \"\")\n",
    "                file.write(f\"  - ID: {sub_id}, Type: {sub_node_type}, Value: {display_sub_value}\\n\")\n",
    "\n",
    "    print(f\"\\nExploration results written to '{filename}'.\")\n",
    "\n",
    "def interactive_exploration(data):\n",
    "    priority_properties = get_priority_properties()\n",
    "    while True:\n",
    "        choice = input(\"\\nEnter a Node ID to explore further (or 'q' to quit): \")\n",
    "        if choice.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            node_id = int(choice)\n",
    "            node_type = input(\"Enter the Node Type (e.g., Dataset, ScienceKeyword, Instrument): \").strip()\n",
    "\n",
    "            # Validate the node type\n",
    "            if node_type not in data.node_types:\n",
    "                print(f\"Invalid node type '{node_type}'. Available types: {data.node_types}\")\n",
    "                continue\n",
    "\n",
    "            num_nodes = data[node_type].num_nodes\n",
    "            if node_id >= num_nodes:\n",
    "                print(f\"No node with ID: {node_id} in type '{node_type}'.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nSelected Node ID: {node_id} (Type: {node_type})\")\n",
    "            action = input(\"Enter 'wiki' to fetch Wikipedia context or 'subgraph' to explore subgraph of node: \").lower()\n",
    "\n",
    "            if action == 'wiki':\n",
    "                # Select a meaningful property using priority_properties\n",
    "                value = None\n",
    "                for prop in priority_properties.get(node_type, []):\n",
    "                    if prop in data[node_type] and len(data[node_type][prop]) > node_id:\n",
    "                        value = data[node_type][prop][node_id]\n",
    "                        break\n",
    "\n",
    "                if value is None:\n",
    "                    value = 'No value'\n",
    "\n",
    "                wikipedia_context = fetch_wikipedia_context([str(value)])\n",
    "                print(\"The prompt used for the Wikipedia context =\", str(value))\n",
    "                display_wikipedia_context(wikipedia_context)\n",
    "\n",
    "            elif action == 'subgraph':\n",
    "                subgraph_nodes = explore_subgraph_nodes(data, node_type, node_id)\n",
    "                print(f\"\\nSubgraph for Node ID: {node_id} (Type: {node_type})\")\n",
    "                for sub_id, sub_node_type, sub_value in subgraph_nodes:\n",
    "                    sub_value_str = str(sub_value)\n",
    "                    display_sub_value = sub_value_str[:MAX_VAL_LEN] + (\"...\" if len(sub_value_str) > MAX_VAL_LEN else \"\")\n",
    "                    print(f\"  - ID: {sub_id}, Type: {sub_node_type}, Value: {display_sub_value}\")\n",
    "\n",
    "            else:\n",
    "                print(\"Invalid action. Please enter 'wiki' or 'subgraph'.\")\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Invalid Node ID. Please enter a valid number.\")\n",
    "\n",
    "def run_exploration_tool(data, graph_results):\n",
    "    write_exploration_to_file(data, graph_results)\n",
    "    interactive_exploration(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploration results written to 'graph_exploration.txt'.\n",
      "\n",
      "Selected Node ID: 6 (Type: Project)\n",
      "result =  {'ns': 0, 'title': 'Earth Observing System', 'pageid': 471278, 'size': 29782, 'wordcount': 2019, 'snippet': 'The <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) is a program of NASA comprising a series of artificial satellite missions and scientific instruments in <span class=\"searchmatch\">Earth</span> orbit designed', 'timestamp': '2024-11-01T14:37:17Z'}\n",
      "result =  {'ns': 0, 'title': 'Aqua (satellite)', 'pageid': 830227, 'size': 14120, 'wordcount': 1300, 'snippet': 'component of the <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) preceded by Terra (launched 1999) and followed by Aura (launched 2004). The name &quot;<span class=\"searchmatch\">Aqua</span>&quot; comes from the Latin', 'timestamp': '2024-12-09T23:30:06Z'}\n",
      "result =  {'ns': 0, 'title': 'Terra (satellite)', 'pageid': 470910, 'size': 12209, 'wordcount': 1013, 'snippet': ' It is the flagship of the <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) and the first satellite of the <span class=\"searchmatch\">system</span> which was followed by <span class=\"searchmatch\">Aqua</span> (launched in 2002) and Aura', 'timestamp': '2024-12-09T23:31:38Z'}\n",
      "result =  {'ns': 0, 'title': 'List of Earth observation satellites', 'pageid': 1934667, 'size': 38842, 'wordcount': 1190, 'snippet': 'IAU. 12 August 2018. Retrieved 25 September 2021. &quot;qua <span class=\"searchmatch\">Earth</span>-<span class=\"searchmatch\">observing</span> satellite mission&quot;. <span class=\"searchmatch\">Aqua</span>.nasa.gov. Retrieved 10 August 2017. &quot;The Aura Mission&quot;', 'timestamp': '2024-12-05T03:30:45Z'}\n",
      "result =  {'ns': 0, 'title': 'Aura (satellite)', 'pageid': 827963, 'size': 14249, 'wordcount': 1240, 'snippet': 'It is the third major component of the <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) following on Terra (launched 1999) and <span class=\"searchmatch\">Aqua</span> (launched 2002). Aura follows on from the', 'timestamp': '2024-07-29T23:05:19Z'}\n",
      "The prompt used for the Wikipedia context = Earth Observing System (EOS), Aqua\n",
      "\n",
      "Wikipedia Context:\n",
      "\n",
      "Result 1:\n",
      "Title: Earth Observing System\n",
      "Description: NASA program involving satellites\n",
      "Summary: The Earth Observing System (EOS) is a program of NASA comprising a series of artificial satellite missions and scientific instruments in Earth orbit designed for long-term global observations of the land surface, biosphere, atmosphere, and oceans. Since the early 1970s, NASA has been developing its Earth Observing System, launching a series of Landsat satellites in the decade. Some of the first included passive microwave imaging in 1972 through the Nimbus 5 satellite. Following the launch of various satellite missions, the conception of the program began in the late 1980s and expanded rapidly through the 1990s. Since the inception of the program, it has continued to develop, including; land, sea, radiation and atmosphere. Collected in a system known as EOSDIS, NASA uses this data in order to study the progression and changes in the biosphere of Earth. The main focus of this data collection surrounds climatic science. The program is the centrepiece of NASA's Earth Science Enterprise.\n",
      "Link: https://en.wikipedia.org/wiki/Earth_Observing_System\n",
      "\n",
      "Result 2:\n",
      "Title: Aqua (satellite)\n",
      "Description: NASA scientific research satellite (2002–Present)\n",
      "Summary: Aqua is a NASA scientific research satellite in orbit around the Earth, studying the precipitation, evaporation, and cycling of water. It is the second major component of the Earth Observing System (EOS) preceded by Terra and followed by Aura.\n",
      "Link: https://en.wikipedia.org/wiki/Aqua_(satellite)\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Aqua_spacecraft_model.png/320px-Aqua_spacecraft_model.png\n",
      "\n",
      "Result 3:\n",
      "Title: Terra (satellite)\n",
      "Description: NASA climate research satellite (1999–Present)\n",
      "Summary: Terra is a multi-national scientific research satellite operated by NASA in a Sun-synchronous orbit around the Earth. It takes simultaneous measurements of Earth's atmosphere, land, and water to understand how Earth is changing and to identify the consequences for life on Earth. It is the flagship of the Earth Observing System (EOS) and the first satellite of the system which was followed by Aqua and Aura. Terra was launched in 1999.\n",
      "Link: https://en.wikipedia.org/wiki/Terra_(satellite)\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Terra_spacecraft_model.png/320px-Terra_spacecraft_model.png\n",
      "\n",
      "Result 4:\n",
      "Title: List of Earth observation satellites\n",
      "Description: No Description Available.\n",
      "Summary: Earth observation satellites are Earth-orbiting spacecraft with sensors used to collect imagery and measurements of the surface of the earth. These satellites are used to monitor short-term weather, long-term climate change, natural disasters. Earth observations satellites provide information for research subjects that benefit from looking at Earth’s surface from above. Types of sensors on these satellites include passive and active remote sensors. Sensors on Earth observation satellites often take measurements of emitted energy over some portion of the electromagnetic spectrum.\n",
      "Link: https://en.wikipedia.org/wiki/List_of_Earth_observation_satellites\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/Earth_from_Space.jpg/320px-Earth_from_Space.jpg\n",
      "\n",
      "Result 5:\n",
      "Title: Aura (satellite)\n",
      "Description: NASA Earth observation satellite (2004–Present)\n",
      "Summary: Aura is a multi-national NASA scientific research satellite in orbit around the Earth, studying the Earth's ozone layer, air quality and climate. It is the third major component of the Earth Observing System (EOS) following on Terra and Aqua. Aura follows on from the Upper Atmosphere Research Satellite (UARS). Aura is a joint mission between NASA, the Netherlands, Finland, and the U.K. The Aura spacecraft is healthy and is expected to operate until at least 2023, likely beyond.\n",
      "Link: https://en.wikipedia.org/wiki/Aura_(satellite)\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/8/88/Aura_spacecraft.png\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Code to run Node Inspection +Graph Exploration (post-LLM summary)\n",
    "#   - Write relevant nodes and their subgraphs to graph_exploration.txt. \n",
    "#     User can then inspect output and examine specific node further by looking\n",
    "#     at graph_exploration.txt. If any node is intersesting, user can obtain \n",
    "#     RAG summary of that specific node and view in combined_summary.md.\n",
    "# \n",
    "# Output: \n",
    "#   1. relevant nodes and their subgraphs written to graph_exploration.txt\n",
    "#   2. interactive exploration tool\n",
    "#       2.1 Enter node_id and node_type to explore further (or 'q' to quit)\n",
    "#       2.2 Enter 'wiki' to fetch Wikipedia context or 'subgraph' to explore subgraph\n",
    "#       2.3 NOTE: may need to enter q for results to show in terminal.\n",
    "#\n",
    "# Known Limitation:\n",
    "#   - If the node text field is too complex (eg. Sounder SIPS: AQUA AIRS \n",
    "#     IR-only Level 3 CLIMCAPS:), wikipedia context fetch may not work.\n",
    "# =======================================================================\n",
    "\n",
    "# Assumes `graph_results` contains the 50 search results\n",
    "run_exploration_tool(data, graph_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Using RAG-integrated Node Inspection + Graph Exploration\n",
    "- Similar to Step 4, except now instead of just outputting to terminal, user receives single LLM response with supplied external\n",
    "information from wikipedia, graph, and Google Scholar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display nodes of interest and their subgraphs in the terminal\n",
    "def display_nodes_and_subgraphs(data, graph_results):\n",
    "    nodes_of_interest = create_nodes_of_interest(graph_results)\n",
    "\n",
    "    print(\"\\n=== Nodes of Interest ===\")\n",
    "    for idx, node_type, key, value in nodes_of_interest:\n",
    "        print(f\"ID: {idx}, Type: {node_type}, Key: {key}, Value: {str(value)[:MAX_VAL_LEN]}\")\n",
    "    \n",
    "    print(\"\\n=== Subgraph Exploration ===\")\n",
    "    for idx, node_type, key, value in nodes_of_interest:\n",
    "        print(f\"\\nExploring Subgraph for Node ID: {idx} (Type: {node_type})\")\n",
    "        subgraph_nodes = explore_subgraph_nodes(data, node_type, idx)\n",
    "        for sub_id, sub_node_type, sub_value in subgraph_nodes:\n",
    "            sub_value_str = str(sub_value)[:MAX_VAL_LEN]\n",
    "            display_sub_value = sub_value_str + (\"...\" if len(sub_value_str) > MAX_VAL_LEN else \"\")\n",
    "            print(f\"  - ID: {sub_id}, Type: {sub_node_type}, Value: {display_sub_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_combined_rag_summary(node_type, node_value, subgraph_nodes, scholarly_query):\n",
    "    # Fetch Wikipedia context\n",
    "    wikipedia_context = fetch_wikipedia_context([str(node_value)])\n",
    "    \n",
    "    # Fetch scholarly articles\n",
    "    scholarly_results = []\n",
    "    try:\n",
    "        search_query = scholarly.search_pubs(scholarly_query)\n",
    "        for _ in range(3):  # Limit to top 3 results\n",
    "            paper = next(search_query)\n",
    "            scholarly_results.append({\n",
    "                'title': paper['bib']['title'],\n",
    "                'abstract': paper['bib'].get('abstract', 'No abstract available'),\n",
    "                'url': paper.get('pub_url', 'No URL available')\n",
    "            })\n",
    "    except StopIteration:\n",
    "        scholarly_results.append({'title': 'No results found on Google Scholar', 'abstract': '', 'url': ''})\n",
    "    except Exception as e:\n",
    "        scholarly_results.append({'title': f\"Error fetching data: {e}\", 'abstract': '', 'url': ''})\n",
    "\n",
    "    # Format Wikipedia context\n",
    "    wiki_text = \"\"\n",
    "    if wikipedia_context:\n",
    "        for result in wikipedia_context:\n",
    "            wiki_text += f\"- {result['title']}: {result['summary']}\\n\"\n",
    "\n",
    "    # Format subgraph nodes\n",
    "    subgraph_text = \"\"\n",
    "    for sub_id, sub_node_type, sub_value in subgraph_nodes:\n",
    "        subgraph_text += f\"- ID: {sub_id}, Type: {sub_node_type}, Value: {str(sub_value)[:300]}\\n\"\n",
    "\n",
    "    # Format scholarly articles\n",
    "    scholarly_text = \"\"\n",
    "    for paper in scholarly_results:\n",
    "        scholarly_text += f\"- Title: {paper['title']}\\n  Abstract: {paper['abstract']}\\n  URL: {paper['url']}\\n\"\n",
    "\n",
    "    # Create the prompt\n",
    "    prompt = f\"\"\"\n",
    "                You are analyzing the node '{node_value}' (Type: {node_type}). Use the following information to generate an integrated explanation that highlights relationships and insights:\n",
    "\n",
    "                **Wikipedia Context**:\n",
    "                {wiki_text}\n",
    "\n",
    "                **Subgraph Nodes**:\n",
    "                {subgraph_text}\n",
    "\n",
    "                **Scholarly Articles**:\n",
    "                {scholarly_text}\n",
    "\n",
    "                **Instructions**:\n",
    "                - Combine insights from all sources into a unified explanation.\n",
    "                - Reference specific datasets, instruments, projects, and scholarly articles by their IDs or names.\n",
    "                - Explain how these elements are interconnected and what they reveal about '{node_value}'.\n",
    "            \"\"\"\n",
    "\n",
    "    # Call the Gemini model to generate the summary\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def interactive_rag_exploration(data):\n",
    "    priority_properties = get_priority_properties()\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"\\nEnter a Node ID to explore further (or 'q' to quit): \")\n",
    "        if choice.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            node_id = int(choice)\n",
    "            node_type = input(\"Enter the Node Type (e.g., Dataset, ScienceKeyword, Instrument): \").strip()\n",
    "\n",
    "            # Validate node type\n",
    "            if node_type not in data.node_types:\n",
    "                print(f\"Invalid node type '{node_type}'. Available types: {data.node_types}\")\n",
    "                continue\n",
    "\n",
    "            # Check if the node ID is valid for the given type\n",
    "            num_nodes = data[node_type].num_nodes\n",
    "            if node_id >= num_nodes:\n",
    "                print(f\"No node with ID: {node_id} in type '{node_type}'.\")\n",
    "                continue\n",
    "\n",
    "            # Get the node value\n",
    "            value = None\n",
    "            for prop in priority_properties.get(node_type, []):\n",
    "                if prop in data[node_type] and len(data[node_type][prop]) > node_id:\n",
    "                    value = data[node_type][prop][node_id]\n",
    "                    break\n",
    "            if value is None:\n",
    "                value = 'No value'\n",
    "\n",
    "            # Get subgraph nodes\n",
    "            subgraph_nodes = explore_subgraph_nodes(data, node_type, node_id)\n",
    "\n",
    "            # Get scholarly query based on node value\n",
    "            scholarly_query = str(value)\n",
    "\n",
    "            # Generate combined RAG summary\n",
    "            print(\"\\nGenerating detailed LLM summary using Wikipedia, subgraph nodes, and scholarly articles...\")\n",
    "            summary = generate_combined_rag_summary(node_type, value, subgraph_nodes, scholarly_query)\n",
    "            print(\"\\n=== Combined LLM Summary ===\")\n",
    "            print(summary)\n",
    "\n",
    "            # write the summary to an .md file\n",
    "            with open(\"combined_summary.md\", \"w\") as file:\n",
    "                # file.write(\"LLM-Generated Summary for the node with ID: \" + str(node_id) + \", Type: \" + str(node_type) + \", and Value: \" + str(value) + \"\\n\")\n",
    "                file.write(summary)\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Invalid Node ID. Please enter a valid number.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Nodes of Interest ===\n",
      "ID: 50763, Type: Publication, Key: abstract, Value: Global climate change is projected to raise global temperatures by 3.3-5.7 °C by 2100, resulting in changes in species composition, abundance, and nutritional quality of organisms at the base of the marine food web. Predicted increases in prey availability and reductions in prey nutritional quality under climate warming in certain marine systems are expected to impact higher trophic levels, such as fish and humans. There is limited knowledge of the interplay between food quantity and quality under warming, specifically when food availability is high, but quality is low. Here, we conducted an experiment assessing the effects of food quality (fatty acid composition and ratios) on juvenile Chinook salmon's (Oncorhynchus tshawytscha) body and nutritional condition, specifically focusing on RNA:DNA ratio, Fulton's K, growth, mortality and their fatty acid composition. Experimental diets represented three different climate change scenarios with 1) a present-day diet (Euphausia pacifica), 2) \n",
      "ID: 88548, Type: Publication, Key: abstract, Value: The equilibrium climate sensitivity (ECS) in the latest version of CNRM climate model, CNRM-CM6-1, and in its high-resolution counterpart, CNRM-CM6-1-HR, is significantly larger than in the previous version (CNRM-CM5.1). The traceability of this climate sensitivity change is investigated using coupled ocean-atmosphere model climate change simulations. These simulations show that the increase in ECS is the result of changes in the atmospheric component. A particular attention is paid to the method used to decompose the equilibrium temperature response difference, by using a linearized decomposition of the individual radiative agents diagnosed by a radiative kernel technique. The climate sensitivity increase is primarily due to the cloud radiative responses, with a predominant contribution of the tropical longwave response (including both feedback and forcing adjustment) and a significant contribution of the extratropical and tropical shortwave feedback changes. A series of stand-alone a\n",
      "ID: 76606, Type: Publication, Key: abstract, Value: Assessment of actual evapotranspiration (ET) is essential as it controls the exchange of water and heat energy between the atmosphere and land surface. ET also influences the available water resources and assists in the crop water assessment in agricultural areas. This study involves the assessment of spatial distribution of seasonal and annual ET using Surface Energy Balance Algorithm for Land (SEBAL) and provides an estimation of future changes in ET due to land use and climate change for a portion of the Narmada river basin in Central India. Climate change effects on future ET are assessed using the ACCESS1-0 model of CMIP5. A Markov Chain model estimated future land use based on the probability of changes in the past. The ET analysis is carried out for the years 2009–2011. The results indicate variation in the seasonal ET with the changed land use. High ET is observed over forest areas and crop lands, but ET decreases over crop lands after harvest. The overall annual ET is high ove\n",
      "ID: 3752, Type: Dataset, Key: abstract, Value: CAL_LID_L3_Tropospheric_APro_CloudySkyOpaque-Standard-V4-20 is the Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observation (CALIPSO) Lidar Level 3 Tropospheric Aerosol Profiles, Cloudy Sky Opaque Data, Standard Version 4-20 data product. This data product was collected using the Cloud-Aerosol Lidar with Orthogonal Polarization (CALIOP) instrument. Data generation and distribution of this V4.20 product ended on July 1, 2020, to support a change in the operating system of the CALIPSO production clusters. The V4.21 data product covers July 1, 2020, to current. \n",
      "\n",
      "The CALIPSO lidar level 3 aerosol data product reports monthly mean profiles of aerosol optical properties on a uniform spatial grid. It is intended to be a tropospheric product, so data are only reported below altitudes of 12km. All level 3 parameters are derived from the version 4.20 CALIOP level 2 aerosol profile product and have been quality screened before averaging. The primary quantities reported are vertical pr\n",
      "ID: 293, Type: Dataset, Key: abstract, Value: Goddard’s LiDAR, Hyperspectral, and Thermal Imager (G-LiHT(https://gliht.gsfc.nasa.gov/)) mission utilizes a portable, airborne imaging system that aims to simultaneously map the composition, structure, and function of terrestrial ecosystems. G-LiHT primarily focuses on a broad diversity of forest communities and ecoregions in North America, mapping aerial swaths over the Conterminous United States (CONUS), Alaska, Puerto Rico, and Mexico.\n",
      "\n",
      "The purpose of G-LiHT’s Canopy Height Model Keyhole Markup Language (KML) data product (GLCHMK) is to provide LiDAR-derived maximum canopy height and canopy variability information to aid in the study and analysis of biodiversity and climate change. Scientists at NASA’s Goddard Space Flight Center began collecting data over locally-defined areas in 2011 and that the collection will continue to grow as aerial campaigns are flown and processed.  \n",
      "\n",
      "GLCHMK data are processed as a Google Earth overlay KML file at a nominal 1 meter spatial resolution \n",
      "ID: 4329, Type: Dataset, Key: abstract, Value: TOLNet_ECCC_Data is the lidar data collected by the Autonomous Mobile Ozone LIDAR instrument for Tropospheric Experiments (AMOLITE) lidar at Environment and Climate Change Canada (ECCC) in Toronto, Canada as part of the Tropospheric Ozone Lidar Network (TOLNet). Data collection for this product is ongoing.\n",
      "\n",
      "In the troposphere, ozone is considered a pollutant and is important to understand due to its harmful effects on human health and vegetation. Tropospheric ozone is also significant for its impact on climate as a greenhouse gas. Operating since 2011, TOLNet is an interagency collaboration between NASA, NOAA, and the EPA designed to perform studies of air quality and atmospheric modeling as well as validation and interpretation of satellite observations. TOLNet is currently comprised of six Differential Absorption Lidars (DIAL). Each of the lidars are unique, and some have had a long history of ozone observations prior to joining the network. Five lidars are mobile systems that can \n",
      "ID: 96, Type: ScienceKeyword, Key: name, Value: COUPLED CLIMATE MODELS\n",
      "ID: 139, Type: ScienceKeyword, Key: name, Value: CHANGE DETECTION SERVICES\n",
      "ID: 802, Type: ScienceKeyword, Key: name, Value: CLIMATE INDICATORS\n",
      "ID: 108, Type: Project, Key: longName, Value: National Climate Assessment - Land Data Assimilation System\n",
      "ID: 89, Type: Project, Key: longName, Value: Solar Radiation and Climate Experiment\n",
      "ID: 26, Type: Project, Key: longName, Value: Distributed Info. Services for Climate/Ocean Prod./Visualizations for Earth Res.\n",
      "ID: 344, Type: Platform, Key: longName, Value: CLIMATE MODELS\n",
      "ID: 166, Type: Platform, Key: longName, Value: NCEP Climate Forecast System Version 2\n",
      "ID: 178, Type: Platform, Key: longName, Value: Constellation Observing System for Meteorology, Ionosphere and Climate\n",
      "\n",
      "=== Subgraph Exploration ===\n",
      "\n",
      "Exploring Subgraph for Node ID: 50763 (Type: Publication)\n",
      "  - ID: 50763, Type: Publication, Value: Constraint of satellite CO2 retrieval on the global carbon cycle from a Chinese atmospheric inversion system\n",
      "\n",
      "Exploring Subgraph for Node ID: 88548 (Type: Publication)\n",
      "  - ID: 88548, Type: Publication, Value: KrigR-a tool for downloading and statistically downscaling climate reanalysis data\n",
      "\n",
      "Exploring Subgraph for Node ID: 76606 (Type: Publication)\n",
      "  - ID: 76606, Type: Publication, Value: Impact of transport model resolution and a priori assumptions on inverse modeling of Swiss F-gas emissions\n",
      "\n",
      "Exploring Subgraph for Node ID: 3752 (Type: Dataset)\n",
      "  - ID: 492, Type: Dataset, Value: SPURS-1 research vessel Thermosalinograph series data for N. Atlantic cruises\n",
      "  - ID: 6, Type: Dataset, Value: GHRSST Level 2P sub-skin Sea Surface Temperature from the Advanced Very High Resolution Radiometer (AVHRR) on Metop satellites (currently Metop-A) (GDS V2) produced by OSI SAF\n",
      "  - ID: 54, Type: Dataset, Value: ASTER Level 1 precision terrain corrected registered at-sensor radiance V003\n",
      "  - ID: 54, Type: DataCenter, Value: Atmospheric Science Data Center, Science Directorate, Langley Research Center, NASA\n",
      "  - ID: 6, Type: DataCenter, Value: Land Processes Distributed Active Archive Center\n",
      "  - ID: 6, Type: Project, Value: Earth Observing System (EOS), Aqua\n",
      "  - ID: 54, Type: Project, Value: East Pacific Origins and Characteristics of Hurricanes\n",
      "  - ID: 6, Type: Platform, Value: Meteorological Operational Satellite - A\n",
      "  - ID: 54, Type: Platform, Value: R/V Sarmiento De Gamboa (SPAIN)\n",
      "  - ID: 778, Type: Instrument, Value: Paroscientific Pressure Altimeter\n",
      "  - ID: 500, Type: Instrument, Value: Airborne Raman Ozone, Temperature, and Aerosol Lidar\n",
      "  - ID: 497, Type: Instrument, Value: 2B Technologies Nitric Oxide Monitor\n",
      "  - ID: 497, Type: ScienceKeyword, Value: PRECIPITATION RATE\n",
      "  - ID: 940, Type: ScienceKeyword, Value: LENGTH OF FREEZE FREE PERIOD\n",
      "  - ID: 500, Type: ScienceKeyword, Value: ICE PELLETS\n",
      "  - ID: 778, Type: Publication, Value: Modeling of the Influence of Sea Ice Cycle and Langmuir Circulation on the Upper Ocean Mixed Layer Depth and Freshwater Distribution at the West Antarctic Peninsula\n",
      "  - ID: 940, Type: Publication, Value: An improved optical flow method to estimate Arctic sea ice velocity (winter 20142016)\n",
      "  - ID: 967, Type: Publication, Value: Estimation of flood-damaged cropland area using a convolutional neural network\n",
      "\n",
      "Exploring Subgraph for Node ID: 293 (Type: Dataset)\n",
      "  - ID: 96, Type: Dataset, Value: MODIS/Terra+Aqua BRDF/Albedo White Sky Albedo NIR Daily L3 Global 30ArcSec CMG V061\n",
      "  - ID: 2164, Type: Dataset, Value: AIRS/Aqua L2 Support Retrieval (AIRS+AMSU+HSB) V006 (AIRH2SUP) at GES DISC\n",
      "  - ID: 1850, Type: Dataset, Value: Aqua/AIRS L3 Monthly Standard Physical Retrieval (AIRS+AMSU+HSB) 1 degree x 1 degree V7.0 at GES DISC\n",
      "  - ID: 22, Type: DataCenter, Value: Global Ecosystem Dynamics Investigation, Goddard Space Flight Center, NASA\n",
      "  - ID: 12, Type: DataCenter, Value: Shuttle Radar Topography Mission Project Office, Jet Propulsion Laboratory, NASA\n",
      "  - ID: 93, Type: DataCenter, Value: N/A\n",
      "  - ID: 26, Type: Project, Value: Distributed Info. Services for Climate/Ocean Prod./Visualizations for Earth Res.\n",
      "  - ID: 299, Type: Project, Value: Pre-IceBridge NASA ATM Suite Greenland 2000\n",
      "  - ID: 220, Type: Project, Value: ICECAP University of Texas, Antarctica 2011\n",
      "  - ID: 347, Type: Platform, Value: \n",
      "  - ID: 20, Type: Platform, Value: Suomi National Polar-orbiting Partnership\n",
      "  - ID: 42, Type: Platform, Value: \n",
      "  - ID: 51, Type: Instrument, Value: Current Meter\n",
      "  - ID: 313, Type: Instrument, Value: \n",
      "  - ID: 401, Type: Instrument, Value: Tandem Differential Mobility Analyzer\n",
      "  - ID: 191, Type: ScienceKeyword, Value: ANIMAL MANURE AND WASTE\n",
      "  - ID: 698, Type: ScienceKeyword, Value: NUTRIENT CYCLING\n",
      "  - ID: 469, Type: ScienceKeyword, Value: CLOUD TOP TEMPERATURE\n",
      "  - ID: 321, Type: Publication, Value: A Source Apportionment and Emission Scenario Assessment of PM2.5 and O3Related Health Impacts in G20 Countries\n",
      "  - ID: 2503, Type: Publication, Value: Separation and attribution of impacts of changes in land use and climate on hydrological processes\n",
      "  - ID: 1, Type: Publication, Value: SW2D-GPU: A two-dimensional shallow water model accelerated by GPGPU\n",
      "\n",
      "Exploring Subgraph for Node ID: 4329 (Type: Dataset)\n",
      "  - ID: 54, Type: Dataset, Value: ASTER Level 1 precision terrain corrected registered at-sensor radiance V003\n",
      "  - ID: 1107, Type: Dataset, Value: High Altitude MMIC Sounding Radiometer (HAMSR) EPOCH V1\n",
      "  - ID: 6, Type: Dataset, Value: GHRSST Level 2P sub-skin Sea Surface Temperature from the Advanced Very High Resolution Radiometer (AVHRR) on Metop satellites (currently Metop-A) (GDS V2) produced by OSI SAF\n",
      "  - ID: 54, Type: DataCenter, Value: Atmospheric Science Data Center, Science Directorate, Langley Research Center, NASA\n",
      "  - ID: 6, Type: DataCenter, Value: Land Processes Distributed Active Archive Center\n",
      "  - ID: 6, Type: Project, Value: Earth Observing System (EOS), Aqua\n",
      "  - ID: 54, Type: Project, Value: East Pacific Origins and Characteristics of Hurricanes\n",
      "  - ID: 6, Type: Platform, Value: Meteorological Operational Satellite - A\n",
      "  - ID: 54, Type: Platform, Value: R/V Sarmiento De Gamboa (SPAIN)\n",
      "  - ID: 492, Type: Instrument, Value: Wide Field Camera\n",
      "  - ID: 497, Type: Instrument, Value: 2B Technologies Nitric Oxide Monitor\n",
      "  - ID: 500, Type: Instrument, Value: Airborne Raman Ozone, Temperature, and Aerosol Lidar\n",
      "  - ID: 497, Type: ScienceKeyword, Value: PRECIPITATION RATE\n",
      "  - ID: 940, Type: ScienceKeyword, Value: LENGTH OF FREEZE FREE PERIOD\n",
      "  - ID: 54, Type: ScienceKeyword, Value: MARINE ADVISORIES\n",
      "  - ID: 778, Type: Publication, Value: Modeling of the Influence of Sea Ice Cycle and Langmuir Circulation on the Upper Ocean Mixed Layer Depth and Freshwater Distribution at the West Antarctic Peninsula\n",
      "  - ID: 1107, Type: Publication, Value: Mapping Bioclimatic Indices by Downscaling MODIS Land Surface\n",
      "  - ID: 500, Type: Publication, Value: The implications of maintaining Earth's hemispheric albedo symmetry for\n",
      "\n",
      "Exploring Subgraph for Node ID: 96 (Type: ScienceKeyword)\n",
      "  - ID: 3313, Type: Dataset, Value: ECCO Ocean Temperature and Salinity - Monthly Mean llc90 Grid (Version 4 Release 4)\n",
      "  - ID: 3399, Type: Dataset, Value: ECCO Sea-Ice and Snow Concentration and Thickness - Snapshot llc90 Grid (Version 4 Release 4)\n",
      "  - ID: 5188, Type: Dataset, Value: Soil Moisture Active Passive (SMAP) L4 Carbon Ancillary MODIS FPAR Preprocessor Output Log Files V001\n",
      "  - ID: 96, Type: DataCenter, Value: N/A\n",
      "  - ID: 90, Type: DataCenter, Value: N/A\n",
      "  - ID: 96, Type: Project, Value: NASA Energy and Water Study\n",
      "  - ID: 90, Type: Project, Value: Cloud Absorption Radiometer\n",
      "  - ID: 96, Type: Platform, Value: \n",
      "  - ID: 90, Type: Platform, Value: FIXED OBSERVATION STATIONS\n",
      "  - ID: 96, Type: Instrument, Value: Forward Scattering Spectrometer Probe\n",
      "  - ID: 90, Type: Instrument, Value: Inertial Navigation System\n",
      "  - ID: 96, Type: ScienceKeyword, Value: COUPLED CLIMATE MODELS\n",
      "  - ID: 90, Type: ScienceKeyword, Value: MODELS\n",
      "  - ID: 3449, Type: Publication, Value: Large wildfire driven increases in nighttime fire activity observed\n",
      "  - ID: 3397, Type: Publication, Value: Convergence of atmospheric and North Atlantic carbon dioxide trends on multidecadal timescales\n",
      "  - ID: 3568, Type: Publication, Value: Changes in Vegetation Cover of the Arctic National Wildlife Refuge Estimated from MODIS Greenness Trends, 2000–18\n",
      "\n",
      "Exploring Subgraph for Node ID: 139 (Type: ScienceKeyword)\n",
      "  - ID: 128, Type: Dataset, Value: MODIS/Terra+Aqua Photosynthetically Active Radiation Daily/3-Hour L3 Global 0.05Deg CMG V062\n",
      "  - ID: 138, Type: Dataset, Value: VIIRS/NPP BRDF/Albedo Parameter 3 Band M3 Daily L3 Global 30 ArcSec CMG V001\n",
      "  - ID: 139, Type: Dataset, Value: VIIRS/NPP BRDF/Albedo Parameter 1 Band M4 Daily L3 Global 30 ArcSec CMG V001\n",
      "  - ID: 128, Type: DataCenter, Value: N/A\n",
      "  - ID: 138, Type: DataCenter, Value: N/A\n",
      "  - ID: 139, Type: DataCenter, Value: N/A\n",
      "  - ID: 128, Type: Project, Value: Oceans Melting Greenland\n",
      "  - ID: 139, Type: Project, Value: Tropospheric Emission Spectrometer\n",
      "  - ID: 138, Type: Project, Value: AQUARIUS SAC-D\n",
      "  - ID: 138, Type: Platform, Value: Shared Mobile Atmospheric Research and Teaching Radar\n",
      "  - ID: 128, Type: Platform, Value: National Oceanic & Atmospheric Administration-6\n",
      "  - ID: 139, Type: Platform, Value: FIELD SURVEYS\n",
      "  - ID: 138, Type: Instrument, Value: TOTAL SKY IMAGER\n",
      "  - ID: 128, Type: Instrument, Value: CEILOMETERS\n",
      "  - ID: 139, Type: Instrument, Value: \n",
      "  - ID: 128, Type: ScienceKeyword, Value: WEB SERVICES\n",
      "  - ID: 138, Type: ScienceKeyword, Value: DATA PROCESSING SERVICES\n",
      "  - ID: 139, Type: ScienceKeyword, Value: CHANGE DETECTION SERVICES\n",
      "  - ID: 128, Type: Publication, Value: Pathways to Better Prediction of the MJO: 2. Impacts of Atmosphere-Ocean\n",
      "  - ID: 138, Type: Publication, Value: NASA'S HURRICANE AND SEVERE STORM SENTINEL (HS3) INVESTIGATION\n",
      "  - ID: 139, Type: Publication, Value: Energy and flux measurements of ultra-high energy cosmic rays observed during the first ANITA flight\n",
      "\n",
      "Exploring Subgraph for Node ID: 802 (Type: ScienceKeyword)\n",
      "  - ID: 2229, Type: Dataset, Value: NLDAS Mosaic Land Surface Model L4 Monthly Climatology 0.125 x 0.125 degree V002 (NLDAS_MOS0125_MC) at GES DISC\n",
      "  - ID: 5470, Type: Dataset, Value: Aquarius L2 Swath Single Orbit Soil Moisture V005\n",
      "  - ID: 5598, Type: Dataset, Value: SMEX04 Soil Characteristics Data: Arizona, Version 1\n",
      "  - ID: 176, Type: DataCenter, Value: N/A\n",
      "  - ID: 90, Type: DataCenter, Value: N/A\n",
      "  - ID: 164, Type: DataCenter, Value: N/A\n",
      "  - ID: 176, Type: Project, Value: Tropical Ozone Transport Experiment - Vortex Ozone Transport\n",
      "  - ID: 75, Type: Project, Value: Joint Polar Satellite System\n",
      "  - ID: 226, Type: Project, Value: Operation IceBridge Greenland/Arctic Sea Ice 2009\n",
      "  - ID: 164, Type: Platform, Value: Atmospheric Laboratory for Applications and Science-3\n",
      "  - ID: 90, Type: Platform, Value: FIXED OBSERVATION STATIONS\n",
      "  - ID: 275, Type: Platform, Value: \n",
      "  - ID: 802, Type: Instrument, Value: Hi-Capability Radar Sounder Version1\n",
      "  - ID: 343, Type: Instrument, Value: Multi-Angle Imaging SpectroRadiometer\n",
      "  - ID: 75, Type: Instrument, Value: NEXt Generation RADar\n",
      "  - ID: 1182, Type: ScienceKeyword, Value: LAND SURFACE\n",
      "  - ID: 275, Type: ScienceKeyword, Value: AEROSOLS\n",
      "  - ID: 459, Type: ScienceKeyword, Value: CLOUD PROPERTIES\n",
      "  - ID: 2978, Type: Publication, Value: Carbon export and fate beneath a dynamic upwelled filament off the California coast\n",
      "  - ID: 75, Type: Publication, Value: In situ observation of warm atmospheric layer and the heat contribution of suspended dust over the Tarim Basin\n",
      "  - ID: 1707, Type: Publication, Value: Profiling of CH4 background mixing ratio in the lower troposphere with\n",
      "\n",
      "Exploring Subgraph for Node ID: 108 (Type: Project)\n",
      "  - ID: 2424, Type: Dataset, Value: NCA-LDAS Noah-3.3 Land Surface Model L4 Trends 0.125 x 0.125 degree V2.0 (NCALDAS_NOAH0125_Trends) at GES DISC\n",
      "  - ID: 2323, Type: Dataset, Value: NCA-LDAS Noah-3.3 Land Surface Model L4 Daily 0.125 x 0.125 degree V2.0 (NCALDAS_NOAH0125_D) at GES DISC\n",
      "  - ID: 108, Type: Dataset, Value: Vegetation Index and Phenology (VIP) Phenology NDVI Yearly Global 0.05Deg CMG V004\n",
      "  - ID: 108, Type: DataCenter, Value: N/A\n",
      "  - ID: 108, Type: Project, Value: National Climate Assessment - Land Data Assimilation System\n",
      "  - ID: 108, Type: Platform, Value: \n",
      "  - ID: 108, Type: Instrument, Value: Optical Transient Detector\n",
      "  - ID: 108, Type: ScienceKeyword, Value: SOCIAL AND ECONOMIC MODELS\n",
      "  - ID: 2323, Type: Publication, Value: Monthly hydroclimatology of the continental United States\n",
      "  - ID: 108, Type: Publication, Value: Air-sea CO2 exchange process in the southern Yellow Sea in April of 2011, and June, July, October of 2012\n",
      "  - ID: 2424, Type: Publication, Value: Human appropriated net primary productivity as a metric for land use\n",
      "\n",
      "Exploring Subgraph for Node ID: 89 (Type: Project)\n",
      "  - ID: 2863, Type: Dataset, Value: SORCE SOLSTICE Level 3 MgII Core-to-Wing Ratio As-Measured Cadence V018 (SOR3SOLS_MGII_018) at GES DISC\n",
      "  - ID: 2444, Type: Dataset, Value: SORCE XPS Level 4 Solar Spectral Irradiance 1.0nm Res 24-Hour Means V012 (SOR4XPSD_LOW) at GES DISC\n",
      "  - ID: 1867, Type: Dataset, Value: SORCE SIM Level 3 TSIS-Adjusted Values Solar Spectral Irradiance 24-Hour Means V002 (SOR3SIMD_TAV) at GES DISC\n",
      "  - ID: 89, Type: DataCenter, Value: N/A\n",
      "  - ID: 89, Type: Project, Value: Solar Radiation and Climate Experiment\n",
      "  - ID: 89, Type: Platform, Value: COMPUTERS\n",
      "  - ID: 89, Type: Instrument, Value: \n",
      "  - ID: 89, Type: ScienceKeyword, Value: SERVICE DISCOVERY\n",
      "  - ID: 2159, Type: Publication, Value: Anthropogenic forcing exacerbating the urban heat islands in India\n",
      "  - ID: 2222, Type: Publication, Value: Performance evaluation of combining ICESat-2 and GEDI laser altimetry\n",
      "  - ID: 2787, Type: Publication, Value: Evidence for a developing plate boundary in the western Mediterranean\n",
      "\n",
      "Exploring Subgraph for Node ID: 26 (Type: Project)\n",
      "  - ID: 624, Type: Dataset, Value: RSS SSMIS OCEAN PRODUCT GRIDS WEEKLY AVERAGE FROM DMSP F16 NETCDF V7\n",
      "  - ID: 873, Type: Dataset, Value: TRMM MICROWAVE IMAGER (TMI) WENTZ OCEAN PRODUCTS V3\n",
      "  - ID: 1050, Type: Dataset, Value: RSS SSM/I OCEAN PRODUCT GRIDS DAILY FROM DMSP F10 NETCDF V7\n",
      "  - ID: 26, Type: DataCenter, Value: ASTER, JET PROPULSION LABORATORY, NASA\n",
      "  - ID: 26, Type: Project, Value: Distributed Info. Services for Climate/Ocean Prod./Visualizations for Earth Res.\n",
      "  - ID: 26, Type: Platform, Value: Goddard's LiDAR, Hyperspectral and Thermal (G-LiHT) airborne imaging system\n",
      "  - ID: 636, Type: Instrument, Value: C-LORAN Atmospheric Sounding System\n",
      "  - ID: 601, Type: Instrument, Value: LI-6252 CO2 Analyzer\n",
      "  - ID: 26, Type: Instrument, Value: Headwall Hyperspectral Camera\n",
      "  - ID: 545, Type: ScienceKeyword, Value: BRITTLE/BASKET STARS\n",
      "  - ID: 831, Type: ScienceKeyword, Value: EROSION\n",
      "  - ID: 611, Type: ScienceKeyword, Value: STURGEONS/PADDLEFISHES\n",
      "  - ID: 1066, Type: Publication, Value: Forest Fire Effects on Landscape Snow Albedo Recovery and Decay\n",
      "  - ID: 727, Type: Publication, Value: What fraction of the Pacific and Indian oceans' deep water is formed in the Southern Ocean?\n",
      "  - ID: 555, Type: Publication, Value: Quantifying the transmission dynamics of MRSA in the community and healthcare settings in a low-prevalence country\n",
      "\n",
      "Exploring Subgraph for Node ID: 344 (Type: Platform)\n",
      "  - ID: 81, Type: Dataset, Value: NASA Global Web-Enabled Landsat Data Annual Global 30 m V031\n",
      "  - ID: 386, Type: Dataset, Value: ASTER Digital Elevation Model V003\n",
      "  - ID: 4539, Type: Dataset, Value: Tropical Ozone Transport Experiment - Vortex Ozone Transport Experiment (TOTE-VOTE) DC-8 Ancillary Data\n",
      "  - ID: 69, Type: DataCenter, Value: Canadian Meteorological Centre, Meteorological Service of Canada, Environment Canada\n",
      "  - ID: 139, Type: DataCenter, Value: N/A\n",
      "  - ID: 81, Type: DataCenter, Value: N/A\n",
      "  - ID: 139, Type: Project, Value: Tropospheric Emission Spectrometer\n",
      "  - ID: 69, Type: Project, Value: Earth Science Information Partners Program\n",
      "  - ID: 98, Type: Project, Value: Famine Early Warning Systems Network (FEWS NET) Land Data Assimilation System\n",
      "  - ID: 139, Type: Platform, Value: FIELD SURVEYS\n",
      "  - ID: 251, Type: Platform, Value: Model\n",
      "  - ID: 81, Type: Platform, Value: Ground-based Observations\n",
      "  - ID: 215, Type: Instrument, Value: Pyrgeometer\n",
      "  - ID: 98, Type: Instrument, Value: Nevzorov Water Vapor Probe\n",
      "  - ID: 214, Type: Instrument, Value: WIND VANES\n",
      "  - ID: 85, Type: ScienceKeyword, Value: METADATA HANDLING\n",
      "  - ID: 386, Type: ScienceKeyword, Value: ATMOSPHERIC HEATING\n",
      "  - ID: 215, Type: ScienceKeyword, Value: FOREST MANAGEMENT\n",
      "  - ID: 139, Type: Publication, Value: Energy and flux measurements of ultra-high energy cosmic rays observed during the first ANITA flight\n",
      "  - ID: 386, Type: Publication, Value: Mississippi River Plume Variability in the Gulf of Mexico From SMAP and MODISAqua Observations\n",
      "  - ID: 3189, Type: Publication, Value: Basal Melting, Roughness and Structural Integrity of Ice Shelves\n",
      "\n",
      "Exploring Subgraph for Node ID: 166 (Type: Platform)\n",
      "  - ID: 751, Type: Dataset, Value: GPM GROUND VALIDATION UND CITATION NAVIGATION DATA GCPEX V2\n",
      "  - ID: 1723, Type: Dataset, Value: Carbon Monitoring System Lake Huron Primary Production Yearly V1 (CMSLakeHuronPPY) at GES DISC\n",
      "  - ID: 69, Type: Dataset, Value: MODIS/Terra+Aqua BRDF/Albedo Parameter2 Band7 Daily L3 Global 30ArcSec CMG V061\n",
      "  - ID: 166, Type: DataCenter, Value: N/A\n",
      "  - ID: 69, Type: DataCenter, Value: Canadian Meteorological Centre, Meteorological Service of Canada, Environment Canada\n",
      "  - ID: 81, Type: DataCenter, Value: N/A\n",
      "  - ID: 69, Type: Project, Value: Earth Science Information Partners Program\n",
      "  - ID: 81, Type: Project, Value: The second Modern-Era Retrospective analysis for Research and Applications\n",
      "  - ID: 166, Type: Project, Value: Studies of Emissions and Atmospheric Composition, Clouds and Climate Coupling by Regional Surveys\n",
      "  - ID: 335, Type: Platform, Value: Research Vessel Thomas G. Thompson\n",
      "  - ID: 166, Type: Platform, Value: NCEP Climate Forecast System Version 2\n",
      "  - ID: 85, Type: Platform, Value: \n",
      "  - ID: 81, Type: Instrument, Value: THERMOMETERS\n",
      "  - ID: 554, Type: Instrument, Value: Lightweight Airborne Chromatograph Experiment\n",
      "  - ID: 85, Type: Instrument, Value: \n",
      "  - ID: 751, Type: ScienceKeyword, Value: SWAMPS\n",
      "  - ID: 735, Type: ScienceKeyword, Value: SYMBIOSIS\n",
      "  - ID: 904, Type: ScienceKeyword, Value: GLACIAL MEASUREMENTS\n",
      "  - ID: 819, Type: Publication, Value: Liquid water content in ice estimated through a full-depth ground radar\n",
      "  - ID: 735, Type: Publication, Value: Spatial modeling of two mosquito vectors of West Nile virus using integrated nested Laplace approximations\n",
      "  - ID: 85, Type: Publication, Value: Improving the Retrieval of Forest Canopy Chlorophyll Content From MERIS\n",
      "\n",
      "Exploring Subgraph for Node ID: 178 (Type: Platform)\n",
      "  - ID: 2411, Type: Dataset, Value: CAR Eco-3D Vegetation Response to Changing Forcing Factors L1 V1 (CAR_ECO3D_L1C) at GES DISC\n",
      "  - ID: 3822, Type: Dataset, Value: ARCTAS Differential Absorption Lidar (DIAL) Remotely Sensed Data\n",
      "  - ID: 5285, Type: Dataset, Value: IceBridge DMS L1B Geolocated and Orthorectified Images V001\n",
      "  - ID: 81, Type: DataCenter, Value: N/A\n",
      "  - ID: 69, Type: DataCenter, Value: Canadian Meteorological Centre, Meteorological Service of Canada, Environment Canada\n",
      "  - ID: 130, Type: DataCenter, Value: N/A\n",
      "  - ID: 85, Type: Project, Value: North American Land Data Assimilation System\n",
      "  - ID: 292, Type: Project, Value: Pre-IceBridge NASA ATM Suite Antarctica 1997\n",
      "  - ID: 263, Type: Project, Value: Operation IceBridge Alaska Glaciers 2009\n",
      "  - ID: 283, Type: Platform, Value: Lockheed P-3B Orion\n",
      "  - ID: 90, Type: Platform, Value: FIXED OBSERVATION STATIONS\n",
      "  - ID: 292, Type: Platform, Value: \n",
      "  - ID: 855, Type: Instrument, Value: GNSS Receivers\n",
      "  - ID: 78, Type: Instrument, Value: Advanced Microwave Precipitation Radiometer\n",
      "  - ID: 621, Type: Instrument, Value: Langley In Situ Fast-Response Ozone Measurements\n",
      "  - ID: 88, Type: ScienceKeyword, Value: METADATA TRANSFORMATION/CONVERSION\n",
      "  - ID: 770, Type: ScienceKeyword, Value: ISLANDS\n",
      "  - ID: 85, Type: ScienceKeyword, Value: METADATA HANDLING\n",
      "  - ID: 5464, Type: Publication, Value: Reduced complexity models for regional aquatic habitat suitability\n",
      "  - ID: 1036, Type: Publication, Value: Streamflow trends of the Pyrenees using observations and multi-model\n",
      "  - ID: 5732, Type: Publication, Value: Ambiguous Agricultural Drought: Characterising Soil Moisture and\n",
      "\n",
      "Generating detailed LLM summary using Wikipedia, subgraph nodes, and scholarly articles...\n",
      "result =  {'ns': 0, 'title': 'Tropospheric Emission Spectrometer', 'pageid': 883683, 'size': 1636, 'wordcount': 139, 'snippet': '<span class=\"searchmatch\">Tropospheric</span> <span class=\"searchmatch\">Emission</span> <span class=\"searchmatch\">Spectrometer</span> or TES was a satellite instrument designed to measure the state of the earth\\'s troposphere. TES was a high-resolution', 'timestamp': '2023-12-22T15:00:31Z'}\n",
      "result =  {'ns': 0, 'title': 'TES', 'pageid': 883689, 'size': 3054, 'wordcount': 398, 'snippet': 'Organization Thermal <span class=\"searchmatch\">Emission</span> <span class=\"searchmatch\">Spectrometer</span>, a scientific instrument aboard Mars Global Surveyor <span class=\"searchmatch\">Tropospheric</span> <span class=\"searchmatch\">Emission</span> <span class=\"searchmatch\">Spectrometer</span>, a satellite instrument', 'timestamp': '2024-07-03T05:24:25Z'}\n",
      "result =  {'ns': 0, 'title': 'Ground-level ozone', 'pageid': 215051, 'size': 35130, 'wordcount': 3997, 'snippet': 'mapping <span class=\"searchmatch\">spectrometer</span>-earth probe (TOMS-EP) aboard a satellite from NASA is an example of an ozone layer measuring satellite, and the <span class=\"searchmatch\">tropospheric</span> <span class=\"searchmatch\">emission</span> spectrometer', 'timestamp': '2024-11-06T16:37:48Z'}\n",
      "result =  {'ns': 0, 'title': 'Tropospheric Emissions: Monitoring of Pollution', 'pageid': 61012947, 'size': 8238, 'wordcount': 688, 'snippet': '<span class=\"searchmatch\">Tropospheric</span> <span class=\"searchmatch\">Emissions</span>: Monitoring of Pollution (TEMPO) is a space-based <span class=\"searchmatch\">spectrometer</span> designed to measure air pollution across greater North America at', 'timestamp': '2024-01-21T21:18:09Z'}\n",
      "result =  {'ns': 0, 'title': 'Methane emissions', 'pageid': 53830256, 'size': 90122, 'wordcount': 9582, 'snippet': 'Increasing methane <span class=\"searchmatch\">emissions</span> are a major contributor to the rising concentration of greenhouse gases in Earth\\'s atmosphere, and are responsible for up', 'timestamp': '2024-11-06T17:22:16Z'}\n",
      "\n",
      "=== Combined LLM Summary ===\n",
      "The Tropospheric Emission Spectrometer (TES), project ID 139, was a satellite instrument designed to measure the state of Earth's troposphere (as detailed in the Wikipedia context).  This project, also identified as a FIELD SURVEYS platform (ID 139), utilized an imaging infrared Fourier-transform spectrometer (as described in \"Tropospheric emission spectrometer for the Earth Observing System's Aura satellite\", URL: https://opg.optica.org/abstract.cfm?uri=ao-40-15-2356).  Its primary goal was to gather data on various atmospheric constituents,  including ozone, water vapor, and temperature.\n",
      "\n",
      "This is evidenced by the existence of several datasets associated with TES: TES/Aura L3 Water Vapor Monthly Gridded V005 (ID 4005), TES/Aura L3 Atmospheric Temperature Monthly Gridded V006 (ID 4115), and TES/Aura L2 Formic Acid Nadir V007 (ID 4204). These datasets, presumably housed at a data center (ID 139, value N/A), provide crucial measurements that support scientific analysis.  The  retrieval methods and error analysis associated with TES data are further elaborated in \"Tropospheric emission spectrometer: Retrieval method and error analysis\" (URL: https://ieeexplore.ieee.org/abstract/document/1624609/).\n",
      "\n",
      "The importance of TES's ozone measurements is highlighted by the Wikipedia context's discussion of ground-level (tropospheric) ozone – a significant greenhouse gas and air pollutant.  The study \"Comparisons of Tropospheric Emission Spectrometer (TES) ozone profiles to ozonesondes: Methods and initial results\" (URL: https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2006JD007258) specifically addresses the comparison of TES ozone data with ground-based measurements, validating the instrument's capabilities.  The high-resolution, hourly data provided by a successor instrument, TEMPO (Tropospheric Emissions: Monitoring of Pollution), further underscores the ongoing need for precise tropospheric ozone monitoring.\n",
      "\n",
      "While TES's primary focus wasn't directly methane emissions, its broader contribution to understanding atmospheric composition provides context for understanding the impact of greenhouse gases like methane, as discussed in the Wikipedia entry concerning increasing methane emissions and their contribution to global warming.  The publications associated with TES (IDs 4572, 3885, 3742) likely utilize TES data, but the provided abstracts don't directly link them to the instrument's ozone or other measurements.  Further investigation of the publications themselves would be required to fully assess their reliance on TES data and their specific contributions to our understanding of tropospheric processes.  The \"CHANGE DETECTION SERVICES\" ScienceKeyword (ID 139) suggests that TES data may also be used for detecting changes in atmospheric conditions over time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Code to run Using RAG-integrated Node Inspection + Graph Exploration\n",
    "#   - User views the output of graph_exploration.txt or the output of this \n",
    "#     cell (i.e. display_nodes_and_subgraphs). Then, to explore a specific\n",
    "#     node further, user can enter the node_id and node_type to get a detailed\n",
    "#     LLM summary of the node with context from Wikipedia, subgraph nodes, and\n",
    "#     Google Scholar. The results are displayed in the terminal and written to\n",
    "#     combined_summary.md.\n",
    "# \n",
    "# Output: \n",
    "#   1. relevant nodes and their subgraphs\n",
    "#   2. interactive exploration tool\n",
    "#       2.1 Enter node_id and node_type to explore further (or 'q' to quit)\n",
    "#       2.2 LLM summary written to combined_summary.md\n",
    "# =======================================================================\n",
    "\n",
    "# Same information as in graph_exploration.txt, just now in termianl\n",
    "display_nodes_and_subgraphs(data, graph_results)\n",
    "\n",
    "# Run the interactive RAG exploration\n",
    "interactive_rag_exploration(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "#### Known Issues as 12/10:\n",
    "- Duplicates in the dataset are not manually removed\n",
    "- Using WIKIPEDIA as external resource. NASA APIs are very specific and not generalizeable to the specifc user queries\n",
    "- Query words are searched separately, so if there's a stopword in the query, it may bias results. \n",
    "    - Soln. Experiment with semantic embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
