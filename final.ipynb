{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project: Query-Driven Retrieval-Augmented Graph Exploration Tool\n",
    "By Karl Simon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the dataset into PyG (PyTorch Geometric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes and Properties:\n",
      "\n",
      "Node Type: Dataset\n",
      "Number of Nodes: 6390\n",
      "  - temporalExtentStart: 6375 items (non-numeric)\n",
      "  - seCorner: 5330 items (non-numeric)\n",
      "  - cmrId: 6390 items (non-numeric)\n",
      "  - globalId: 6390 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([6390, 512])\n",
      "  - abstract: 6390 items (non-numeric)\n",
      "  - daac: 6131 items (non-numeric)\n",
      "  - nwCorner: 5330 items (non-numeric)\n",
      "  - temporalFrequency: 6390 items (non-numeric)\n",
      "  - pagerank_global: torch.Size([6390, 1])\n",
      "  - temporalExtentEnd: 3765 items (non-numeric)\n",
      "  - shortName: 6390 items (non-numeric)\n",
      "  - landingPageUrl: 3037 items (non-numeric)\n",
      "  - doi: 6390 items (non-numeric)\n",
      "  - longName: 6390 items (non-numeric)\n",
      "\n",
      "Node Type: DataCenter\n",
      "Number of Nodes: 184\n",
      "  - pagerank_global: torch.Size([184, 1])\n",
      "  - globalId: 184 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([184, 512])\n",
      "  - shortName: 184 items (non-numeric)\n",
      "  - url: 184 items (non-numeric)\n",
      "  - longName: 184 items (non-numeric)\n",
      "\n",
      "Node Type: Project\n",
      "Number of Nodes: 333\n",
      "  - pagerank_global: torch.Size([333, 1])\n",
      "  - globalId: 333 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([333, 512])\n",
      "  - shortName: 333 items (non-numeric)\n",
      "  - longName: 333 items (non-numeric)\n",
      "\n",
      "Node Type: Platform\n",
      "Number of Nodes: 442\n",
      "  - Type: 442 items (non-numeric)\n",
      "  - pagerank_global: torch.Size([442, 1])\n",
      "  - globalId: 442 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([442, 512])\n",
      "  - shortName: 442 items (non-numeric)\n",
      "  - longName: 442 items (non-numeric)\n",
      "\n",
      "Node Type: Instrument\n",
      "Number of Nodes: 867\n",
      "  - pagerank_global: torch.Size([867, 1])\n",
      "  - globalId: 867 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([867, 512])\n",
      "  - shortName: 867 items (non-numeric)\n",
      "  - longName: 867 items (non-numeric)\n",
      "\n",
      "Node Type: ScienceKeyword\n",
      "Number of Nodes: 1609\n",
      "  - pagerank_global: torch.Size([1609, 1])\n",
      "  - globalId: 1609 items (non-numeric)\n",
      "  - name: 1609 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([1609, 512])\n",
      "  - pagerank_publication_dataset: torch.Size([1609, 1])\n",
      "\n",
      "Node Type: Publication\n",
      "Number of Nodes: 125939\n",
      "  - year: 125939 items (non-numeric)\n",
      "  - pagerank_global: torch.Size([125939, 1])\n",
      "  - globalId: 125939 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([125939, 512])\n",
      "  - title: 125937 items (non-numeric)\n",
      "  - DOI: 125939 items (non-numeric)\n",
      "  - abstract: 99859 items (non-numeric)\n",
      "  - authors: 109975 items (non-numeric)\n",
      "\n",
      "Edges and Types:\n",
      "Edge Type: ('DataCenter', 'HAS_DATASET', 'Dataset') - Number of Edges: 9017 - Shape: torch.Size([2, 9017])\n",
      "Edge Type: ('Dataset', 'OF_PROJECT', 'Project') - Number of Edges: 6049 - Shape: torch.Size([2, 6049])\n",
      "Edge Type: ('Dataset', 'HAS_PLATFORM', 'Platform') - Number of Edges: 9884 - Shape: torch.Size([2, 9884])\n",
      "Edge Type: ('Platform', 'HAS_INSTRUMENT', 'Instrument') - Number of Edges: 2469 - Shape: torch.Size([2, 2469])\n",
      "Edge Type: ('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword') - Number of Edges: 1823 - Shape: torch.Size([2, 1823])\n",
      "Edge Type: ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword') - Number of Edges: 20436 - Shape: torch.Size([2, 20436])\n",
      "Edge Type: ('Publication', 'CITES', 'Publication') - Number of Edges: 208670 - Shape: torch.Size([2, 208670])\n",
      "Edge Type: ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword') - Number of Edges: 89039 - Shape: torch.Size([2, 89039])\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports for entire notebook\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from torch_geometric.data import HeteroData\n",
    "from collections import defaultdict\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "# from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import random\n",
    "from scholarly import scholarly\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "MAX_VAL_LEN = 1000 # max text length for input to LLM from graph_results for each node\n",
    "\n",
    "\n",
    "# Load JSON data from file\n",
    "file_path = \"/home/karlsimon/CSCI6365/final/graph.json\"\n",
    "graph_data = []\n",
    "\n",
    "# Load data line by line to prevent memory overload\n",
    "with open(file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            graph_data.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON line: {e}\")\n",
    "            continue\n",
    "\n",
    "# Initialize HeteroData object\n",
    "data = HeteroData()\n",
    "\n",
    "# Mapping for node indices per node type\n",
    "node_mappings = defaultdict(dict)\n",
    "\n",
    "# Temporary storage for properties\n",
    "node_properties = defaultdict(lambda: defaultdict(list))\n",
    "edge_indices = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# # Define limits for node subsets based on type\n",
    "# node_limits = {\n",
    "#     'Publication': 1000,\n",
    "#     'Dataset': 500,\n",
    "#     'ScienceKeyword': 300,\n",
    "#     'Instrument': 200,\n",
    "#     'Platform': 150,\n",
    "#     'Project': 100,\n",
    "#     'DataCenter': 50\n",
    "# }\n",
    "\n",
    "# Track the number of nodes added per type\n",
    "node_counts = defaultdict(int)\n",
    "\n",
    "# Process nodes with limits\n",
    "for item in graph_data:\n",
    "    if item['type'] == 'node':\n",
    "        node_type = item['labels'][0]\n",
    "        # if node_counts[node_type] >= node_limits.get(node_type, 50):\n",
    "        #     continue  # Skip nodes once the limit is reached\n",
    "\n",
    "        node_id = item['id']\n",
    "        properties = item['properties']\n",
    "\n",
    "        # Store the node index mapping\n",
    "        node_index = len(node_mappings[node_type])\n",
    "        node_mappings[node_type][node_id] = node_index\n",
    "        node_counts[node_type] += 1\n",
    "\n",
    "        # Store properties temporarily by type\n",
    "        for key, value in properties.items():\n",
    "            if isinstance(value, list) and all(isinstance(v, (int, float)) for v in value):\n",
    "                node_properties[node_type][key].append(torch.tensor(value, dtype=torch.float))\n",
    "            elif isinstance(value, (int, float)):\n",
    "                node_properties[node_type][key].append(torch.tensor([value], dtype=torch.float))\n",
    "            else:\n",
    "                node_properties[node_type][key].append(value)  # non-numeric properties as lists\n",
    "\n",
    "# # Define limits for relationships based on type\n",
    "# relationship_limits = {\n",
    "#     'CITES': 2000,\n",
    "#     'HAS_APPLIED_RESEARCH_AREA': 1000,\n",
    "#     'HAS_SCIENCEKEYWORD': 500,\n",
    "#     'HAS_PLATFORM': 500,\n",
    "#     'HAS_DATASET': 500,\n",
    "#     'OF_PROJECT': 300,\n",
    "#     'HAS_INSTRUMENT': 200\n",
    "# }\n",
    "\n",
    "# Track the number of relationships added per type\n",
    "relationship_counts = defaultdict(int)\n",
    "\n",
    "# Filter relationships to only include sampled nodes\n",
    "for item in graph_data:\n",
    "    if item['type'] == 'relationship':\n",
    "        start_type = item['start']['labels'][0]\n",
    "        end_type = item['end']['labels'][0]\n",
    "        start_id = item['start']['id']\n",
    "        end_id = item['end']['id']\n",
    "        edge_type = item['label']\n",
    "\n",
    "        # # Skip if relationship limit reached\n",
    "        # if relationship_counts[edge_type] >= relationship_limits.get(edge_type, 100):\n",
    "        #     continue\n",
    "\n",
    "        # Check if start and end nodes exist in the sampled nodes\n",
    "        if start_id in node_mappings[start_type] and end_id in node_mappings[end_type]:\n",
    "            start_idx = node_mappings[start_type][start_id]\n",
    "            end_idx = node_mappings[end_type][end_id]\n",
    "\n",
    "            # Append to edge list\n",
    "            edge_indices[(start_type, edge_type, end_type)]['start'].append(start_idx)\n",
    "            edge_indices[(start_type, edge_type, end_type)]['end'].append(end_idx)\n",
    "            relationship_counts[edge_type] += 1\n",
    "\n",
    "# Finalize node properties by batch processing\n",
    "for node_type, properties in node_properties.items():\n",
    "    data[node_type].num_nodes = len(node_mappings[node_type])\n",
    "    for key, values in properties.items():\n",
    "        if isinstance(values[0], torch.Tensor):\n",
    "            data[node_type][key] = torch.stack(values)\n",
    "        else:\n",
    "            data[node_type][key] = values  # Keep non-tensor properties as lists\n",
    "\n",
    "# Finalize edge indices in bulk\n",
    "for (start_type, edge_type, end_type), indices in edge_indices.items():\n",
    "    edge_index = torch.tensor([indices['start'], indices['end']], dtype=torch.long)\n",
    "    data[start_type, edge_type, end_type].edge_index = edge_index\n",
    "\n",
    "# Display statistics for verification\n",
    "print(\"Nodes and Properties:\")\n",
    "for node_type in data.node_types:\n",
    "    print(f\"\\nNode Type: {node_type}\")\n",
    "    print(f\"Number of Nodes: {data[node_type].num_nodes}\")\n",
    "    for key, value in data[node_type].items():\n",
    "        if key != 'num_nodes':\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"  - {key}: {value.shape}\")\n",
    "            else:\n",
    "                print(f\"  - {key}: {len(value)} items (non-numeric)\")\n",
    "\n",
    "print(\"\\nEdges and Types:\")\n",
    "for edge_type in data.edge_types:\n",
    "    edge_index = data[edge_type].edge_index\n",
    "    print(f\"Edge Type: {edge_type} - Number of Edges: {edge_index.size(1)} - Shape: {edge_index.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 : Search Graph for nodes based on user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Steps:\n",
    "# 1. improve graph search and rank results.\n",
    "# 2. improve subgraph exploration.\n",
    "# 3. improve external context retrieval (NASA API).\n",
    "\n",
    "# Functions definitions for keywords, search and display used in next cell\n",
    "def extract_keywords(query):\n",
    "    keywords = re.findall(r'\\b\\w+\\b', query)\n",
    "    return [kw.lower() for kw in keywords]\n",
    "\n",
    "# Updated search_graph function with TF-IDF scoring\n",
    "# TODO: make max_per_type specific to each node type\n",
    "def search_graph(data, keywords, node_types=['Dataset', 'Project', 'ScienceKeyword', 'Instrument', 'Platform', 'Publication'], max_results=50, max_per_type=10):\n",
    "    results = []\n",
    "    texts = []  # Collect text data for TF-IDF processing\n",
    "    metadata = []  # To store corresponding metadata (node type, index, key, value)\n",
    "\n",
    "    # Step 1: Collect all matching nodes and their text data\n",
    "    for node_type in node_types:\n",
    "        for key in data[node_type]:\n",
    "            if key == 'num_nodes':\n",
    "                continue\n",
    "            \n",
    "            values = data[node_type][key]\n",
    "            if isinstance(values, list):\n",
    "                for idx, value in enumerate(values):\n",
    "                    value_str = str(value).lower()\n",
    "                    if any(kw in value_str for kw in keywords):\n",
    "                        texts.append(value_str)\n",
    "                        metadata.append((node_type, idx, key, value))\n",
    "\n",
    "    if not texts:\n",
    "        return []\n",
    "\n",
    "    # Step 2: Compute TF-IDF scores for the collected texts\n",
    "    # NOTE: texts stores the properties of the nodes which contain the keywords\n",
    "    vectorizer = TfidfVectorizer(vocabulary=keywords)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    scores = tfidf_matrix.sum(axis=1).A1  # Sum the TF-IDF scores for each text\n",
    "\n",
    "    # Step 3: Sort the results by TF-IDF score in descending order\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    sorted_results = [metadata[i] for i in sorted_indices]\n",
    "    # with open(\"sorted_results.txt\", \"w\") as file:\n",
    "    #     for result in sorted_results:\n",
    "    #         file.write(f\"{result}\\n\")\n",
    "\n",
    "    # Step 4: Limit the number of results overall and per node type\n",
    "    final_results = []\n",
    "    counts_per_type = {node_type: 0 for node_type in node_types}\n",
    "\n",
    "    for result in sorted_results:\n",
    "        node_type = result[0]\n",
    "        if len(final_results) >= max_results:\n",
    "            break\n",
    "        if counts_per_type[node_type] < max_per_type:\n",
    "            final_results.append(result)\n",
    "            counts_per_type[node_type] += 1\n",
    "\n",
    "    # write the 50 final_results to a file\n",
    "    print(\"Writing 50 final_results to file\")\n",
    "    with open(\"final_results.txt\", \"w\") as file:\n",
    "        for result in final_results:\n",
    "            file.write(f\"{result}\\n\")\n",
    "\n",
    "    return final_results\n",
    "\n",
    "# Updated display_results function to trim long values\n",
    "def display_results(results, max_value_length=MAX_VAL_LEN):\n",
    "    if not results:\n",
    "        print(\"No relevant nodes found.\")\n",
    "        return\n",
    "\n",
    "    with open(\"query_results.txt\", \"w\") as file:\n",
    "        print(f\"\\nFound {len(results)} relevant nodes:\\n\")\n",
    "        for node_type, idx, key, value in results:\n",
    "            value_str = str(value)\n",
    "            if len(value_str) > max_value_length:\n",
    "                value_str = value_str[:max_value_length] + \"...\"\n",
    "            output_line = f\"Node Type: {node_type} | Index: {idx} | Property: {key} | Value: {value_str}\\n\"\n",
    "            file.write(output_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given query, extract keywords, search the graph for relevant nodes, and display the results\n",
    "# NOTE: currently only searches for exact keyword matches in node properties\n",
    "\n",
    "def get_subgraph(data, node_type, node_indices, num_hops=2):\n",
    "    # Find all edge types where the node_type is either the source or target\n",
    "    relevant_edges = [\n",
    "        (src, rel, dst) for (src, rel, dst) in data.edge_types if src == node_type or dst == node_type\n",
    "    ]\n",
    "    \n",
    "    print(\"relevant_edges = \", relevant_edges)\n",
    "\n",
    "    if not relevant_edges:\n",
    "        print(f\"No edges found for node type '{node_type}'\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Combine edge indices from all relevant edge types\n",
    "    combined_edge_index = []\n",
    "    combined_edge_types = []\n",
    "\n",
    "    for edge_type in relevant_edges:\n",
    "        edge_index = data[edge_type].edge_index\n",
    "        combined_edge_index.append(edge_index)\n",
    "        combined_edge_types.append(edge_type)\n",
    "\n",
    "    # Stack all edge indices into a single tensor\n",
    "    combined_edge_index = torch.cat(combined_edge_index, dim=1)\n",
    "\n",
    "    # Extract the subgraph using the combined edge index\n",
    "    subset, edge_index, _, _ = k_hop_subgraph(node_idx=node_indices, num_hops=num_hops, edge_index=combined_edge_index)\n",
    "    return subset, edge_index, combined_edge_types\n",
    "\n",
    "\n",
    "# Explore subgraphs based on the search results.\n",
    "def explore_subgraphs(data, results, num_hops=2):\n",
    "    if not results:\n",
    "        print(\"No nodes to explore for subgraphs.\")\n",
    "        return\n",
    "\n",
    "    # Group the results by node type\n",
    "    nodes_by_type = defaultdict(list)\n",
    "    for node_type, idx, _, _ in results:\n",
    "        nodes_by_type[node_type].append(idx)\n",
    "\n",
    "    # Extract and display subgraphs for each node type\n",
    "    for node_type, indices in nodes_by_type.items():\n",
    "        print(f\"\\nExploring subgraph for node type: {node_type}\")\n",
    "        # print(f\"Number of nodes: {len(indices)}\") #10 nodes\n",
    "        # Get the valid range for node indices\n",
    "        num_nodes = data[node_type].num_nodes\n",
    "        valid_indices = [idx for idx in indices if idx < num_nodes]\n",
    "\n",
    "        if not valid_indices:\n",
    "            print(f\"No valid indices for node type '{node_type}'.\")\n",
    "            continue\n",
    "\n",
    "        node_indices = torch.tensor(valid_indices[:10])  # Limit to 10 nodes (only using 10 per node_type anyways for now)\n",
    "        print(f\"Exploring subgraph for node indices: {node_indices}\") # may not be sequential due to search results ordering \n",
    "        subset, edge_index, edge_type = get_subgraph(data, node_type, node_indices, num_hops=num_hops)\n",
    "\n",
    "        if subset is not None and edge_index is not None:\n",
    "            print(f\"Extracted subgraph with {len(subset)} nodes and {edge_index.size(1)} edges.\")\n",
    "            print(f\"Edge Type: {edge_type}\")\n",
    "        else:\n",
    "            print(f\"Could not extract subgraph for node type: {node_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 : Use APIs Wikipedia for external information based on user query\n",
    "- Question: should API be queries based on keywords or the extracted graph nodes from keywords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get external context from Wikipedia using the REST API\n",
    "def fetch_wikipedia_context(keywords):\n",
    "    search_term = \" \".join(keywords)\n",
    "    \n",
    "    # Step 1: Use the Action API to get the top 5 search results\n",
    "    search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    search_params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": search_term,\n",
    "        \"srlimit\": 5,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"GraphExplorationTool/1.0 (ksimon24@gwu.edu)\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        search_response = requests.get(search_url, params=search_params, headers=headers)\n",
    "        search_response.raise_for_status()\n",
    "        search_data = search_response.json()\n",
    "        \n",
    "        search_results = search_data.get(\"query\", {}).get(\"search\", [])\n",
    "        \n",
    "        if not search_results:\n",
    "            return None\n",
    "\n",
    "        # Step 2: Fetch summaries using the REST API for each search result\n",
    "        context_list = []\n",
    "        for result in search_results:\n",
    "            print(\"result = \", result)\n",
    "            page_title = result.get(\"title\")\n",
    "            rest_url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{page_title.replace(' ', '_')}\"\n",
    "            \n",
    "            rest_response = requests.get(rest_url, headers=headers)\n",
    "            rest_response.raise_for_status()\n",
    "            rest_data = rest_response.json()\n",
    "            \n",
    "            # Extract relevant information\n",
    "            title = rest_data.get(\"title\", \"No Title\")\n",
    "            description = rest_data.get(\"description\", \"No Description Available.\")\n",
    "            summary = rest_data.get(\"extract\", \"No Summary Available.\")\n",
    "            link = rest_data.get(\"content_urls\", {}).get(\"desktop\", {}).get(\"page\", \"No Link Available.\")\n",
    "            thumbnail = rest_data.get(\"thumbnail\", {}).get(\"source\", None)\n",
    "            \n",
    "            context_entry = {\n",
    "                \"title\": title,\n",
    "                \"description\": description,\n",
    "                \"summary\": summary,\n",
    "                \"link\": link,\n",
    "                \"thumbnail\": thumbnail\n",
    "            }\n",
    "            context_list.append(context_entry)\n",
    "        \n",
    "        return context_list\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching Wikipedia context: {e}\")\n",
    "        return None\n",
    "    \n",
    "def display_wikipedia_context(context_list):\n",
    "    if not context_list:\n",
    "        print(\"\\nNo external context available from Wikipedia.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nWikipedia Context:\")\n",
    "    for i, context in enumerate(context_list, start=1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"Title: {context['title']}\")\n",
    "        print(f\"Description: {context['description']}\")\n",
    "        print(f\"Summary: {context['summary']}\")\n",
    "        print(f\"Link: {context['link']}\")\n",
    "        if context['thumbnail']:\n",
    "            print(f\"Thumbnail: {context['thumbnail']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Keywords: ['climate', 'change']\n",
      "Writing 50 final_results to file\n",
      "\n",
      "Found 50 relevant nodes:\n",
      "\n",
      "result =  {'ns': 0, 'title': 'Climate change', 'pageid': 5042951, 'size': 317341, 'wordcount': 27919, 'snippet': 'Present-day <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span> includes both global warming—the ongoing increase in global average temperature—and its wider effects on Earth\\'s <span class=\"searchmatch\">climate</span>. <span class=\"searchmatch\">Climate</span> change', 'timestamp': '2024-12-10T03:05:32Z'}\n",
      "result =  {'ns': 0, 'title': 'Climate change denial', 'pageid': 12474403, 'size': 237127, 'wordcount': 22133, 'snippet': '<span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">change</span> denial (also global warming denial) is a form of science denial characterized by rejecting, refusing to acknowledge, disputing, or fighting', 'timestamp': '2024-12-04T00:54:58Z'}\n",
      "result =  {'ns': 0, 'title': 'Climate change mitigation', 'pageid': 2119179, 'size': 229408, 'wordcount': 22636, 'snippet': '<span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">change</span> mitigation (or decarbonisation) is action to limit the greenhouse gases in the atmosphere that cause <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span>. <span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">change</span> mitigation', 'timestamp': '2024-12-09T04:31:50Z'}\n",
      "result =  {'ns': 0, 'title': 'Effects of climate change', 'pageid': 2119174, 'size': 180605, 'wordcount': 20335, 'snippet': 'Effects of <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span> are well documented and growing for Earth\\'s natural environment and human societies. <span class=\"searchmatch\">Changes</span> to the <span class=\"searchmatch\">climate</span> system include an', 'timestamp': '2024-12-08T12:02:41Z'}\n",
      "result =  {'ns': 0, 'title': 'United Nations Framework Convention on Climate Change', 'pageid': 31898, 'size': 95299, 'wordcount': 9711, 'snippet': 'Framework Convention on <span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">Change</span> (UNFCCC) is the UN process for negotiating an agreement to limit dangerous <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span>. It is an international', 'timestamp': '2024-11-18T10:15:56Z'}\n",
      "\n",
      "Wikipedia Context:\n",
      "\n",
      "Result 1:\n",
      "Title: Climate change\n",
      "Description: Human-caused changes to climate on Earth\n",
      "Summary: Present-day climate change includes both global warming—the ongoing increase in global average temperature—and its wider effects on Earth's climate. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The current rise in global temperatures is driven by human activities, especially fossil fuel burning since the Industrial Revolution. Fossil fuel use, deforestation, and some agricultural and industrial practices release greenhouse gases. These gases absorb some of the heat that the Earth radiates after it warms from sunlight, warming the lower atmosphere. Carbon dioxide, the primary greenhouse gas driving global warming, has grown by about 50% and is at levels not seen for millions of years.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Change_in_Average_Temperature_With_Fahrenheit.svg/320px-Change_in_Average_Temperature_With_Fahrenheit.svg.png\n",
      "\n",
      "Result 2:\n",
      "Title: Climate change denial\n",
      "Description: Denial of the scientific consensus on climate change\n",
      "Summary: Climate change denial is a form of science denial characterized by rejecting, refusing to acknowledge, disputing, or fighting the scientific consensus on climate change. Those promoting denial commonly use rhetorical tactics to give the appearance of a scientific controversy where there is none. Climate change denial includes unreasonable doubts about the extent to which climate change is caused by humans, its effects on nature and human society, and the potential of adaptation to global warming by human actions. To a lesser extent, climate change denial can also be implicit when people accept the science but fail to reconcile it with their belief or action. Several studies have analyzed these positions as forms of denialism, pseudoscience, or propaganda.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change_denial\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Inhofe_holding_snowball.jpg/320px-Inhofe_holding_snowball.jpg\n",
      "\n",
      "Result 3:\n",
      "Title: Climate change mitigation\n",
      "Description: Actions to reduce net greenhouse gas emissions to limit climate change\n",
      "Summary: Climate change mitigation (or decarbonisation) is action to limit the greenhouse gases in the atmosphere that cause climate change. Climate change mitigation actions include conserving energy and replacing fossil fuels with clean energy sources. Secondary mitigation strategies include changes to land use and removing carbon dioxide (CO2) from the atmosphere. Current climate change mitigation policies are insufficient as they would still result in global warming of about 2.7 °C by 2100, significantly above the 2015 Paris Agreement's goal of limiting global warming to below 2 °C.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change_mitigation\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Westmill_Solar_2.jpg/320px-Westmill_Solar_2.jpg\n",
      "\n",
      "Result 4:\n",
      "Title: Effects of climate change\n",
      "Description: No Description Available.\n",
      "Summary: Effects of climate change are well documented and growing for Earth's natural environment and human societies. Changes to the climate system include an overall warming trend, changes to precipitation patterns, and more extreme weather. As the climate changes it impacts the natural environment with effects such as more intense forest fires, thawing permafrost, and desertification. These changes impact ecosystems and societies, and can become irreversible once tipping points are crossed. Climate activists are engaged in a range of activities around the world that seek to ameliorate these issues or prevent them from happening.\n",
      "Link: https://en.wikipedia.org/wiki/Effects_of_climate_change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/062821Yreka_Fire_CalFire_-2wiki.jpg/320px-062821Yreka_Fire_CalFire_-2wiki.jpg\n",
      "\n",
      "Result 5:\n",
      "Title: United Nations Framework Convention on Climate Change\n",
      "Description: International environmental treaty\n",
      "Summary: The United Nations Framework Convention on Climate Change (UNFCCC) is the UN process for negotiating an agreement to limit dangerous climate change. It is an international treaty among countries to combat \"dangerous human interference with the climate system\". The main way to do this is limiting the increase in greenhouse gases in the atmosphere. It was signed in 1992 by 154 states at the United Nations Conference on Environment and Development (UNCED), informally known as the Earth Summit, held in Rio de Janeiro. The treaty entered into force on 21 March 1994. \"UNFCCC\" is also the name of the Secretariat charged with supporting the operation of the convention, with offices on the UN Campus in Bonn, Germany.\n",
      "Link: https://en.wikipedia.org/wiki/United_Nations_Framework_Convention_on_Climate_Change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/UNFCCC_Annex_I_Parties%2C_OECD%2C_EU.svg/320px-UNFCCC_Annex_I_Parties%2C_OECD%2C_EU.svg.png\n",
      "\n",
      "Exploring subgraph for node type: Publication\n",
      "Exploring subgraph for node indices: tensor([76606, 94077, 50763, 55702, 88548, 42689, 82757, 12919, 60848, 35890])\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 18 nodes and 8 edges.\n",
      "Edge Type: [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Dataset\n",
      "Exploring subgraph for node indices: tensor([4329, 2323,   97,   61,   65, 2131, 4180, 3752,  293, 4253])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Extracted subgraph with 2263 nodes and 16366 edges.\n",
      "Edge Type: [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: ScienceKeyword\n",
      "Exploring subgraph for node indices: tensor([ 94, 802, 888, 989, 139,  70,  72,  96, 729, 229])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 374 nodes and 738 edges.\n",
      "Edge Type: [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Project\n",
      "Exploring subgraph for node indices: tensor([109, 117,  26,  59,  89, 105, 108, 121,   0, 137])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Extracted subgraph with 197 nodes and 206 edges.\n",
      "Edge Type: [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "\n",
      "Exploring subgraph for node type: Platform\n",
      "Exploring subgraph for node indices: tensor([165, 166, 173, 178, 344,  68, 248, 307, 344, 409])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Extracted subgraph with 1953 nodes and 4553 edges.\n",
      "Edge Type: [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n"
     ]
    }
   ],
   "source": [
    "# ################ Pre-LLM Steps ################\n",
    "# query = input(\"Enter your query (e.g., 'Find datasets related to climate change projects'): \")\n",
    "query = \"climate change\" #TODO: remove hardcoded query\n",
    "keywords = extract_keywords(query)\n",
    "print(f\"\\nExtracted Keywords: {keywords}\")\n",
    "\n",
    "# Search the graph with TF-IDF ranking\n",
    "graph_results = search_graph(data, keywords)\n",
    "display_results(graph_results) # 50 results\n",
    "\n",
    "# Fetch Wikipedia context\n",
    "wikipedia_context = fetch_wikipedia_context(keywords)\n",
    "display_wikipedia_context(wikipedia_context)\n",
    "\n",
    "# Explore subgraphs based on the results\n",
    "# TODO: save these subgraphs\n",
    "explore_subgraphs(data, graph_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin the RAG Pipeline with LLM Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Gemini API ############\n",
    "# Define the path to the text file containing the API key\n",
    "file_path = \"/home/karlsimon/CSCI6365/final/gemini_api_key.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    api_key = file.read().strip()\n",
    "# print(api_key)\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Create a model instance (using Gemini 1.5 Flash in this case)\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM-Generated Summary:\n",
      "The provided text comprises abstracts from several research papers investigating diverse aspects of climate change and its impacts.  The studies cover a wide range of topics including:\n",
      "\n",
      "* **Impact on Evapotranspiration (ET):** One study assesses the spatial and temporal variations in ET in the Narmada river basin (India) using SEBAL and predicts future changes based on land use and climate change models (ACCESS1-0 and Markov Chain).\n",
      "\n",
      "* **Climate Change and Forest Fires:** Another study examines the relationship between climate change (using GRACE data) and forest fires in Yunnan province, China, analyzing the spatiotemporal distribution of fires and their correlation with hydrological and climatic factors.\n",
      "\n",
      "* **Climate Change Impacts on Marine Ecosystems:**  Research investigates the effect of climate change-induced alterations in prey quality (fatty acid composition) on juvenile Chinook salmon, focusing on their nutritional condition and growth.\n",
      "\n",
      "* **Stratospheric Ozone and Climate Change Interactions:** A study discusses the complex interplay between stratospheric ozone depletion, recovery (due to the Montreal Protocol), and climate change, highlighting the uncertainties involved.\n",
      "\n",
      "* **Climate Model Sensitivity:**  Analysis focuses on the increased equilibrium climate sensitivity (ECS) in the CNRM-CM6-1 climate model compared to its predecessor, attributing the increase to changes in atmospheric components, particularly cloud radiative responses.\n",
      "\n",
      "* **Climate Change and Human Migration:** Research explores the individual-level decision-making processes behind climate-induced migration, emphasizing the interplay of economic, social, environmental, and cultural factors in a vulnerable region.\n",
      "\n",
      "* **Climate Change and Groundwater:** A study investigates the impact of climate change on groundwater table fluctuations in Hungary's Great Hungarian Plain, utilizing hydrological models and climate projections to predict future groundwater levels.\n",
      "\n",
      "* **Climate Change Impacts on Vegetation:** Research uses a Land Surface Model (SURFEX) to simulate the impact of climate change on the biomass and soil water content of various vegetation types (cereals, grasslands, forests) in France.\n",
      "\n",
      "* **Climate Change and Flood Frequency:** A study assesses the implications of climate change for the bivariate quantiles of flood peak and volume in the Ganjiang River basin (China), using climate model outputs and hydrological modeling.\n",
      "\n",
      "* **Climate Change Mitigation and Adaptation in Agriculture:**  A systematic review analyzes the evidence for synergies and trade-offs between climate change mitigation and adaptation strategies in agriculture, revealing that claims of synergies may be overstated.\n",
      "\n",
      "\n",
      "The Wikipedia context provides background information on climate change, encompassing its causes, consequences, mitigation efforts, and the scientific consensus surrounding it, as well as the issue of climate change denial.  The research papers largely contribute to a deeper understanding of the multifaceted impacts of climate change across various geographical locations and ecological systems.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to summarize combined results using the LLM\n",
    "def summarize_results_with_llm(graph_results, wikipedia_context):\n",
    "    prompt = \"Summarize the following search results and Wikipedia context:\\n\\n\"\n",
    "\n",
    "    # Add graph results to the prompt\n",
    "    prompt += \"Graph Search Results:\\n\"\n",
    "    for node_type, idx, key, value in graph_results[:10]:  # Limit to top 10 results\n",
    "        prompt += f\"- Node Type: {node_type}, Property: {key}, Value: {str(value)[:MAX_VAL_LEN]}...\\n\"\n",
    "\n",
    "    # Add Wikipedia context to the prompt\n",
    "    prompt += \"\\nWikipedia Context:\\n\"\n",
    "    for i, context in enumerate(wikipedia_context, start=1):\n",
    "        prompt += f\"{i}. Title: {context['title']}\\n\"\n",
    "        prompt += f\"   Summary: {context['summary'][:MAX_VAL_LEN]}...\\n\"\n",
    "    \n",
    "    with open(\"prompt_file.txt\", \"w\") as file:\n",
    "        file.write(f\"{prompt}\\n\")\n",
    "\n",
    "    # Call the Gemini model to generate the summary\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Generate an LLM summary of the combined results\n",
    "summary = summarize_results_with_llm(graph_results, wikipedia_context)\n",
    "print(\"\\nLLM-Generated Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Use subgraph for additional information and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# specify the priorities to use in value selection\n",
    "def get_priority_properties():\n",
    "    priority_properties = {\n",
    "        'Dataset': ['longName', 'abstract', 'shortName'],\n",
    "        'Publication': ['title', 'abstract'],\n",
    "        'ScienceKeyword': ['name'],\n",
    "        'Instrument': ['longName', 'shortName'],\n",
    "        'Platform': ['longName', 'shortName'],\n",
    "        'Project': ['longName', 'shortName'],\n",
    "        'DataCenter': ['longName', 'shortName']\n",
    "    }\n",
    "    return priority_properties\n",
    "\n",
    "def create_nodes_of_interest(graph_results, max_per_type=3):\n",
    "    nodes_by_type = defaultdict(list)\n",
    "    for node_type, idx, key, value in graph_results:\n",
    "        nodes_by_type[node_type].append((idx, node_type, key, value))\n",
    "\n",
    "    # Select up to max_per_type nodes for each type\n",
    "    nodes_of_interest = []\n",
    "    for node_type, nodes in nodes_by_type.items():\n",
    "        nodes_of_interest.extend(random.sample(nodes, min(max_per_type, len(nodes))))\n",
    "\n",
    "    return nodes_of_interest\n",
    "\n",
    "def explore_subgraph_nodes(data, node_type, node_id, num_hops=2, max_per_type=3):\n",
    "    priority_properties = get_priority_properties()\n",
    "    subset, edge_index, edge_types = get_subgraph(data, node_type, torch.tensor([node_id]), num_hops=num_hops)\n",
    "\n",
    "    if subset is None:\n",
    "        return []\n",
    "\n",
    "    # Map node indices to their types and values\n",
    "    subgraph_nodes = []\n",
    "    print(f\"Number of nodes in subgraph for node_id: {node_id} = {len(subset)} | subset = {subset}\")\n",
    "\n",
    "    for sub_id in subset.tolist():\n",
    "        for sub_node_type in data.node_types:\n",
    "            num_nodes = data[sub_node_type].num_nodes\n",
    "            if sub_id < num_nodes:\n",
    "                # Attempt to find a meaningful property\n",
    "                value = None\n",
    "                for prop in priority_properties.get(sub_node_type, []):\n",
    "                    if prop in data[sub_node_type] and len(data[sub_node_type][prop]) > sub_id:\n",
    "                        value = data[sub_node_type][prop][sub_id]\n",
    "                        break\n",
    "                if value is None:  # Fallback to globalId or indicate no value\n",
    "                    value = data[sub_node_type].get('globalId', ['No value'])[sub_id] if 'globalId' in data[sub_node_type] else 'No value'\n",
    "                \n",
    "                subgraph_nodes.append((sub_id, sub_node_type, value))\n",
    "\n",
    "    # Group by node type and select a random subset of up to max_per_type nodes\n",
    "    nodes_by_type = defaultdict(list)\n",
    "    for node_id, node_type, value in subgraph_nodes:\n",
    "        nodes_by_type[node_type].append((node_id, node_type, value))\n",
    "\n",
    "    exploration_list = []\n",
    "    for node_type, nodes in nodes_by_type.items():\n",
    "        exploration_list.extend(random.sample(nodes, min(max_per_type, len(nodes))))\n",
    "\n",
    "    return exploration_list\n",
    "\n",
    "def write_exploration_to_file(data, graph_results, filename=\"graph_exploration.txt\"):\n",
    "    nodes_of_interest = create_nodes_of_interest(graph_results)\n",
    "\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(\"=== Nodes of Interest ===\\n\")\n",
    "        for idx, node_type, key, value in nodes_of_interest:\n",
    "            value_str = str(value)\n",
    "            display_value = value_str[:MAX_VAL_LEN] + (\"...\" if len(value_str) > MAX_VAL_LEN else \"\")\n",
    "            file.write(f\"ID: {idx}, Type: {node_type}, Key: {key}, Value: {display_value}\\n\")\n",
    "\n",
    "        file.write(\"\\n=== Subgraph Exploration ===\\n\")\n",
    "        for idx, node_type, key, value in nodes_of_interest:\n",
    "            file.write(f\"\\nExploring Subgraph for Node ID: {idx} (Type: {node_type})\\n\")\n",
    "            subgraph_nodes = explore_subgraph_nodes(data, node_type, idx)\n",
    "            for sub_id, sub_node_type, sub_value in subgraph_nodes:\n",
    "                sub_value_str = str(sub_value)\n",
    "                display_sub_value = sub_value_str[:MAX_VAL_LEN] + (\"...\" if len(sub_value_str) > MAX_VAL_LEN else \"\")\n",
    "                file.write(f\"  - ID: {sub_id}, Type: {sub_node_type}, Value: {display_sub_value}\\n\")\n",
    "\n",
    "    print(f\"\\nExploration results written to '{filename}'.\")\n",
    "\n",
    "\n",
    "def interactive_exploration(data):\n",
    "    priority_properties = get_priority_properties()\n",
    "    while True:\n",
    "        choice = input(\"\\nEnter a Node ID to explore further (or 'q' to quit): \")\n",
    "        if choice.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            node_id = int(choice)\n",
    "            node_type = input(\"Enter the Node Type (e.g., Dataset, ScienceKeyword, Instrument): \").strip()\n",
    "\n",
    "            # Validate the node type\n",
    "            if node_type not in data.node_types:\n",
    "                print(f\"Invalid node type '{node_type}'. Available types: {data.node_types}\")\n",
    "                continue\n",
    "\n",
    "            num_nodes = data[node_type].num_nodes\n",
    "            if node_id >= num_nodes:\n",
    "                print(f\"No node with ID: {node_id} in type '{node_type}'.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nSelected Node ID: {node_id} (Type: {node_type})\")\n",
    "            action = input(\"Enter 'wiki' to fetch Wikipedia context or 'subgraph' to explore subgraph of node: \").lower()\n",
    "\n",
    "            if action == 'wiki':\n",
    "                # Select a meaningful property using priority_properties\n",
    "                value = None\n",
    "                for prop in priority_properties.get(node_type, []):\n",
    "                    if prop in data[node_type] and len(data[node_type][prop]) > node_id:\n",
    "                        value = data[node_type][prop][node_id]\n",
    "                        break\n",
    "\n",
    "                if value is None:\n",
    "                    value = 'No value'\n",
    "\n",
    "                wikipedia_context = fetch_wikipedia_context([str(value)])\n",
    "                print(\"The prompt used for the Wikipedia context =\", str(value))\n",
    "                display_wikipedia_context(wikipedia_context)\n",
    "\n",
    "            elif action == 'subgraph':\n",
    "                subgraph_nodes = explore_subgraph_nodes(data, node_type, node_id)\n",
    "                print(f\"\\nSubgraph for Node ID: {node_id} (Type: {node_type})\")\n",
    "                for sub_id, sub_node_type, sub_value in subgraph_nodes:\n",
    "                    sub_value_str = str(sub_value)\n",
    "                    display_sub_value = sub_value_str[:MAX_VAL_LEN] + (\"...\" if len(sub_value_str) > MAX_VAL_LEN else \"\")\n",
    "                    print(f\"  - ID: {sub_id}, Type: {sub_node_type}, Value: {display_sub_value}\")\n",
    "\n",
    "            else:\n",
    "                print(\"Invalid action. Please enter 'wiki' or 'subgraph'.\")\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Invalid Node ID. Please enter a valid number.\")\n",
    "\n",
    "def run_exploration_tool(data, graph_results):\n",
    "    # Write initial exploration to file\n",
    "    write_exploration_to_file(data, graph_results)\n",
    "\n",
    "    # Start interactive exploration\n",
    "    interactive_exploration(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 60848 = 1 | subset = tensor([60848])\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 76606 = 1 | subset = tensor([76606])\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 42689 = 1 | subset = tensor([42689])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 3752 = 10 | subset = tensor([   6,   54,  492,  497,  500,  778,  940,  967, 1107, 3752])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 4253 = 10 | subset = tensor([   6,   54,  492,  497,  500,  778,  940,  967, 1107, 4253])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 4329 = 10 | subset = tensor([   6,   54,  492,  497,  500,  778,  940,  967, 1107, 4329])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 888 = 3 | subset = tensor([803, 887, 888])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 802 = 279 | subset = tensor([  70,   75,   90,  108,  123,  128,  129,  164,  174,  176,  182,  184,\n",
      "         226,  250,  271,  275,  281,  343,  459,  466,  802,  803,  887,  992,\n",
      "         996, 1182, 1704, 1707, 1714, 1736, 1772, 1775, 1786, 1820, 1831, 1838,\n",
      "        1846, 1858, 1860, 1905, 1920, 1922, 1939, 1957, 1998, 2010, 2013, 2020,\n",
      "        2098, 2101, 2117, 2137, 2168, 2184, 2208, 2229, 2245, 2250, 2258, 2275,\n",
      "        2293, 2299, 2303, 2323, 2344, 2345, 2347, 2361, 2391, 2409, 2424, 2431,\n",
      "        2520, 2522, 2526, 2559, 2591, 2599, 2607, 2612, 2632, 2647, 2670, 2685,\n",
      "        2689, 2700, 2703, 2705, 2732, 2739, 2759, 2779, 2803, 2806, 2844, 2854,\n",
      "        2865, 2874, 2875, 2877, 2912, 2922, 2946, 2953, 2960, 2978, 2983, 2985,\n",
      "        2998, 3005, 3011, 3017, 3023, 3026, 3033, 3036, 3221, 3276, 3335, 3362,\n",
      "        3383, 3409, 3415, 3430, 3628, 3680, 4149, 5080, 5088, 5089, 5094, 5095,\n",
      "        5096, 5101, 5105, 5106, 5108, 5111, 5120, 5121, 5129, 5133, 5140, 5141,\n",
      "        5146, 5148, 5153, 5156, 5161, 5165, 5167, 5171, 5197, 5198, 5208, 5233,\n",
      "        5238, 5243, 5244, 5247, 5252, 5253, 5259, 5272, 5273, 5293, 5295, 5302,\n",
      "        5303, 5319, 5330, 5342, 5348, 5355, 5361, 5366, 5367, 5371, 5372, 5373,\n",
      "        5376, 5381, 5389, 5392, 5393, 5394, 5404, 5421, 5426, 5430, 5432, 5437,\n",
      "        5442, 5449, 5458, 5459, 5461, 5470, 5473, 5477, 5478, 5480, 5483, 5494,\n",
      "        5497, 5498, 5506, 5511, 5517, 5530, 5537, 5538, 5541, 5546, 5557, 5566,\n",
      "        5567, 5571, 5574, 5576, 5580, 5598, 5604, 5607, 5611, 5625, 5632, 5640,\n",
      "        5651, 5656, 5665, 5666, 5674, 5675, 5676, 5678, 5679, 5681, 5686, 5695,\n",
      "        5696, 5699, 5702, 5704, 5705, 5711, 5713, 5728, 5729, 5735, 5736, 5741,\n",
      "        5742, 5744, 5755, 5760, 5765, 5767, 5783, 5798, 5803, 5816, 5827, 5830,\n",
      "        5834, 5837, 5843, 5846, 5848, 5851, 5852, 5867, 5868, 5872, 5873, 5874,\n",
      "        5898, 5906, 5907])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 96 = 82 | subset = tensor([  90,   96, 3220, 3222, 3224, 3225, 3227, 3230, 3231, 3246, 3248, 3262,\n",
      "        3264, 3270, 3286, 3301, 3305, 3307, 3310, 3312, 3313, 3315, 3318, 3327,\n",
      "        3330, 3347, 3350, 3358, 3372, 3375, 3377, 3379, 3382, 3389, 3393, 3397,\n",
      "        3399, 3400, 3410, 3414, 3416, 3418, 3422, 3428, 3432, 3433, 3440, 3442,\n",
      "        3449, 3452, 3462, 3476, 3481, 3485, 3492, 3493, 3494, 3496, 3504, 3511,\n",
      "        3512, 3517, 3522, 3529, 3531, 3532, 3546, 3561, 3564, 3568, 3575, 3576,\n",
      "        3577, 3578, 5102, 5118, 5188, 5347, 5405, 5410, 5712, 5789])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Number of nodes in subgraph for node_id: 105 = 7 | subset = tensor([ 105, 2245, 2347, 2431, 2689, 2844, 2877])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Number of nodes in subgraph for node_id: 121 = 26 | subset = tensor([ 121, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205,\n",
      "        3206, 3207, 3209, 3210, 3362, 3374, 3409, 3415, 3514, 3521, 5916, 5917,\n",
      "        5918, 5919])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Number of nodes in subgraph for node_id: 108 = 3 | subset = tensor([ 108, 2323, 2424])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Number of nodes in subgraph for node_id: 166 = 28 | subset = tensor([  69,   81,   85,  166,  335,  548,  554,  607,  673,  690,  697,  721,\n",
      "         735,  751,  752,  819,  834,  904,  946,  979,  994, 1095, 1723, 1742,\n",
      "        1779, 2029, 2467, 2760])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Number of nodes in subgraph for node_id: 409 = 576 | subset = tensor([  42,   69,   71,   76,  113,  152,  173,  180,  227,  291,  330,  409,\n",
      "         532,  533,  537,  541,  547,  549,  557,  559,  560,  561,  563,  566,\n",
      "         567,  568,  571,  575,  581,  582,  586,  588,  592,  595,  596,  597,\n",
      "         603,  604,  606,  608,  609,  612,  615,  617,  619,  623,  625,  627,\n",
      "         628,  631,  634,  637,  640,  641,  643,  646,  649,  651,  653,  655,\n",
      "         656,  659,  664,  665,  666,  668,  671,  674,  677,  678,  680,  684,\n",
      "         685,  689,  692,  693,  696,  709,  712,  714,  730,  732,  734,  735,\n",
      "         737,  741,  742,  743,  745,  755,  760,  762,  769,  771,  774,  776,\n",
      "         777,  779,  780,  787,  788,  789,  790,  795,  797,  799,  801,  804,\n",
      "         805,  808,  809,  811,  815,  817,  821,  822,  823,  828,  830,  832,\n",
      "         833,  836,  838,  841,  842,  843,  844,  846,  848,  851,  852,  854,\n",
      "         858,  859,  860,  861,  867,  868,  870,  874,  875,  878,  880,  888,\n",
      "         889,  895,  896,  899,  909,  921,  927,  932,  933,  934,  937,  938,\n",
      "         939,  942,  943,  952,  953,  955,  957,  958,  960,  964,  965,  970,\n",
      "         971,  976,  977,  985,  986,  992,  995,  998, 1001, 1002, 1004, 1006,\n",
      "        1007, 1009, 1011, 1012, 1013, 1014, 1015, 1019, 1022, 1024, 1025, 1034,\n",
      "        1043, 1044, 1045, 1046, 1048, 1049, 1051, 1054, 1055, 1062, 1063, 1064,\n",
      "        1068, 1072, 1074, 1078, 1079, 1085, 1088, 1090, 1091, 1092, 1093, 1097,\n",
      "        1098, 1099, 1100, 1103, 1104, 1106, 1109, 1110, 1111, 1115, 1118, 1122,\n",
      "        1124, 1126, 1129, 1130, 1134, 1135, 1139, 3610, 3611, 3614, 3617, 3620,\n",
      "        3632, 3635, 3636, 3646, 3650, 3657, 3658, 3659, 3670, 3672, 3674, 3675,\n",
      "        3677, 3682, 3694, 3695, 3706, 3721, 3722, 3723, 3734, 3745, 3746, 3776,\n",
      "        3777, 3783, 3784, 3791, 3793, 3795, 3799, 3804, 3810, 3821, 3829, 3830,\n",
      "        3835, 3847, 3855, 3860, 3864, 3868, 3872, 3874, 3877, 3883, 3891, 3899,\n",
      "        3902, 3906, 3916, 3934, 3940, 3943, 3947, 3949, 3950, 3953, 3954, 3973,\n",
      "        3975, 3978, 3985, 3998, 4002, 4012, 4014, 4019, 4026, 4027, 4032, 4043,\n",
      "        4045, 4054, 4055, 4058, 4068, 4072, 4081, 4099, 4123, 4136, 4157, 4161,\n",
      "        4165, 4167, 4168, 4170, 4171, 4181, 4199, 4200, 4205, 4208, 4211, 4213,\n",
      "        4221, 4235, 4251, 4252, 4260, 4262, 4273, 4288, 4292, 4295, 4301, 4302,\n",
      "        4307, 4311, 4315, 4319, 4329, 4340, 4347, 4350, 4352, 4368, 4371, 4372,\n",
      "        4380, 4388, 4406, 4410, 4414, 4415, 4421, 4434, 4438, 4459, 4461, 4468,\n",
      "        4470, 4480, 4485, 4496, 4499, 4500, 4501, 4511, 4515, 4529, 4530, 4533,\n",
      "        4543, 4548, 4549, 4556, 4579, 4582, 4586, 4587, 4590, 4595, 4596, 4615,\n",
      "        4621, 4625, 4627, 4629, 4635, 4643, 4656, 4665, 4671, 4694, 4695, 4697,\n",
      "        4717, 4722, 4740, 4749, 4752, 4774, 4784, 4788, 4792, 4799, 4802, 4809,\n",
      "        4816, 4821, 4843, 4847, 4858, 4859, 5130, 5145, 5232, 5235, 5244, 5252,\n",
      "        5278, 5286, 5288, 5295, 5304, 5314, 5376, 5393, 5511, 5594, 5625, 5656,\n",
      "        5681, 5729, 5768, 5830, 5890, 5937, 5938, 5939, 5940, 5941, 5942, 5943,\n",
      "        5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955,\n",
      "        5956, 5957, 5958, 5960, 5961, 5962, 5963, 5965, 5966, 5967, 5968, 5969,\n",
      "        5970, 5971, 5973, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983,\n",
      "        5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995,\n",
      "        5996, 5997, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008,\n",
      "        6009, 6010, 6011, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021,\n",
      "        6022, 6023, 6026, 6027, 6028, 6030, 6031, 6032, 6033, 6034, 6035, 6036,\n",
      "        6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048,\n",
      "        6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6060, 6061,\n",
      "        6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6071, 6072, 6073, 6287])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Number of nodes in subgraph for node_id: 173 = 580 | subset = tensor([  42,   69,   71,   76,  113,  152,  173,  532,  533,  537,  541,  547,\n",
      "         549,  557,  559,  560,  561,  563,  566,  567,  568,  571,  575,  581,\n",
      "         582,  586,  588,  592,  595,  596,  597,  603,  604,  606,  608,  609,\n",
      "         612,  615,  617,  619,  623,  625,  627,  628,  631,  634,  637,  640,\n",
      "         641,  643,  646,  649,  651,  653,  655,  656,  659,  664,  665,  666,\n",
      "         668,  671,  674,  677,  678,  680,  684,  685,  689,  692,  693,  696,\n",
      "         709,  712,  714,  730,  732,  734,  735,  737,  741,  742,  743,  745,\n",
      "         755,  760,  762,  769,  771,  774,  776,  777,  779,  780,  787,  788,\n",
      "         789,  790,  795,  797,  799,  801,  804,  805,  808,  809,  811,  815,\n",
      "         817,  821,  822,  823,  828,  830,  832,  833,  836,  838,  841,  842,\n",
      "         843,  844,  846,  848,  851,  852,  854,  858,  859,  860,  861,  867,\n",
      "         868,  870,  874,  875,  878,  880,  888,  889,  895,  896,  899,  909,\n",
      "         921,  927,  932,  933,  934,  937,  938,  939,  942,  943,  952,  953,\n",
      "         955,  957,  958,  960,  964,  965,  970,  971,  976,  977,  985,  986,\n",
      "         992,  995,  998, 1001, 1002, 1004, 1006, 1007, 1009, 1011, 1012, 1013,\n",
      "        1014, 1015, 1019, 1022, 1024, 1025, 1034, 1043, 1044, 1045, 1046, 1048,\n",
      "        1049, 1051, 1054, 1055, 1062, 1063, 1064, 1068, 1072, 1074, 1078, 1079,\n",
      "        1085, 1088, 1090, 1091, 1092, 1093, 1097, 1098, 1099, 1100, 1103, 1104,\n",
      "        1106, 1109, 1110, 1111, 1115, 1118, 1122, 1124, 1126, 1129, 1130, 1134,\n",
      "        1135, 1139, 1739, 1867, 1954, 1968, 2068, 2087, 2159, 2222, 2363, 2444,\n",
      "        2613, 2644, 2695, 2787, 2863, 2955, 3610, 3611, 3614, 3617, 3620, 3632,\n",
      "        3635, 3636, 3650, 3657, 3658, 3659, 3670, 3672, 3674, 3675, 3677, 3682,\n",
      "        3694, 3695, 3706, 3721, 3722, 3723, 3734, 3745, 3776, 3777, 3783, 3784,\n",
      "        3791, 3793, 3795, 3799, 3804, 3810, 3821, 3829, 3830, 3835, 3847, 3855,\n",
      "        3860, 3864, 3872, 3874, 3877, 3883, 3891, 3899, 3902, 3906, 3916, 3934,\n",
      "        3940, 3943, 3947, 3949, 3950, 3953, 3954, 3973, 3975, 3978, 3985, 3998,\n",
      "        4014, 4019, 4026, 4027, 4032, 4043, 4054, 4055, 4058, 4068, 4072, 4081,\n",
      "        4099, 4123, 4136, 4157, 4161, 4165, 4167, 4168, 4170, 4171, 4181, 4199,\n",
      "        4200, 4205, 4208, 4211, 4213, 4221, 4235, 4251, 4252, 4260, 4262, 4273,\n",
      "        4288, 4292, 4295, 4301, 4302, 4307, 4311, 4315, 4319, 4329, 4340, 4347,\n",
      "        4350, 4352, 4368, 4371, 4372, 4380, 4388, 4406, 4410, 4414, 4415, 4421,\n",
      "        4434, 4438, 4459, 4461, 4468, 4470, 4480, 4485, 4496, 4499, 4500, 4501,\n",
      "        4511, 4515, 4529, 4530, 4533, 4543, 4548, 4549, 4556, 4579, 4582, 4586,\n",
      "        4587, 4590, 4595, 4596, 4615, 4621, 4625, 4627, 4629, 4635, 4643, 4656,\n",
      "        4665, 4671, 4694, 4695, 4697, 4717, 4722, 4740, 4749, 4752, 4774, 4784,\n",
      "        4788, 4792, 4799, 4802, 4809, 4816, 4821, 4843, 4847, 4858, 4859, 5130,\n",
      "        5145, 5232, 5235, 5244, 5252, 5278, 5286, 5288, 5295, 5304, 5314, 5376,\n",
      "        5393, 5511, 5625, 5656, 5681, 5729, 5768, 5830, 5890, 5937, 5938, 5939,\n",
      "        5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951,\n",
      "        5952, 5953, 5954, 5955, 5956, 5957, 5958, 5960, 5961, 5962, 5963, 5965,\n",
      "        5966, 5967, 5968, 5969, 5970, 5971, 5973, 5975, 5976, 5977, 5978, 5979,\n",
      "        5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991,\n",
      "        5992, 5993, 5994, 5995, 5996, 5997, 5999, 6000, 6001, 6002, 6003, 6004,\n",
      "        6005, 6006, 6007, 6008, 6009, 6010, 6011, 6013, 6014, 6015, 6016, 6017,\n",
      "        6018, 6019, 6020, 6021, 6022, 6023, 6026, 6027, 6028, 6030, 6031, 6032,\n",
      "        6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044,\n",
      "        6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056,\n",
      "        6057, 6058, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069,\n",
      "        6071, 6072, 6073, 6287])\n",
      "\n",
      "Exploration results written to 'graph_exploration.txt'.\n",
      "\n",
      "Selected Node ID: 6 (Type: Project)\n",
      "result =  {'ns': 0, 'title': 'Earth Observing System', 'pageid': 471278, 'size': 29782, 'wordcount': 2019, 'snippet': 'The <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) is a program of NASA comprising a series of artificial satellite missions and scientific instruments in <span class=\"searchmatch\">Earth</span> orbit designed', 'timestamp': '2024-11-01T14:37:17Z'}\n",
      "result =  {'ns': 0, 'title': 'Aqua (satellite)', 'pageid': 830227, 'size': 14120, 'wordcount': 1300, 'snippet': 'component of the <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) preceded by Terra (launched 1999) and followed by Aura (launched 2004). The name &quot;<span class=\"searchmatch\">Aqua</span>&quot; comes from the Latin', 'timestamp': '2024-12-09T23:30:06Z'}\n",
      "result =  {'ns': 0, 'title': 'Terra (satellite)', 'pageid': 470910, 'size': 12209, 'wordcount': 1013, 'snippet': ' It is the flagship of the <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) and the first satellite of the <span class=\"searchmatch\">system</span> which was followed by <span class=\"searchmatch\">Aqua</span> (launched in 2002) and Aura', 'timestamp': '2024-12-09T23:31:38Z'}\n",
      "result =  {'ns': 0, 'title': 'List of Earth observation satellites', 'pageid': 1934667, 'size': 38842, 'wordcount': 1190, 'snippet': 'IAU. 12 August 2018. Retrieved 25 September 2021. &quot;qua <span class=\"searchmatch\">Earth</span>-<span class=\"searchmatch\">observing</span> satellite mission&quot;. <span class=\"searchmatch\">Aqua</span>.nasa.gov. Retrieved 10 August 2017. &quot;The Aura Mission&quot;', 'timestamp': '2024-12-05T03:30:45Z'}\n",
      "result =  {'ns': 0, 'title': 'Aura (satellite)', 'pageid': 827963, 'size': 14249, 'wordcount': 1240, 'snippet': 'It is the third major component of the <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) following on Terra (launched 1999) and <span class=\"searchmatch\">Aqua</span> (launched 2002). Aura follows on from the', 'timestamp': '2024-07-29T23:05:19Z'}\n",
      "The prompt used for the Wikipedia context = Earth Observing System (EOS), Aqua\n",
      "\n",
      "Wikipedia Context:\n",
      "\n",
      "Result 1:\n",
      "Title: Earth Observing System\n",
      "Description: NASA program involving satellites\n",
      "Summary: The Earth Observing System (EOS) is a program of NASA comprising a series of artificial satellite missions and scientific instruments in Earth orbit designed for long-term global observations of the land surface, biosphere, atmosphere, and oceans. Since the early 1970s, NASA has been developing its Earth Observing System, launching a series of Landsat satellites in the decade. Some of the first included passive microwave imaging in 1972 through the Nimbus 5 satellite. Following the launch of various satellite missions, the conception of the program began in the late 1980s and expanded rapidly through the 1990s. Since the inception of the program, it has continued to develop, including; land, sea, radiation and atmosphere. Collected in a system known as EOSDIS, NASA uses this data in order to study the progression and changes in the biosphere of Earth. The main focus of this data collection surrounds climatic science. The program is the centrepiece of NASA's Earth Science Enterprise.\n",
      "Link: https://en.wikipedia.org/wiki/Earth_Observing_System\n",
      "\n",
      "Result 2:\n",
      "Title: Aqua (satellite)\n",
      "Description: NASA scientific research satellite (2002–Present)\n",
      "Summary: Aqua is a NASA scientific research satellite in orbit around the Earth, studying the precipitation, evaporation, and cycling of water. It is the second major component of the Earth Observing System (EOS) preceded by Terra and followed by Aura.\n",
      "Link: https://en.wikipedia.org/wiki/Aqua_(satellite)\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Aqua_spacecraft_model.png/320px-Aqua_spacecraft_model.png\n",
      "\n",
      "Result 3:\n",
      "Title: Terra (satellite)\n",
      "Description: NASA climate research satellite (1999–Present)\n",
      "Summary: Terra is a multi-national scientific research satellite operated by NASA in a Sun-synchronous orbit around the Earth. It takes simultaneous measurements of Earth's atmosphere, land, and water to understand how Earth is changing and to identify the consequences for life on Earth. It is the flagship of the Earth Observing System (EOS) and the first satellite of the system which was followed by Aqua and Aura. Terra was launched in 1999.\n",
      "Link: https://en.wikipedia.org/wiki/Terra_(satellite)\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Terra_spacecraft_model.png/320px-Terra_spacecraft_model.png\n",
      "\n",
      "Result 4:\n",
      "Title: List of Earth observation satellites\n",
      "Description: No Description Available.\n",
      "Summary: Earth observation satellites are Earth-orbiting spacecraft with sensors used to collect imagery and measurements of the surface of the earth. These satellites are used to monitor short-term weather, long-term climate change, natural disasters. Earth observations satellites provide information for research subjects that benefit from looking at Earth’s surface from above. Types of sensors on these satellites include passive and active remote sensors. Sensors on Earth observation satellites often take measurements of emitted energy over some portion of the electromagnetic spectrum.\n",
      "Link: https://en.wikipedia.org/wiki/List_of_Earth_observation_satellites\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/Earth_from_Space.jpg/320px-Earth_from_Space.jpg\n",
      "\n",
      "Result 5:\n",
      "Title: Aura (satellite)\n",
      "Description: NASA Earth observation satellite (2004–Present)\n",
      "Summary: Aura is a multi-national NASA scientific research satellite in orbit around the Earth, studying the Earth's ozone layer, air quality and climate. It is the third major component of the Earth Observing System (EOS) following on Terra and Aqua. Aura follows on from the Upper Atmosphere Research Satellite (UARS). Aura is a joint mission between NASA, the Netherlands, Finland, and the U.K. The Aura spacecraft is healthy and is expected to operate until at least 2023, likely beyond.\n",
      "Link: https://en.wikipedia.org/wiki/Aura_(satellite)\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/8/88/Aura_spacecraft.png\n",
      "\n",
      "Selected Node ID: 6 (Type: Project)\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Number of nodes in subgraph for node_id: 6 = 1728 | subset = tensor([   6,   11,   12,  ..., 6346, 6347, 6353])\n",
      "\n",
      "Subgraph for Node ID: 6 (Type: Project)\n",
      "  - ID: 2499, Type: Dataset, Value: MLS/Aura Level 2 Temperature V005 (ML2T) at GES DISC\n",
      "  - ID: 1222, Type: Dataset, Value: MODIS/Aqua Calibrated Radiances 5-Min L1B Swath 250m - NRT\n",
      "  - ID: 2204, Type: Dataset, Value: HIRDLS/Aura Level 3 Chlorine Nitrate (ClONO2) 1deg Lat Zonal Fourier Coefficients V007 (H3ZFCCLONO2) at GES DISC\n",
      "  - ID: 46, Type: DataCenter, Value: Goddard Earth Sciences Data and Information Services Center (formerly Goddard DAAC), Global Change Data Center, Earth Sciences Division, Science and Exploration Directorate, Goddard Space Flight Center, NASA\n",
      "  - ID: 164, Type: DataCenter, Value: N/A\n",
      "  - ID: 33, Type: DataCenter, Value: Cyclone Global Navigation Satellite System Mission, NASA Earth Science System Pathfinder, University of Michigan\n",
      "  - ID: 109, Type: Project, Value: Gravity Recovery and Climate Experiment Data Assimilation for Drought Monitoring\n",
      "  - ID: 190, Type: Project, Value: Alpha Jet Atmospheric eXperiment\n",
      "  - ID: 199, Type: Project, Value: Clouds, Aerosol and Monsoon Processes-Philippines Experiment\n",
      "  - ID: 22, Type: Platform, Value: Digital Elevation Model\n",
      "  - ID: 384, Type: Platform, Value: TanDEM-X\n",
      "  - ID: 100, Type: Platform, Value: \n",
      "  - ID: 208, Type: Instrument, Value: Advanced Microwave Sounding Unit-A\n",
      "  - ID: 189, Type: Instrument, Value: Second Generation Airborne Precipitation Radar\n",
      "  - ID: 227, Type: Instrument, Value: Optical Disdrometer\n",
      "  - ID: 6, Type: ScienceKeyword, Value: GLOBAL POSITIONING SYSTEMS\n",
      "  - ID: 82, Type: ScienceKeyword, Value: DISASTER RESPONSE\n",
      "  - ID: 20, Type: ScienceKeyword, Value: SUBSETTING/SUPERSETTING\n",
      "  - ID: 4724, Type: Publication, Value: Human density impacts Nubian Flapshell turtle survival in Sub-Saharan Africa: Future conservation strategies\n",
      "  - ID: 2682, Type: Publication, Value: Coupling End-Member Mixing Analysis and Isotope Mass Balancing (222-Rn) for Differentiation of Fresh and Recirculated Submarine Groundwater Discharge Into Knysna Estuary, South Africa\n",
      "  - ID: 1699, Type: Publication, Value: The underappreciated role of anthropogenic sources in atmospheric soluble iron flux to the Southern Ocean\n"
     ]
    }
   ],
   "source": [
    "# Assumes `graph_results` contains the 50 search results\n",
    "run_exploration_tool(data, graph_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connections for node 76606 of type 'Publication': [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n"
     ]
    }
   ],
   "source": [
    "# See what kinds of edge types exist for a given node\n",
    "def check_node_connections(data, node_type, node_idx):\n",
    "    connections = []\n",
    "    for edge_type in data.edge_types:\n",
    "        edge_index = data[edge_type].edge_index\n",
    "        if node_idx in edge_index[0] or node_idx in edge_index[1]:\n",
    "            connections.append(edge_type)\n",
    "    return connections\n",
    "\n",
    "node_idx = 76606\n",
    "node_type = 'Publication'\n",
    "connections = check_node_connections(data, node_type, node_idx)\n",
    "print(f\"Connections for node {node_idx} of type '{node_type}': {connections}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Technique Analysis of CO2 in Troposphere using AIRS\n",
      "Abstract: CO2 pollutants (in this study on troposphere layer) and the data used are derived AIRS which  is  The result from the analysis is CO2 profile obtained from AIRS/Aqua L3 Monthly CO2\n",
      "URL: http://sunankalijaga.org/prosiding/index.php/icse/article/view/282\n",
      "\n",
      "Title: Seven years of observations of mid-tropospheric CO2 from the Atmospheric Infrared Sounder\n",
      "Abstract: 1, is a hyperspectral infrared instrument on the EOS Aqua Spacecraft, launched on May 4,   We are finding that the AIRS mid-tropospheric CO 2 is a good indicator of vertical motion in\n",
      "URL: https://www.sciencedirect.com/science/article/pii/S0094576511001457\n",
      "\n",
      "Title: Midtropospheric CO2 concentration retrieval from AIRS observations in the tropics\n",
      "Abstract: Atmospheric Infrared Sounder (AIRS), launched onboard the NASA's Aqua platform in May   sensitive to CO 2 and well covering the mid-to-high troposphere. Also flying onboard Aqua,\n",
      "URL: https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2004GL020141\n",
      "\n",
      "Title: The contribution of AIRS data to the estimation of CO2 sources and sinks\n",
      "Abstract: (AIRS) has been providing the first global maps of CO 2 concentrations in the cloud-free  upper troposphere This paper explores the usefulness of this data for the estimation of CO 2\n",
      "URL: https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2005GL024229\n",
      "\n",
      "Title: AIRS channel selection for CO2 and other trace‐gas retrievals\n",
      "Abstract: Space Administration’s Aqua satellite Atmospheric Infrared Sounder (AIRS) or the European   in the two CO2 bands. Indeed, for the lower troposphere, an increase of CO2 decreases the\n",
      "URL: https://rmets.onlinelibrary.wiley.com/doi/abs/10.1256/qj.02.180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test scholarly PiPy package\n",
    "from scholarly import scholarly\n",
    "\n",
    "def fetch_scholar_info(query):\n",
    "    try:\n",
    "        search_query = scholarly.search_pubs(query)\n",
    "        for i in range(5):  # Get top 3 results\n",
    "            paper = next(search_query)\n",
    "            print(f\"Title: {paper['bib']['title']}\")\n",
    "            print(f\"Abstract: {paper['bib'].get('abstract', 'No abstract available')}\")\n",
    "            print(f\"URL: {paper.get('pub_url', 'No URL available')}\\n\")\n",
    "    except StopIteration:\n",
    "        print(\"No results found on Google Scholar.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from Google Scholar: {e}\")\n",
    "\n",
    "# Example usage\n",
    "query = \"AIRS Aqua CO2 free troposphere\"\n",
    "fetch_scholar_info(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Nodes of Interest ===\n",
      "ID: 94077, Type: Publication, Key: abstract, Value: Yunnan province in China has rich forest resources but high forest fire frequency. Therefore, a better understanding of the relationship between climate change and forest fires in this region is important for forest fire prevention. This study used the Gravity Recovery and Climate Experiment (GRACE) terrestrial water storage change (TWSC) data to analyze the influence of climate change on forest fires in the region during 2003-2016. To improve the accuracy and reliability of GRACE TWSC data, we used the generalized three-cornered hat (GTCH) and the least square method to fuse TWSC data from six GRACE solutions. The spatiotemporal variation of forest fires during 2003-2016 was investigated using burned area data. Then, the relationship between burned area and hydrological and climatic factors was analyzed. The results indicate that more than 90% of burned areas are located in northwestern and southern Yunnan (NW and S). On the seasonal scale, forest fires are mainly concentrated in Janu\n",
      "ID: 76606, Type: Publication, Key: abstract, Value: Assessment of actual evapotranspiration (ET) is essential as it controls the exchange of water and heat energy between the atmosphere and land surface. ET also influences the available water resources and assists in the crop water assessment in agricultural areas. This study involves the assessment of spatial distribution of seasonal and annual ET using Surface Energy Balance Algorithm for Land (SEBAL) and provides an estimation of future changes in ET due to land use and climate change for a portion of the Narmada river basin in Central India. Climate change effects on future ET are assessed using the ACCESS1-0 model of CMIP5. A Markov Chain model estimated future land use based on the probability of changes in the past. The ET analysis is carried out for the years 2009–2011. The results indicate variation in the seasonal ET with the changed land use. High ET is observed over forest areas and crop lands, but ET decreases over crop lands after harvest. The overall annual ET is high ove\n",
      "ID: 55702, Type: Publication, Key: abstract, Value: The Environmental Effects Assessment Panel (EEAP) is one of three Panels of experts that inform the Parties to the Montreal Protocol. The EEAP focuses on the effects of UV radiation on human health, terrestrial and aquatic ecosystems, air quality, and materials, as well as on the interactive effects of UV radiation and global climate change. When considering the effects of climate change, it has become clear that processes resulting in changes in stratospheric ozone are more complex than previously held. Because of the Montreal Protocol, there are now indications of the beginnings of a recovery of stratospheric ozone, although the time required to reach levels like those before the 1960s is still uncertain, particularly as the effects of stratospheric ozone on climate change and vice versa, are not yet fully understood. Some regions will likely receive enhanced levels of UV radiation, while other areas will likely experience a reduction in UV radiation as ozone- and climate-driven chan\n",
      "ID: 2323, Type: Dataset, Key: abstract, Value: The National Climate Assessment - Land Data Assimilation System, or NCA-LDAS, is a terrestrial water reanalysis in support of the United States Global Change Research Program's NCA activities. NCA-LDAS features high resolution, gridded, daily time series data products of terrestrial water and energy balance stores, states, and fluxes over the continental U.S., derived from land surface hydrologic modeling with multivariate assimilation of satellite Environmental Data Records (EDRs). The overall goal is to provide the highest quality terrestrial hydrology products that enable improved scientific understanding, adaptation, and management of water and related energy resources during a changing climate.\n",
      "\n",
      "An overview of NCA-LDAS and its capability for developing climate change indicators are provided in Jasinski et al. (2019).  Details on the data assimilation used in NCA-LDAS are described in Kumar et al. (2019).  Sample mean annual trends are provided in the NCA-LDAS V2.0 README document.\n",
      "ID: 3752, Type: Dataset, Key: abstract, Value: CAL_LID_L3_Tropospheric_APro_CloudySkyOpaque-Standard-V4-20 is the Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observation (CALIPSO) Lidar Level 3 Tropospheric Aerosol Profiles, Cloudy Sky Opaque Data, Standard Version 4-20 data product. This data product was collected using the Cloud-Aerosol Lidar with Orthogonal Polarization (CALIOP) instrument. Data generation and distribution of this V4.20 product ended on July 1, 2020, to support a change in the operating system of the CALIPSO production clusters. The V4.21 data product covers July 1, 2020, to current. \n",
      "\n",
      "The CALIPSO lidar level 3 aerosol data product reports monthly mean profiles of aerosol optical properties on a uniform spatial grid. It is intended to be a tropospheric product, so data are only reported below altitudes of 12km. All level 3 parameters are derived from the version 4.20 CALIOP level 2 aerosol profile product and have been quality screened before averaging. The primary quantities reported are vertical pr\n",
      "ID: 4253, Type: Dataset, Key: abstract, Value: CAL_LID_L2_05kmMLay-Standard-V4-21 is the Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observations (CALIPSO) Lidar Level 2 5 km Merged Layer, Version 4-21 data product. Data for this product was collected using the CALIPSO Cloud-Aerosol Lidar with Orthogonal Polarization (CALIOP) instrument. The version of this product was changed from 4-20 to 4-21 to account for a change in the operating system of the CALIPSO production cluster. \n",
      "\n",
      "CALIPSO was launched on April 28, 2006, to study the impact of clouds and aerosols on the Earth's radiation budget and climate. It flies in the international A-Train constellation for coincident Earth observations. The CALIPSO satellite comprises three instruments: CALIOP, Imaging Infrared Radiometer (IIR), and Wide Field Camera (WFC). CALIPSO is a joint satellite mission between NASA and the French Agency CNES (Centre National d'Etudes Spatiales).\n",
      "ID: 72, Type: ScienceKeyword, Key: name, Value: CLIMATE ADVISORIES\n",
      "ID: 96, Type: ScienceKeyword, Key: name, Value: COUPLED CLIMATE MODELS\n",
      "ID: 989, Type: ScienceKeyword, Key: name, Value: ORBITAL CHANGE FORCING\n",
      "ID: 26, Type: Project, Key: longName, Value: Distributed Info. Services for Climate/Ocean Prod./Visualizations for Earth Res.\n",
      "ID: 137, Type: Project, Key: shortName, Value: NOAA CLIMATE DATA RECORD (CDR) PROGRAM\n",
      "ID: 108, Type: Project, Key: longName, Value: National Climate Assessment - Land Data Assimilation System\n",
      "ID: 344, Type: Platform, Key: shortName, Value: CLIMATE MODELS\n",
      "ID: 178, Type: Platform, Key: longName, Value: Constellation Observing System for Meteorology, Ionosphere and Climate\n",
      "ID: 307, Type: Platform, Key: longName, Value: Deep Space Climate Observatory\n",
      "\n",
      "=== Subgraph Exploration ===\n",
      "\n",
      "Exploring Subgraph for Node ID: 94077 (Type: Publication)\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 94077 = 1 | subset = tensor([94077])\n",
      "  - ID: 94077, Type: Publication, Value: Evolutionary leap in large-scale flood risk assessment needed\n",
      "\n",
      "Exploring Subgraph for Node ID: 76606 (Type: Publication)\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 76606 = 1 | subset = tensor([76606])\n",
      "  - ID: 76606, Type: Publication, Value: Impact of transport model resolution and a priori assumptions on inverse modeling of Swiss F-gas emissions\n",
      "\n",
      "Exploring Subgraph for Node ID: 55702 (Type: Publication)\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 55702 = 1 | subset = tensor([55702])\n",
      "  - ID: 55702, Type: Publication, Value: Soil CO2, CH4 and N2O fluxes in open lawns, treed lawns and urban woodlands in Angers, France\n",
      "\n",
      "Exploring Subgraph for Node ID: 2323 (Type: Dataset)\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 2323 = 22 | subset = tensor([   6,    8,   46,  478,  482,  486,  629,  716,  739,  871,  881,  886,\n",
      "         897,  924,  936,  939,  993, 1000, 1005, 1032, 1102, 2323])\n",
      "  - ID: 1005, Type: Dataset, Value: HURRICANE AND SEVERE STORM SENTINEL (HS3) GLOBAL HAWK HIGH ALTITUDE MMIC SOUNDING RADIOMETER (HAMSR) V1\n",
      "  - ID: 936, Type: Dataset, Value: Hurricane and Severe Storm Sentinel (HS3) Cooperative Institute for Meteorological Satellite Studies (CIMSS) Brightness Temperature V1\n",
      "  - ID: 2323, Type: Dataset, Value: NCA-LDAS Noah-3.3 Land Surface Model L4 Daily 0.125 x 0.125 degree V2.0 (NCALDAS_NOAH0125_D) at GES DISC\n",
      "  - ID: 8, Type: DataCenter, Value: MODIS Adaptive Processing System, Terrestrial Information Systems Laboratory, Earth Sciences Division, Science and Exploration Directorate, Goddard Space Flight Center, NASA\n",
      "  - ID: 46, Type: DataCenter, Value: Goddard Earth Sciences Data and Information Services Center (formerly Goddard DAAC), Global Change Data Center, Earth Sciences Division, Science and Exploration Directorate, Goddard Space Flight Center, NASA\n",
      "  - ID: 6, Type: DataCenter, Value: Land Processes Distributed Active Archive Center\n",
      "  - ID: 8, Type: Project, Value: National Polar Orbiting Partnership-Joint Polar Satellite System\n",
      "  - ID: 6, Type: Project, Value: Earth Observing System (EOS), Aqua\n",
      "  - ID: 46, Type: Project, Value: Hurricane and Severe Storm Sentinel\n",
      "  - ID: 6, Type: Platform, Value: Meteorological Operational Satellite - A\n",
      "  - ID: 8, Type: Platform, Value: Tropical Rainfall Measuring Mission\n",
      "  - ID: 46, Type: Platform, Value: ARGO profiler floats\n",
      "  - ID: 6, Type: Instrument, Value: Heitronics Wing IR Pyrometer\n",
      "  - ID: 629, Type: Instrument, Value: Chlorine Nitrate Instrument\n",
      "  - ID: 46, Type: Instrument, Value: Ecomapper Dissolved Oxygen\n",
      "  - ID: 478, Type: ScienceKeyword, Value: NOCTILUCENT CLOUDS\n",
      "  - ID: 871, Type: ScienceKeyword, Value: NORTH PACIFIC OSCILLATION\n",
      "  - ID: 1102, Type: ScienceKeyword, Value: CONSERVATION\n",
      "  - ID: 2323, Type: Publication, Value: Monthly hydroclimatology of the continental United States\n",
      "  - ID: 871, Type: Publication, Value: Analysis of groundwater dynamics in the complex aquifer system of Kazan Trona, Turkey, using environmental tracers and noble gases\n",
      "  - ID: 1032, Type: Publication, Value: Drivers of Pine Island Glacier speed-up between 1996 and 2016\n",
      "\n",
      "Exploring Subgraph for Node ID: 3752 (Type: Dataset)\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 3752 = 10 | subset = tensor([   6,   54,  492,  497,  500,  778,  940,  967, 1107, 3752])\n",
      "  - ID: 940, Type: Dataset, Value: ER-2 X-Band Doppler Radar (EXRAD) EPOCH\n",
      "  - ID: 6, Type: Dataset, Value: GHRSST Level 2P sub-skin Sea Surface Temperature from the Advanced Very High Resolution Radiometer (AVHRR) on Metop satellites (currently Metop-A) (GDS V2) produced by OSI SAF\n",
      "  - ID: 778, Type: Dataset, Value: Global Hawk Navigation EPOCH\n",
      "  - ID: 54, Type: DataCenter, Value: Atmospheric Science Data Center, Science Directorate, Langley Research Center, NASA\n",
      "  - ID: 6, Type: DataCenter, Value: Land Processes Distributed Active Archive Center\n",
      "  - ID: 54, Type: Project, Value: East Pacific Origins and Characteristics of Hurricanes\n",
      "  - ID: 6, Type: Project, Value: Earth Observing System (EOS), Aqua\n",
      "  - ID: 54, Type: Platform, Value: R/V Sarmiento De Gamboa (SPAIN)\n",
      "  - ID: 6, Type: Platform, Value: Meteorological Operational Satellite - A\n",
      "  - ID: 500, Type: Instrument, Value: Airborne Raman Ozone, Temperature, and Aerosol Lidar\n",
      "  - ID: 497, Type: Instrument, Value: 2B Technologies Nitric Oxide Monitor\n",
      "  - ID: 54, Type: Instrument, Value: Expendable Bathythermographs\n",
      "  - ID: 778, Type: ScienceKeyword, Value: CHLOROPHYLL\n",
      "  - ID: 492, Type: ScienceKeyword, Value: ACID RAIN\n",
      "  - ID: 967, Type: ScienceKeyword, Value: BOREHOLES\n",
      "  - ID: 500, Type: Publication, Value: The implications of maintaining Earth's hemispheric albedo symmetry for\n",
      "  - ID: 940, Type: Publication, Value: An improved optical flow method to estimate Arctic sea ice velocity (winter 20142016)\n",
      "  - ID: 778, Type: Publication, Value: Modeling of the Influence of Sea Ice Cycle and Langmuir Circulation on the Upper Ocean Mixed Layer Depth and Freshwater Distribution at the West Antarctic Peninsula\n",
      "\n",
      "Exploring Subgraph for Node ID: 4253 (Type: Dataset)\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 4253 = 10 | subset = tensor([   6,   54,  492,  497,  500,  778,  940,  967, 1107, 4253])\n",
      "  - ID: 54, Type: Dataset, Value: ASTER Level 1 precision terrain corrected registered at-sensor radiance V003\n",
      "  - ID: 4253, Type: Dataset, Value: CALIPSO Lidar Level 2 5 km Merged Layer, V4-21\n",
      "  - ID: 6, Type: Dataset, Value: GHRSST Level 2P sub-skin Sea Surface Temperature from the Advanced Very High Resolution Radiometer (AVHRR) on Metop satellites (currently Metop-A) (GDS V2) produced by OSI SAF\n",
      "  - ID: 54, Type: DataCenter, Value: Atmospheric Science Data Center, Science Directorate, Langley Research Center, NASA\n",
      "  - ID: 6, Type: DataCenter, Value: Land Processes Distributed Active Archive Center\n",
      "  - ID: 6, Type: Project, Value: Earth Observing System (EOS), Aqua\n",
      "  - ID: 54, Type: Project, Value: East Pacific Origins and Characteristics of Hurricanes\n",
      "  - ID: 54, Type: Platform, Value: R/V Sarmiento De Gamboa (SPAIN)\n",
      "  - ID: 6, Type: Platform, Value: Meteorological Operational Satellite - A\n",
      "  - ID: 6, Type: Instrument, Value: Heitronics Wing IR Pyrometer\n",
      "  - ID: 500, Type: Instrument, Value: Airborne Raman Ozone, Temperature, and Aerosol Lidar\n",
      "  - ID: 492, Type: Instrument, Value: Wide Field Camera\n",
      "  - ID: 54, Type: ScienceKeyword, Value: MARINE ADVISORIES\n",
      "  - ID: 500, Type: ScienceKeyword, Value: ICE PELLETS\n",
      "  - ID: 492, Type: ScienceKeyword, Value: ACID RAIN\n",
      "  - ID: 54, Type: Publication, Value: Radiative forcing under mixed aerosol conditions\n",
      "  - ID: 6, Type: Publication, Value: Arctic Sea Level During the Satellite Altimetry Era\n",
      "  - ID: 492, Type: Publication, Value: Urban Warming of the Two Most Populated Cities in the Canadian Province\n",
      "\n",
      "Exploring Subgraph for Node ID: 72 (Type: ScienceKeyword)\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 72 = 3 | subset = tensor([33, 70, 72])\n",
      "  - ID: 33, Type: Dataset, Value: ECOSTRESS Gridded Water Use Efficiency Instantaneous L4 Global 70 m V002\n",
      "  - ID: 70, Type: Dataset, Value: VIIRS/NPP BRDF/Albedo Valid Observation DNB Daily L3 Global 30ArcSec CMG V001\n",
      "  - ID: 72, Type: Dataset, Value: MODIS/Aqua Vegetation Indices Monthly L3 Global 0.05Deg CMG V061\n",
      "  - ID: 72, Type: DataCenter, Value: College of Forest Resources, University of Washington\n",
      "  - ID: 70, Type: DataCenter, Value: Danish Meteorological Institute, Center for Ocean and Ice\n",
      "  - ID: 33, Type: DataCenter, Value: Cyclone Global Navigation Satellite System Mission, NASA Earth Science System Pathfinder, University of Michigan\n",
      "  - ID: 72, Type: Project, Value: Earth Observing System (EOS), AQUA\n",
      "  - ID: 70, Type: Project, Value: Open-source Project for a Network Data Access Protocol\n",
      "  - ID: 33, Type: Project, Value: Convection and Moisture Experiment 3\n",
      "  - ID: 72, Type: Platform, Value: Global Precipitation Measurement\n",
      "  - ID: 33, Type: Platform, Value: National Oceanic & Atmospheric Administration-11\n",
      "  - ID: 70, Type: Platform, Value: Defense Meteorological Satellite Program-F14\n",
      "  - ID: 33, Type: Instrument, Value: Geostationary Operational Environmental Satellite 11-Imager\n",
      "  - ID: 72, Type: Instrument, Value: Visible and Infrared Spin Scan Radiometer (GMS Series)\n",
      "  - ID: 70, Type: Instrument, Value: Dual-frequency Precipitation Radar\n",
      "  - ID: 70, Type: ScienceKeyword, Value: WEATHER/CLIMATE ADVISORIES\n",
      "  - ID: 72, Type: ScienceKeyword, Value: CLIMATE ADVISORIES\n",
      "  - ID: 33, Type: ScienceKeyword, Value: ENVIRONMENTAL ADVISORIES\n",
      "  - ID: 72, Type: Publication, Value: Meso and microscale response to variation in cloudiness at three forested sites in the Maritime Continent\n",
      "  - ID: 70, Type: Publication, Value: Optimal stomatal behaviour under stochastic rainfall\n",
      "  - ID: 33, Type: Publication, Value: Spatial Patterns and Drivers of Nonperennial Flow Regimes in the Contiguous United States\n",
      "\n",
      "Exploring Subgraph for Node ID: 96 (Type: ScienceKeyword)\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 96 = 82 | subset = tensor([  90,   96, 3220, 3222, 3224, 3225, 3227, 3230, 3231, 3246, 3248, 3262,\n",
      "        3264, 3270, 3286, 3301, 3305, 3307, 3310, 3312, 3313, 3315, 3318, 3327,\n",
      "        3330, 3347, 3350, 3358, 3372, 3375, 3377, 3379, 3382, 3389, 3393, 3397,\n",
      "        3399, 3400, 3410, 3414, 3416, 3418, 3422, 3428, 3432, 3433, 3440, 3442,\n",
      "        3449, 3452, 3462, 3476, 3481, 3485, 3492, 3493, 3494, 3496, 3504, 3511,\n",
      "        3512, 3517, 3522, 3529, 3531, 3532, 3546, 3561, 3564, 3568, 3575, 3576,\n",
      "        3577, 3578, 5102, 5118, 5188, 5347, 5405, 5410, 5712, 5789])\n",
      "  - ID: 3399, Type: Dataset, Value: ECCO Sea-Ice and Snow Concentration and Thickness - Snapshot llc90 Grid (Version 4 Release 4)\n",
      "  - ID: 3225, Type: Dataset, Value: ECCO Ocean Temperature and Salinity - Daily Mean llc90 Grid (Version 4 Release 4)\n",
      "  - ID: 3379, Type: Dataset, Value: ECCO Gent-McWilliams Ocean Bolus Velocity - Daily Mean 0.5 Degree (Version 4 Release 4)\n",
      "  - ID: 96, Type: DataCenter, Value: N/A\n",
      "  - ID: 90, Type: DataCenter, Value: N/A\n",
      "  - ID: 96, Type: Project, Value: NASA Energy and Water Study\n",
      "  - ID: 90, Type: Project, Value: Cloud Absorption Radiometer\n",
      "  - ID: 90, Type: Platform, Value: FIXED OBSERVATION STATIONS\n",
      "  - ID: 96, Type: Platform, Value: \n",
      "  - ID: 90, Type: Instrument, Value: Inertial Navigation System\n",
      "  - ID: 96, Type: Instrument, Value: Forward Scattering Spectrometer Probe\n",
      "  - ID: 96, Type: ScienceKeyword, Value: COUPLED CLIMATE MODELS\n",
      "  - ID: 90, Type: ScienceKeyword, Value: MODELS\n",
      "  - ID: 96, Type: Publication, Value: Temporal patterns in species zonation in a mangrove forest in the Mekong Delta, Vietnam, using a time series of Landsat imagery\n",
      "  - ID: 3485, Type: Publication, Value: A hybrid approach for recovering high-resolution temporal gravity fields from satellite laser ranging\n",
      "  - ID: 5118, Type: Publication, Value: Effects of roads on individual caribou movements during migration\n",
      "\n",
      "Exploring Subgraph for Node ID: 989 (Type: ScienceKeyword)\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 989 = 3 | subset = tensor([945, 987, 989])\n",
      "  - ID: 989, Type: Dataset, Value: CAMEX-4 NOAA WP-3D CLOUD PHYSICS V1\n",
      "  - ID: 945, Type: Dataset, Value: RSS SSM/I OCEAN PRODUCT GRIDS 3-DAY AVERAGE FROM DMSP F8 NETCDF V7\n",
      "  - ID: 987, Type: Dataset, Value: LIS/OTD 0.5 Degree High Resolution Monthly Climatology (HRMC) V2.3.2015\n",
      "  - ID: 987, Type: ScienceKeyword, Value: PALEOCLIMATE FORCING\n",
      "  - ID: 945, Type: ScienceKeyword, Value: PALEOCLIMATE INDICATORS\n",
      "  - ID: 989, Type: ScienceKeyword, Value: ORBITAL CHANGE FORCING\n",
      "  - ID: 945, Type: Publication, Value: Correlation Analysis of CO2 Concentration Based on DMSP-OLS and\n",
      "  - ID: 989, Type: Publication, Value: PrecipGradeNet: A New Paradigm and Model for Precipitation Retrieval\n",
      "  - ID: 987, Type: Publication, Value: Assessment of Soil Moisture Anomaly Sensitivity to Detect Drought\n",
      "\n",
      "Exploring Subgraph for Node ID: 26 (Type: Project)\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Number of nodes in subgraph for node_id: 26 = 39 | subset = tensor([  26,  534,  539,  545,  555,  556,  572,  576,  594,  601,  610,  611,\n",
      "         624,  636,  676,  695,  711,  727,  757,  786,  831,  849,  862,  865,\n",
      "         873,  892,  910,  919,  945,  969,  984, 1033, 1037, 1050, 1057, 1066,\n",
      "        1119, 1120, 1127])\n",
      "  - ID: 539, Type: Dataset, Value: REGIONAL AIR-SEA INTERACTION (RASI) GAP WIND AND COASTAL UPWELLING EVENTS CLIMATOLOGY GULF OF TEHUANTEPEC, MEXICO V1\n",
      "  - ID: 1127, Type: Dataset, Value: RSS SSMIS OCEAN PRODUCT GRIDS WEEKLY AVERAGE FROM DMSP F17 NETCDF V7\n",
      "  - ID: 892, Type: Dataset, Value: RSS SSMIS OCEAN PRODUCT GRIDS MONTHLY AVERAGE FROM DMSP F17 NETCDF V7\n",
      "  - ID: 26, Type: DataCenter, Value: ASTER, JET PROPULSION LABORATORY, NASA\n",
      "  - ID: 26, Type: Project, Value: Distributed Info. Services for Climate/Ocean Prod./Visualizations for Earth Res.\n",
      "  - ID: 26, Type: Platform, Value: Goddard's LiDAR, Hyperspectral and Thermal (G-LiHT) airborne imaging system\n",
      "  - ID: 545, Type: Instrument, Value: Fourier Transform Infrared Spectrometer\n",
      "  - ID: 831, Type: Instrument, Value: Stevens Water HydraProbe\n",
      "  - ID: 601, Type: Instrument, Value: LI-6252 CO2 Analyzer\n",
      "  - ID: 545, Type: ScienceKeyword, Value: BRITTLE/BASKET STARS\n",
      "  - ID: 636, Type: ScienceKeyword, Value: CYANOBACTERIA (BLUE-GREEN ALGAE)\n",
      "  - ID: 1037, Type: ScienceKeyword, Value: ICE TEMPERATURE\n",
      "  - ID: 1033, Type: Publication, Value: Global predictions of primary soil salinization under changing climate in the 21st century\n",
      "  - ID: 539, Type: Publication, Value: Contribution of traffic-originated nanoparticle emissions to regional\n",
      "  - ID: 534, Type: Publication, Value: Validating and improving the uncertainty assumptions for the\n",
      "\n",
      "Exploring Subgraph for Node ID: 137 (Type: Project)\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Number of nodes in subgraph for node_id: 137 = 2 | subset = tensor([ 137, 3502])\n",
      "  - ID: 3502, Type: Dataset, Value: NOAA Smith and Reynolds Extended Reconstructed Sea Surface Temperature (ERSST) Level 4 Monthly Version 5 Dataset in netCDF\n",
      "  - ID: 137, Type: Dataset, Value: MODIS/Terra Land Surface Temperature/Emissivity Daily L3 Global 6km SIN Grid V061\n",
      "  - ID: 137, Type: DataCenter, Value: N/A\n",
      "  - ID: 137, Type: Project, Value: \n",
      "  - ID: 137, Type: Platform, Value: Advanced Ground Based Field Mill\n",
      "  - ID: 137, Type: Instrument, Value: GAS SENSORS\n",
      "  - ID: 137, Type: ScienceKeyword, Value: WEB PORTAL SERVICES\n",
      "  - ID: 3502, Type: Publication, Value: An inverse dielectric mixing model at 50 MHz that considers soil organic carbon\n",
      "  - ID: 137, Type: Publication, Value: fenics_ice 1.0: a framework for quantifying initialization uncertainty for time-dependent ice sheet models\n",
      "\n",
      "Exploring Subgraph for Node ID: 108 (Type: Project)\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Number of nodes in subgraph for node_id: 108 = 3 | subset = tensor([ 108, 2323, 2424])\n",
      "  - ID: 2424, Type: Dataset, Value: NCA-LDAS Noah-3.3 Land Surface Model L4 Trends 0.125 x 0.125 degree V2.0 (NCALDAS_NOAH0125_Trends) at GES DISC\n",
      "  - ID: 108, Type: Dataset, Value: Vegetation Index and Phenology (VIP) Phenology NDVI Yearly Global 0.05Deg CMG V004\n",
      "  - ID: 2323, Type: Dataset, Value: NCA-LDAS Noah-3.3 Land Surface Model L4 Daily 0.125 x 0.125 degree V2.0 (NCALDAS_NOAH0125_D) at GES DISC\n",
      "  - ID: 108, Type: DataCenter, Value: N/A\n",
      "  - ID: 108, Type: Project, Value: National Climate Assessment - Land Data Assimilation System\n",
      "  - ID: 108, Type: Platform, Value: \n",
      "  - ID: 108, Type: Instrument, Value: Optical Transient Detector\n",
      "  - ID: 108, Type: ScienceKeyword, Value: SOCIAL AND ECONOMIC MODELS\n",
      "  - ID: 2323, Type: Publication, Value: Monthly hydroclimatology of the continental United States\n",
      "  - ID: 2424, Type: Publication, Value: Human appropriated net primary productivity as a metric for land use\n",
      "  - ID: 108, Type: Publication, Value: Air-sea CO2 exchange process in the southern Yellow Sea in April of 2011, and June, July, October of 2012\n",
      "\n",
      "Exploring Subgraph for Node ID: 344 (Type: Platform)\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Number of nodes in subgraph for node_id: 344 = 22 | subset = tensor([  69,   81,   85,   98,  139,  192,  214,  215,  251,  285,  292,  344,\n",
      "         359,  360,  362,  363,  373,  386, 3189, 4247, 4539, 4573])\n",
      "  - ID: 4539, Type: Dataset, Value: Tropical Ozone Transport Experiment - Vortex Ozone Transport Experiment (TOTE-VOTE) DC-8 Ancillary Data\n",
      "  - ID: 373, Type: Dataset, Value: MODIS/Terra+Aqua BRDF/Albedo QA ValidobsBand4 Daily L3 Global 30ArcSec CMG V061\n",
      "  - ID: 386, Type: Dataset, Value: ASTER Digital Elevation Model V003\n",
      "  - ID: 139, Type: DataCenter, Value: N/A\n",
      "  - ID: 98, Type: DataCenter, Value: N/A\n",
      "  - ID: 85, Type: DataCenter, Value: N/A\n",
      "  - ID: 251, Type: Project, Value: ICECAP University of Texas, Antarctica 2016\n",
      "  - ID: 69, Type: Project, Value: Earth Science Information Partners Program\n",
      "  - ID: 215, Type: Project, Value: Commercial Smallsat Data Acquisition Program\n",
      "  - ID: 69, Type: Platform, Value: Ground Stations\n",
      "  - ID: 292, Type: Platform, Value: \n",
      "  - ID: 251, Type: Platform, Value: Model\n",
      "  - ID: 214, Type: Instrument, Value: WIND VANES\n",
      "  - ID: 285, Type: Instrument, Value: Atmospheric Trace Molecule Spectroscopy\n",
      "  - ID: 359, Type: Instrument, Value: Rapid Scatterometer\n",
      "  - ID: 292, Type: ScienceKeyword, Value: NITROGEN OXIDES\n",
      "  - ID: 192, Type: ScienceKeyword, Value: ANIMAL NUTRITION\n",
      "  - ID: 139, Type: ScienceKeyword, Value: CHANGE DETECTION SERVICES\n",
      "  - ID: 363, Type: Publication, Value: Validation of ICESat-2 terrain and canopy heights in boreal forests\n",
      "  - ID: 344, Type: Publication, Value: Uncertainties in carbon residence time and NPP-driven carbon uptake in\n",
      "                        terrestrial ecosystems of the conterminous USA: a Bayesian approach\n",
      "  - ID: 98, Type: Publication, Value: Southern Ocean Phytoplankton Community Structure as a Gatekeeper for\n",
      "\n",
      "Exploring Subgraph for Node ID: 178 (Type: Platform)\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Number of nodes in subgraph for node_id: 178 = 358 | subset = tensor([   0,   31,   32,   33,   34,   65,   69,   78,   80,   81,   85,   88,\n",
      "          90,   98,  126,  127,  128,  129,  130,  154,  178,  263,  283,  292,\n",
      "         300,  355,  536,  538,  542,  558,  564,  565,  574,  583,  589,  591,\n",
      "         602,  618,  621,  630,  632,  635,  643,  644,  661,  663,  667,  672,\n",
      "         694,  704,  718,  723,  725,  728,  729,  731,  735,  746,  749,  756,\n",
      "         759,  764,  767,  770,  772,  796,  814,  818,  835,  847,  855,  857,\n",
      "         863,  876,  898,  901,  906,  912,  913,  914,  922,  931,  935,  949,\n",
      "         954,  962,  966,  972,  974,  975,  990, 1017, 1023, 1028, 1031, 1036,\n",
      "        1065, 1081, 1086, 1096, 1117, 1128, 1136, 1749, 1904, 2017, 2043, 2110,\n",
      "        2142, 2269, 2411, 2457, 2822, 3604, 3605, 3622, 3630, 3653, 3659, 3664,\n",
      "        3666, 3667, 3678, 3697, 3699, 3717, 3733, 3736, 3748, 3754, 3756, 3770,\n",
      "        3772, 3773, 3801, 3802, 3809, 3816, 3822, 3828, 3840, 3850, 3858, 3870,\n",
      "        3886, 3887, 3890, 3907, 3908, 3915, 3918, 3919, 3922, 3923, 3929, 3960,\n",
      "        3961, 3962, 3970, 3984, 4001, 4003, 4034, 4036, 4048, 4063, 4069, 4071,\n",
      "        4078, 4082, 4083, 4089, 4094, 4102, 4107, 4124, 4126, 4132, 4149, 4166,\n",
      "        4174, 4191, 4195, 4197, 4198, 4203, 4219, 4227, 4229, 4234, 4244, 4245,\n",
      "        4248, 4256, 4257, 4263, 4266, 4268, 4278, 4279, 4287, 4312, 4313, 4314,\n",
      "        4316, 4325, 4327, 4333, 4356, 4362, 4363, 4367, 4378, 4383, 4387, 4400,\n",
      "        4420, 4424, 4429, 4440, 4446, 4448, 4449, 4456, 4464, 4468, 4472, 4479,\n",
      "        4481, 4483, 4503, 4514, 4525, 4539, 4557, 4561, 4564, 4571, 4574, 4585,\n",
      "        4589, 4593, 4602, 4603, 4608, 4612, 4626, 4646, 4655, 4659, 4669, 4673,\n",
      "        4678, 4679, 4680, 4691, 4699, 4702, 4704, 4705, 4707, 4712, 4718, 4721,\n",
      "        4727, 4737, 4753, 4757, 4763, 4778, 4793, 4798, 4810, 4822, 4826, 4830,\n",
      "        4838, 4840, 4865, 5087, 5109, 5113, 5115, 5144, 5162, 5200, 5205, 5217,\n",
      "        5227, 5242, 5248, 5264, 5285, 5287, 5289, 5297, 5337, 5343, 5344, 5354,\n",
      "        5357, 5369, 5370, 5377, 5378, 5396, 5402, 5406, 5413, 5414, 5424, 5426,\n",
      "        5438, 5442, 5446, 5447, 5457, 5464, 5472, 5486, 5508, 5509, 5513, 5518,\n",
      "        5522, 5530, 5548, 5552, 5568, 5586, 5600, 5619, 5627, 5641, 5667, 5672,\n",
      "        5683, 5684, 5688, 5722, 5726, 5728, 5732, 5753, 5761, 5776, 5778, 5796,\n",
      "        5821, 5823, 5840, 5859, 5862, 5874, 5877, 5882, 5910, 5911])\n",
      "  - ID: 5486, Type: Dataset, Value: IceBridge Narrow Swath ATM L1B Elevation and Return Strength with Waveforms V001\n",
      "  - ID: 3887, Type: Dataset, Value: Tropical Ozone Transport Experiment – Vortex Ozone Transport Experiment (TOTE-VOTE) DC-8 Remotely Sensed Lidar Data\n",
      "  - ID: 4227, Type: Dataset, Value: NAAMES C-130 Ocean Remote Sensing Data, Version 1\n",
      "  - ID: 78, Type: DataCenter, Value: AQUARIUS, Jet Propulsion Laboratory, NASA\n",
      "  - ID: 154, Type: DataCenter, Value: N/A\n",
      "  - ID: 80, Type: DataCenter, Value: N/A\n",
      "  - ID: 292, Type: Project, Value: Pre-IceBridge NASA ATM Suite Antarctica 1997\n",
      "  - ID: 263, Type: Project, Value: Operation IceBridge Alaska Glaciers 2009\n",
      "  - ID: 69, Type: Project, Value: Earth Science Information Partners Program\n",
      "  - ID: 283, Type: Platform, Value: Lockheed P-3B Orion\n",
      "  - ID: 355, Type: Platform, Value: Douglas DC-8\n",
      "  - ID: 126, Type: Platform, Value: National Oceanic & Atmospheric Administration-10\n",
      "  - ID: 129, Type: Instrument, Value: RADIOSONDES\n",
      "  - ID: 130, Type: Instrument, Value: RAWINSONDES\n",
      "  - ID: 767, Type: Instrument, Value: \n",
      "  - ID: 814, Type: ScienceKeyword, Value: HUMIDITY INDEX\n",
      "  - ID: 764, Type: ScienceKeyword, Value: REEF\n",
      "  - ID: 1065, Type: ScienceKeyword, Value: ECONOMIC RESOURCES\n",
      "  - ID: 901, Type: Publication, Value: Computation Approach for Quantitative Dielectric Constant from Time\n",
      "  - ID: 4234, Type: Publication, Value: Environmental predictors of forest change: An analysis of natural predisposition to deforestation in the tropical Andes region, Peru\n",
      "  - ID: 672, Type: Publication, Value: Recent Increases in Exposure to Extreme Humid-Heat Events\n",
      "\n",
      "Exploring Subgraph for Node ID: 307 (Type: Platform)\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Number of nodes in subgraph for node_id: 307 = 38 | subset = tensor([  69,   81,  117,  124,  139,  200,  307,  401, 1894, 2016, 2400, 2450,\n",
      "        2498, 2668, 2974, 3782, 3789, 3792, 3796, 3946, 3990, 3997, 4092, 4210,\n",
      "        4223, 4259, 4269, 4357, 4366, 4369, 4409, 4620, 4729, 4743, 4760, 4790,\n",
      "        4794, 4856])\n",
      "  - ID: 3782, Type: Dataset, Value: DSCOVR EPIC Level 2 EPICAERUV-Fast\n",
      "  - ID: 4620, Type: Dataset, Value: DSCOVR EPIC Level 1B Version 3\n",
      "  - ID: 3796, Type: Dataset, Value: EPIC-view satellite composites for DSCOVR, Version 1\n",
      "  - ID: 69, Type: DataCenter, Value: Canadian Meteorological Centre, Meteorological Service of Canada, Environment Canada\n",
      "  - ID: 124, Type: DataCenter, Value: N/A\n",
      "  - ID: 81, Type: DataCenter, Value: N/A\n",
      "  - ID: 200, Type: Project, Value: CALIPSO Night Validation Flights\n",
      "  - ID: 81, Type: Project, Value: The second Modern-Era Retrospective analysis for Research and Applications\n",
      "  - ID: 124, Type: Project, Value: Estimating the Circulation and Climate of the Ocean\n",
      "  - ID: 69, Type: Platform, Value: Ground Stations\n",
      "  - ID: 124, Type: Platform, Value: WEATHER STATIONS\n",
      "  - ID: 139, Type: Platform, Value: FIELD SURVEYS\n",
      "  - ID: 117, Type: Instrument, Value: Micro Rain Radar\n",
      "  - ID: 69, Type: Instrument, Value: Visible and Infrared Spin Scan Radiometer (METEOSAT Series)\n",
      "  - ID: 139, Type: Instrument, Value: \n",
      "  - ID: 81, Type: ScienceKeyword, Value: DISASTER RECOVERY/RELIEF\n",
      "  - ID: 69, Type: ScienceKeyword, Value: SOLAR WINDS\n",
      "  - ID: 117, Type: ScienceKeyword, Value: ASK-A BIOLOGIST\n",
      "  - ID: 4790, Type: Publication, Value: Why Do CMIP6 Models Fail to Simulate Snow Depth in Terms of Temporal Change and High Mountain Snow of China Skillfully?\n",
      "  - ID: 1894, Type: Publication, Value: Ideas and perspectives: Alleviation of functional limitations by soil\n",
      "  - ID: 81, Type: Publication, Value: Ice and mixed-phase cloud statistics on the Antarctic Plateau\n"
     ]
    }
   ],
   "source": [
    "# Function to display nodes of interest and their subgraphs in the terminal\n",
    "def display_nodes_and_subgraphs(data, graph_results):\n",
    "    nodes_of_interest = create_nodes_of_interest(graph_results)\n",
    "\n",
    "    print(\"\\n=== Nodes of Interest ===\")\n",
    "    for idx, node_type, key, value in nodes_of_interest:\n",
    "        print(f\"ID: {idx}, Type: {node_type}, Key: {key}, Value: {str(value)[:MAX_VAL_LEN]}\")\n",
    "    \n",
    "    print(\"\\n=== Subgraph Exploration ===\")\n",
    "    for idx, node_type, key, value in nodes_of_interest:\n",
    "        print(f\"\\nExploring Subgraph for Node ID: {idx} (Type: {node_type})\")\n",
    "        subgraph_nodes = explore_subgraph_nodes(data, node_type, idx)\n",
    "        for sub_id, sub_node_type, sub_value in subgraph_nodes:\n",
    "            sub_value_str = str(sub_value)[:MAX_VAL_LEN]\n",
    "            display_sub_value = sub_value_str + (\"...\" if len(sub_value_str) > MAX_VAL_LEN else \"\")\n",
    "            print(f\"  - ID: {sub_id}, Type: {sub_node_type}, Value: {display_sub_value}\")\n",
    "\n",
    "# Run the display function\n",
    "display_nodes_and_subgraphs(data, graph_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Number of nodes in subgraph for node_id: 6 = 1728 | subset = tensor([   6,   11,   12,  ..., 6346, 6347, 6353])\n",
      "\n",
      "Generating detailed LLM summary using Wikipedia, subgraph nodes, and scholarly articles...\n",
      "result =  {'ns': 0, 'title': 'Earth Observing System', 'pageid': 471278, 'size': 29782, 'wordcount': 2019, 'snippet': 'The <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) is a program of NASA comprising a series of artificial satellite missions and scientific instruments in <span class=\"searchmatch\">Earth</span> orbit designed', 'timestamp': '2024-11-01T14:37:17Z'}\n",
      "result =  {'ns': 0, 'title': 'Aqua (satellite)', 'pageid': 830227, 'size': 14120, 'wordcount': 1300, 'snippet': 'component of the <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) preceded by Terra (launched 1999) and followed by Aura (launched 2004). The name &quot;<span class=\"searchmatch\">Aqua</span>&quot; comes from the Latin', 'timestamp': '2024-12-09T23:30:06Z'}\n",
      "result =  {'ns': 0, 'title': 'Terra (satellite)', 'pageid': 470910, 'size': 12209, 'wordcount': 1013, 'snippet': ' It is the flagship of the <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) and the first satellite of the <span class=\"searchmatch\">system</span> which was followed by <span class=\"searchmatch\">Aqua</span> (launched in 2002) and Aura', 'timestamp': '2024-12-09T23:31:38Z'}\n",
      "result =  {'ns': 0, 'title': 'List of Earth observation satellites', 'pageid': 1934667, 'size': 38842, 'wordcount': 1190, 'snippet': 'IAU. 12 August 2018. Retrieved 25 September 2021. &quot;qua <span class=\"searchmatch\">Earth</span>-<span class=\"searchmatch\">observing</span> satellite mission&quot;. <span class=\"searchmatch\">Aqua</span>.nasa.gov. Retrieved 10 August 2017. &quot;The Aura Mission&quot;', 'timestamp': '2024-12-05T03:30:45Z'}\n",
      "result =  {'ns': 0, 'title': 'Aura (satellite)', 'pageid': 827963, 'size': 14249, 'wordcount': 1240, 'snippet': 'It is the third major component of the <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) following on Terra (launched 1999) and <span class=\"searchmatch\">Aqua</span> (launched 2002). Aura follows on from the', 'timestamp': '2024-07-29T23:05:19Z'}\n",
      "\n",
      "=== Combined LLM Summary ===\n",
      "This information describes a subgraph centered around the Earth Observing System (EOS) Aqua satellite, a key component of NASA's larger Earth Observing System.  Let's break down the relationships between the different node types:\n",
      "\n",
      "**Central Focus: EOS Aqua (Project Node)**\n",
      "\n",
      "The central project is the EOS Aqua satellite, a NASA mission dedicated to studying Earth's water cycle (precipitation, evaporation, and cycling).  This is supported by the provided Wikipedia summaries and scholarly articles.  Aqua is part of the larger EOS program, which involves a series of satellites designed for comprehensive global observations.  Other EOS satellites mentioned include Terra and Aura, each with different scientific focuses.\n",
      "\n",
      "**Related Datasets (Dataset Nodes):**\n",
      "\n",
      "Several datasets are linked to Aqua.  For example, `AIRS/Aqua L2 Support Retrieval` demonstrates data produced by the Atmospheric Infrared Sounder (AIRS) instrument aboard Aqua.  The `ECOSTRESS Evapotranspiration` dataset, while not explicitly stated as coming from Aqua, likely uses data influenced by or complementary to Aqua's observations.  The presence of `INTEX-B DC-8 Aircraft data` suggests ground-truthing or complementary airborne measurements were taken in conjunction with Aqua's satellite observations.\n",
      "\n",
      "**Data Sources and Infrastructure (DataCenter and Platform Nodes):**\n",
      "\n",
      "The `DataCenter` nodes are marked as \"N/A,\" indicating a missing link in the data.  This needs further investigation to understand where the data associated with the listed datasets are archived and accessed.  `Platform` nodes like \"Ground Stations\" are essential for receiving and processing data from Aqua.  The inclusion of an aircraft platform (`Short Brothers C-23 Sherpa`) and an unspecified platform suggests the use of diverse data collection methods beyond the Aqua satellite itself.\n",
      "\n",
      "**Associated Instruments (Instrument Nodes):**\n",
      "\n",
      "The `AIRS` instrument is explicitly mentioned in one dataset.  The inclusion of other instruments like the `Advanced Himawari Imager` and `Raman Lidar` suggests the analysis may incorporate data from other sources, potentially for comparison or validation purposes.  The unspecified instrument node indicates other instruments' involvement, requiring further investigation.\n",
      "\n",
      "**Research Context (ScienceKeyword, Publication Nodes):**\n",
      "\n",
      "`ScienceKeyword` nodes such as `SEA LEVEL PRESSURE` and `MARICULTURE PRODUCTION` suggest the types of scientific questions being addressed using the data.  These keywords highlight the diverse applications of EOS Aqua data, ranging from climate studies to impact assessments on human activities.  The `Publication` nodes indicate research papers using EOS Aqua data, e.g., those related to evapotranspiration and fire activity.  These publications provide context and show how the data contributes to scientific knowledge.\n",
      "\n",
      "**Related Projects (Project Nodes):**\n",
      "\n",
      "Other projects are listed, including `Operation IceBridge`, which focuses on polar ice measurements, and a working group focusing on remote sensing data needs.  The inclusion of these projects hints at the broader scientific context and the collaborative nature of Earth observation research.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "The subgraph represents a network of data, instruments, projects, and publications connected through the EOS Aqua satellite mission. It demonstrates a complex interplay between satellite remote sensing, ground-based and airborne observations, data processing and dissemination, and ultimately, the scientific publications that derive knowledge from this complex data ecosystem.  Many nodes are marked \"N/A\" highlighting the incompleteness of this specific subgraph, which requires further detail to fully understand the connections and data flows.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_combined_rag_summary(node_type, node_value, subgraph_nodes, scholarly_query):\n",
    "    # Fetch Wikipedia context\n",
    "    wikipedia_context = fetch_wikipedia_context([str(node_value)])\n",
    "    \n",
    "    # Fetch scholarly articles\n",
    "    scholarly_results = []\n",
    "    try:\n",
    "        search_query = scholarly.search_pubs(scholarly_query)\n",
    "        for _ in range(3):  # Limit to top 3 results\n",
    "            paper = next(search_query)\n",
    "            scholarly_results.append({\n",
    "                'title': paper['bib']['title'],\n",
    "                'abstract': paper['bib'].get('abstract', 'No abstract available'),\n",
    "                'url': paper.get('pub_url', 'No URL available')\n",
    "            })\n",
    "    except StopIteration:\n",
    "        scholarly_results.append({'title': 'No results found on Google Scholar', 'abstract': '', 'url': ''})\n",
    "    except Exception as e:\n",
    "        scholarly_results.append({'title': f\"Error fetching data: {e}\", 'abstract': '', 'url': ''})\n",
    "\n",
    "    # Prepare prompt for Gemini model\n",
    "    prompt = f\"Provide a detailed explanation based on the following information:\\n\\n\"\n",
    "    prompt += f\"Node Type: {node_type}\\nNode Value: {node_value}\\n\\n\"\n",
    "\n",
    "    # Add Wikipedia context\n",
    "    if wikipedia_context:\n",
    "        prompt += \"Wikipedia Context:\\n\"\n",
    "        for result in wikipedia_context:\n",
    "            prompt += f\"- Title: {result['title']}\\n  Summary: {result['summary'][:300]}...\\n\"\n",
    "\n",
    "    # Add subgraph nodes\n",
    "    prompt += \"\\nSubgraph Nodes:\\n\"\n",
    "    for sub_id, sub_node_type, sub_value in subgraph_nodes:\n",
    "        prompt += f\"- ID: {sub_id}, Type: {sub_node_type}, Value: {str(sub_value)[:300]}...\\n\"\n",
    "\n",
    "    # Add scholarly articles\n",
    "    prompt += \"\\nScholarly Articles:\\n\"\n",
    "    for paper in scholarly_results:\n",
    "        prompt += f\"- Title: {paper['title']}\\n  Abstract: {paper['abstract'][:300]}...\\n  URL: {paper['url']}\\n\"\n",
    "\n",
    "    # Generate LLM summary using the Gemini model\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "def interactive_rag_exploration(data):\n",
    "    priority_properties = get_priority_properties()\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"\\nEnter a Node ID to explore further (or 'q' to quit): \")\n",
    "        if choice.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            node_id = int(choice)\n",
    "            node_type = input(\"Enter the Node Type (e.g., Dataset, ScienceKeyword, Instrument): \").strip()\n",
    "\n",
    "            # Validate node type\n",
    "            if node_type not in data.node_types:\n",
    "                print(f\"Invalid node type '{node_type}'. Available types: {data.node_types}\")\n",
    "                continue\n",
    "\n",
    "            # Check if the node ID is valid for the given type\n",
    "            num_nodes = data[node_type].num_nodes\n",
    "            if node_id >= num_nodes:\n",
    "                print(f\"No node with ID: {node_id} in type '{node_type}'.\")\n",
    "                continue\n",
    "\n",
    "            # Get the node value\n",
    "            value = None\n",
    "            for prop in priority_properties.get(node_type, []):\n",
    "                if prop in data[node_type] and len(data[node_type][prop]) > node_id:\n",
    "                    value = data[node_type][prop][node_id]\n",
    "                    break\n",
    "            if value is None:\n",
    "                value = 'No value'\n",
    "\n",
    "            # Get subgraph nodes\n",
    "            subgraph_nodes = explore_subgraph_nodes(data, node_type, node_id)\n",
    "\n",
    "            # Get scholarly query based on node value\n",
    "            scholarly_query = str(value)\n",
    "\n",
    "            # Generate combined RAG summary\n",
    "            print(\"\\nGenerating detailed LLM summary using Wikipedia, subgraph nodes, and scholarly articles...\")\n",
    "            summary = generate_combined_rag_summary(node_type, value, subgraph_nodes, scholarly_query)\n",
    "            print(\"\\n=== Combined LLM Summary ===\")\n",
    "            print(summary)\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Invalid Node ID. Please enter a valid number.\")\n",
    "\n",
    "# Run the interactive RAG exploration\n",
    "interactive_rag_exploration(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "#### Known Issues:\n",
    "- Duplicates in the dataset are not manually removed\n",
    "- Using WIKIPEDIA as external resource. NASA APIs are very specific and not generalizeable to the specifc user queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
