{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project: Query-Driven Retrieval-Augmented Graph Exploration Tool\n",
    "By Karl Simon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the dataset into PyG (PyTorch Geometric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes and Properties:\n",
      "\n",
      "Node Type: Dataset\n",
      "Number of Nodes: 6390\n",
      "  - temporalExtentStart: 6375 items (non-numeric)\n",
      "  - seCorner: 5330 items (non-numeric)\n",
      "  - cmrId: 6390 items (non-numeric)\n",
      "  - globalId: 6390 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([6390, 512])\n",
      "  - abstract: 6390 items (non-numeric)\n",
      "  - daac: 6131 items (non-numeric)\n",
      "  - nwCorner: 5330 items (non-numeric)\n",
      "  - temporalFrequency: 6390 items (non-numeric)\n",
      "  - pagerank_global: torch.Size([6390, 1])\n",
      "  - temporalExtentEnd: 3765 items (non-numeric)\n",
      "  - shortName: 6390 items (non-numeric)\n",
      "  - landingPageUrl: 3037 items (non-numeric)\n",
      "  - doi: 6390 items (non-numeric)\n",
      "  - longName: 6390 items (non-numeric)\n",
      "\n",
      "Node Type: DataCenter\n",
      "Number of Nodes: 184\n",
      "  - pagerank_global: torch.Size([184, 1])\n",
      "  - globalId: 184 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([184, 512])\n",
      "  - shortName: 184 items (non-numeric)\n",
      "  - url: 184 items (non-numeric)\n",
      "  - longName: 184 items (non-numeric)\n",
      "\n",
      "Node Type: Project\n",
      "Number of Nodes: 333\n",
      "  - pagerank_global: torch.Size([333, 1])\n",
      "  - globalId: 333 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([333, 512])\n",
      "  - shortName: 333 items (non-numeric)\n",
      "  - longName: 333 items (non-numeric)\n",
      "\n",
      "Node Type: Platform\n",
      "Number of Nodes: 442\n",
      "  - Type: 442 items (non-numeric)\n",
      "  - pagerank_global: torch.Size([442, 1])\n",
      "  - globalId: 442 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([442, 512])\n",
      "  - shortName: 442 items (non-numeric)\n",
      "  - longName: 442 items (non-numeric)\n",
      "\n",
      "Node Type: Instrument\n",
      "Number of Nodes: 867\n",
      "  - pagerank_global: torch.Size([867, 1])\n",
      "  - globalId: 867 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([867, 512])\n",
      "  - shortName: 867 items (non-numeric)\n",
      "  - longName: 867 items (non-numeric)\n",
      "\n",
      "Node Type: ScienceKeyword\n",
      "Number of Nodes: 1609\n",
      "  - pagerank_global: torch.Size([1609, 1])\n",
      "  - globalId: 1609 items (non-numeric)\n",
      "  - name: 1609 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([1609, 512])\n",
      "  - pagerank_publication_dataset: torch.Size([1609, 1])\n",
      "\n",
      "Node Type: Publication\n",
      "Number of Nodes: 125939\n",
      "  - year: 125939 items (non-numeric)\n",
      "  - pagerank_global: torch.Size([125939, 1])\n",
      "  - globalId: 125939 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([125939, 512])\n",
      "  - title: 125937 items (non-numeric)\n",
      "  - DOI: 125939 items (non-numeric)\n",
      "  - abstract: 99859 items (non-numeric)\n",
      "  - authors: 109975 items (non-numeric)\n",
      "\n",
      "Edges and Types:\n",
      "Edge Type: ('DataCenter', 'HAS_DATASET', 'Dataset') - Number of Edges: 9017 - Shape: torch.Size([2, 9017])\n",
      "Edge Type: ('Dataset', 'OF_PROJECT', 'Project') - Number of Edges: 6049 - Shape: torch.Size([2, 6049])\n",
      "Edge Type: ('Dataset', 'HAS_PLATFORM', 'Platform') - Number of Edges: 9884 - Shape: torch.Size([2, 9884])\n",
      "Edge Type: ('Platform', 'HAS_INSTRUMENT', 'Instrument') - Number of Edges: 2469 - Shape: torch.Size([2, 2469])\n",
      "Edge Type: ('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword') - Number of Edges: 1823 - Shape: torch.Size([2, 1823])\n",
      "Edge Type: ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword') - Number of Edges: 20436 - Shape: torch.Size([2, 20436])\n",
      "Edge Type: ('Publication', 'CITES', 'Publication') - Number of Edges: 208670 - Shape: torch.Size([2, 208670])\n",
      "Edge Type: ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword') - Number of Edges: 89039 - Shape: torch.Size([2, 89039])\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports for entire notebook\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from torch_geometric.data import HeteroData\n",
    "from collections import defaultdict\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load JSON data from file\n",
    "file_path = \"/home/karlsimon/CSCI6365/final/graph.json\"\n",
    "graph_data = []\n",
    "\n",
    "# Load data line by line to prevent memory overload\n",
    "with open(file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            graph_data.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON line: {e}\")\n",
    "            continue\n",
    "\n",
    "# Initialize HeteroData object\n",
    "data = HeteroData()\n",
    "\n",
    "# Mapping for node indices per node type\n",
    "node_mappings = defaultdict(dict)\n",
    "\n",
    "# Temporary storage for properties\n",
    "node_properties = defaultdict(lambda: defaultdict(list))\n",
    "edge_indices = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# # Define limits for node subsets based on type\n",
    "# node_limits = {\n",
    "#     'Publication': 1000,\n",
    "#     'Dataset': 500,\n",
    "#     'ScienceKeyword': 300,\n",
    "#     'Instrument': 200,\n",
    "#     'Platform': 150,\n",
    "#     'Project': 100,\n",
    "#     'DataCenter': 50\n",
    "# }\n",
    "\n",
    "# Track the number of nodes added per type\n",
    "node_counts = defaultdict(int)\n",
    "\n",
    "# Process nodes with limits\n",
    "for item in graph_data:\n",
    "    if item['type'] == 'node':\n",
    "        node_type = item['labels'][0]\n",
    "        # if node_counts[node_type] >= node_limits.get(node_type, 50):\n",
    "        #     continue  # Skip nodes once the limit is reached\n",
    "\n",
    "        node_id = item['id']\n",
    "        properties = item['properties']\n",
    "\n",
    "        # Store the node index mapping\n",
    "        node_index = len(node_mappings[node_type])\n",
    "        node_mappings[node_type][node_id] = node_index\n",
    "        node_counts[node_type] += 1\n",
    "\n",
    "        # Store properties temporarily by type\n",
    "        for key, value in properties.items():\n",
    "            if isinstance(value, list) and all(isinstance(v, (int, float)) for v in value):\n",
    "                node_properties[node_type][key].append(torch.tensor(value, dtype=torch.float))\n",
    "            elif isinstance(value, (int, float)):\n",
    "                node_properties[node_type][key].append(torch.tensor([value], dtype=torch.float))\n",
    "            else:\n",
    "                node_properties[node_type][key].append(value)  # non-numeric properties as lists\n",
    "\n",
    "# # Define limits for relationships based on type\n",
    "# relationship_limits = {\n",
    "#     'CITES': 2000,\n",
    "#     'HAS_APPLIED_RESEARCH_AREA': 1000,\n",
    "#     'HAS_SCIENCEKEYWORD': 500,\n",
    "#     'HAS_PLATFORM': 500,\n",
    "#     'HAS_DATASET': 500,\n",
    "#     'OF_PROJECT': 300,\n",
    "#     'HAS_INSTRUMENT': 200\n",
    "# }\n",
    "\n",
    "# Track the number of relationships added per type\n",
    "relationship_counts = defaultdict(int)\n",
    "\n",
    "# Filter relationships to only include sampled nodes\n",
    "for item in graph_data:\n",
    "    if item['type'] == 'relationship':\n",
    "        start_type = item['start']['labels'][0]\n",
    "        end_type = item['end']['labels'][0]\n",
    "        start_id = item['start']['id']\n",
    "        end_id = item['end']['id']\n",
    "        edge_type = item['label']\n",
    "\n",
    "        # # Skip if relationship limit reached\n",
    "        # if relationship_counts[edge_type] >= relationship_limits.get(edge_type, 100):\n",
    "        #     continue\n",
    "\n",
    "        # Check if start and end nodes exist in the sampled nodes\n",
    "        if start_id in node_mappings[start_type] and end_id in node_mappings[end_type]:\n",
    "            start_idx = node_mappings[start_type][start_id]\n",
    "            end_idx = node_mappings[end_type][end_id]\n",
    "\n",
    "            # Append to edge list\n",
    "            edge_indices[(start_type, edge_type, end_type)]['start'].append(start_idx)\n",
    "            edge_indices[(start_type, edge_type, end_type)]['end'].append(end_idx)\n",
    "            relationship_counts[edge_type] += 1\n",
    "\n",
    "# Finalize node properties by batch processing\n",
    "for node_type, properties in node_properties.items():\n",
    "    data[node_type].num_nodes = len(node_mappings[node_type])\n",
    "    for key, values in properties.items():\n",
    "        if isinstance(values[0], torch.Tensor):\n",
    "            data[node_type][key] = torch.stack(values)\n",
    "        else:\n",
    "            data[node_type][key] = values  # Keep non-tensor properties as lists\n",
    "\n",
    "# Finalize edge indices in bulk\n",
    "for (start_type, edge_type, end_type), indices in edge_indices.items():\n",
    "    edge_index = torch.tensor([indices['start'], indices['end']], dtype=torch.long)\n",
    "    data[start_type, edge_type, end_type].edge_index = edge_index\n",
    "\n",
    "# Display statistics for verification\n",
    "print(\"Nodes and Properties:\")\n",
    "for node_type in data.node_types:\n",
    "    print(f\"\\nNode Type: {node_type}\")\n",
    "    print(f\"Number of Nodes: {data[node_type].num_nodes}\")\n",
    "    for key, value in data[node_type].items():\n",
    "        if key != 'num_nodes':\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"  - {key}: {value.shape}\")\n",
    "            else:\n",
    "                print(f\"  - {key}: {len(value)} items (non-numeric)\")\n",
    "\n",
    "print(\"\\nEdges and Types:\")\n",
    "for edge_type in data.edge_types:\n",
    "    edge_index = data[edge_type].edge_index\n",
    "    print(f\"Edge Type: {edge_type} - Number of Edges: {edge_index.size(1)} - Shape: {edge_index.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions definitions for keywords, search and display used in next cell\n",
    "def extract_keywords(query):\n",
    "    keywords = re.findall(r'\\b\\w+\\b', query)\n",
    "    return [kw.lower() for kw in keywords]\n",
    "\n",
    "# TODO: improve search metrics (i.e. amond preliminary results, find the most relevant ones)\n",
    "def search_graph(data, keywords, node_types=['Dataset', 'Project', 'ScienceKeyword', 'Instrument' ,'Platform', 'Publication']):\n",
    "    results = []\n",
    "    \n",
    "    for node_type in node_types:\n",
    "        for key in data[node_type]:\n",
    "            if key == 'num_nodes':\n",
    "                continue\n",
    "            \n",
    "            values = data[node_type][key]\n",
    "            if isinstance(values, list):\n",
    "                for idx, value in enumerate(values):\n",
    "                    if any(kw in str(value).lower() for kw in keywords):\n",
    "                        results.append((node_type, idx, key, value))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def display_results(results):\n",
    "    if not results:\n",
    "        print(\"No relevant nodes found.\")\n",
    "        return\n",
    "\n",
    "    with open(\"query_results.txt\", \"w\") as file:\n",
    "        print(f\"\\nFound {len(results)} relevant nodes:\\n\")\n",
    "        for node_type, idx, key, value in results:\n",
    "            file.write(f\"Node Type: {node_type} | Index: {idx} | Property: {key} | Value: {value}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Keywords: ['climate', 'change']\n",
      "\n",
      "Found 72539 relevant nodes:\n",
      "\n",
      "\n",
      "Exploring subgraph for node type: Dataset\n",
      "Exploring subgraph for node indices: tensor([ 0,  1,  2, 11, 12, 13, 14, 15, 16, 17])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Extracted subgraph with 6130 nodes and 44118 edges.\n",
      "Edge Type: [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Project\n",
      "Exploring subgraph for node indices: tensor([137,   0,  26,  59,  89, 105, 108, 109, 117, 121])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Extracted subgraph with 197 nodes and 206 edges.\n",
      "Edge Type: [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "\n",
      "Exploring subgraph for node type: ScienceKeyword\n",
      "Exploring subgraph for node indices: tensor([ 70,  72,  94,  96, 101, 139, 229, 729, 802, 888])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 372 nodes and 733 edges.\n",
      "Edge Type: [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Instrument\n",
      "Exploring subgraph for node indices: tensor([97, 97])\n",
      "relevant_edges =  [('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Extracted subgraph with 26 nodes and 71 edges.\n",
      "Edge Type: [('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "\n",
      "Exploring subgraph for node type: Platform\n",
      "Exploring subgraph for node indices: tensor([344,  68, 165, 166, 173, 178, 248, 307, 344, 409])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Extracted subgraph with 1953 nodes and 4553 edges.\n",
      "Edge Type: [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "\n",
      "Exploring subgraph for node type: Publication\n",
      "Exploring subgraph for node indices: tensor([ 2,  4, 10, 35, 43, 46, 47, 53, 63, 77])\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 6806 nodes and 8419 edges.\n",
      "Edge Type: [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n"
     ]
    }
   ],
   "source": [
    "# Given query, extract keywords, search the graph for relevant nodes, and display the results\n",
    "# NOTE: currently only searches for exact keyword matches in node properties\n",
    "\n",
    "def get_subgraph(data, node_type, node_indices, num_hops=2):\n",
    "    # Find all edge types where the node_type is either the source or target\n",
    "    relevant_edges = [\n",
    "        (src, rel, dst) for (src, rel, dst) in data.edge_types if src == node_type or dst == node_type\n",
    "    ]\n",
    "    \n",
    "    print(\"relevant_edges = \", relevant_edges)\n",
    "\n",
    "    if not relevant_edges:\n",
    "        print(f\"No edges found for node type '{node_type}'\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Combine edge indices from all relevant edge types\n",
    "    combined_edge_index = []\n",
    "    combined_edge_types = []\n",
    "\n",
    "    for edge_type in relevant_edges:\n",
    "        edge_index = data[edge_type].edge_index\n",
    "        combined_edge_index.append(edge_index)\n",
    "        combined_edge_types.append(edge_type)\n",
    "\n",
    "    # Stack all edge indices into a single tensor\n",
    "    combined_edge_index = torch.cat(combined_edge_index, dim=1)\n",
    "\n",
    "    # Extract the subgraph using the combined edge index\n",
    "    subset, edge_index, _, _ = k_hop_subgraph(node_idx=node_indices, num_hops=num_hops, edge_index=combined_edge_index)\n",
    "    return subset, edge_index, combined_edge_types\n",
    "\n",
    "\n",
    "# Explore subgraphs based on the search results.\n",
    "def explore_subgraphs(data, results, num_hops=2):\n",
    "    if not results:\n",
    "        print(\"No nodes to explore for subgraphs.\")\n",
    "        return\n",
    "\n",
    "    # Group the results by node type\n",
    "    nodes_by_type = defaultdict(list)\n",
    "    for node_type, idx, _, _ in results:\n",
    "        nodes_by_type[node_type].append(idx)\n",
    "\n",
    "    # Extract and display subgraphs for each node type\n",
    "    for node_type, indices in nodes_by_type.items():\n",
    "        print(f\"\\nExploring subgraph for node type: {node_type}\")\n",
    "        \n",
    "        # Get the valid range for node indices\n",
    "        num_nodes = data[node_type].num_nodes\n",
    "        valid_indices = [idx for idx in indices if idx < num_nodes]\n",
    "\n",
    "        if not valid_indices:\n",
    "            print(f\"No valid indices for node type '{node_type}'.\")\n",
    "            continue\n",
    "\n",
    "        node_indices = torch.tensor(valid_indices[:10])  # Limit to 10 nodes to keep it manageable\n",
    "        print(f\"Exploring subgraph for node indices: {node_indices}\") # may not be sequential due to search results ordering \n",
    "        subset, edge_index, edge_type = get_subgraph(data, node_type, node_indices, num_hops=num_hops)\n",
    "\n",
    "        if subset is not None and edge_index is not None:\n",
    "            print(f\"Extracted subgraph with {len(subset)} nodes and {edge_index.size(1)} edges.\")\n",
    "            print(f\"Edge Type: {edge_type}\")\n",
    "        else:\n",
    "            print(f\"Could not extract subgraph for node type: {node_type}\")\n",
    "\n",
    "# Example: Run the combined query and subgraph exploration module\n",
    "# query = input(\"Enter your query (e.g., 'Find datasets related to climate change projects'): \")\n",
    "query = \"climate change\" #TODO: remove hardcoded query\n",
    "keywords = extract_keywords(query)\n",
    "print(f\"\\nExtracted Keywords: {keywords}\")\n",
    "\n",
    "results = search_graph(data, keywords)\n",
    "display_results(results)\n",
    "\n",
    "# Explore subgraphs based on the results\n",
    "explore_subgraphs(data, results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USING MEDIAWIKI API FOR SNIPPIT\n",
    "\n",
    "# def fetch_wikipedia_context(keywords):\n",
    "#     search_term = \" \".join(keywords)\n",
    "#     url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    \n",
    "#     # Parameters for the MediaWiki API search request\n",
    "#     params = {\n",
    "#         \"action\": \"query\",\n",
    "#         \"list\": \"search\",\n",
    "#         \"srsearch\": search_term,\n",
    "#         \"srlimit\": 5,  # Limit to top 5 search results\n",
    "#         \"srprop\": \"snippet|timestamp\",\n",
    "#         \"format\": \"json\"\n",
    "#     }\n",
    "    \n",
    "#     headers = {\n",
    "#         \"User-Agent\": \"GraphExplorationTool/1.0 (karlsimon@example.com)\"\n",
    "#     }\n",
    "    \n",
    "#     try:\n",
    "#         # Make the request to the MediaWiki API\n",
    "#         response = requests.get(url, params=params, headers=headers)\n",
    "#         response.raise_for_status()\n",
    "#         data = response.json()\n",
    "        \n",
    "#         search_results = data.get(\"query\", {}).get(\"search\", [])\n",
    "        \n",
    "#         if not search_results:\n",
    "#             return None\n",
    "        \n",
    "#         # Collect the top search results\n",
    "#         context_list = []\n",
    "#         for result in search_results:\n",
    "#             title = result.get(\"title\", \"No Title\")\n",
    "#             snippet = result.get(\"snippet\", \"No Snippet Available.\")\n",
    "#             timestamp = result.get(\"timestamp\", \"No Timestamp Available.\")\n",
    "#             page_url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "            \n",
    "#             context_entry = {\n",
    "#                 \"title\": title,\n",
    "#                 \"snippet\": snippet,\n",
    "#                 \"timestamp\": timestamp,\n",
    "#                 \"link\": page_url\n",
    "#             }\n",
    "#             context_list.append(context_entry)\n",
    "        \n",
    "#         return context_list\n",
    "\n",
    "#     except requests.RequestException as e:\n",
    "#         print(f\"Error fetching Wikipedia context: {e}\")\n",
    "#         return None\n",
    "\n",
    "\n",
    "# # Display the Wikipedia context for the extracted keywords\n",
    "# def display_wikipedia_context(context_list):\n",
    "#     if not context_list:\n",
    "#         print(\"\\nNo external context available from Wikipedia.\")\n",
    "#         return\n",
    "\n",
    "#     print(\"\\nWikipedia Context using MEDIAWIKI API:\")\n",
    "#     for i, context in enumerate(context_list, start=1):\n",
    "#         print(f\"\\nResult {i}:\")\n",
    "#         print(f\"Title: {context['title']}\")\n",
    "#         print(f\"Snippet: {context['snippet']}\")\n",
    "#         print(f\"Timestamp: {context['timestamp']}\")\n",
    "#         print(f\"Link: {context['link']}\")\n",
    "\n",
    "# wikipedia_context = fetch_wikipedia_context(keywords)\n",
    "# display_wikipedia_context(wikipedia_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wikipedia Context:\n",
      "\n",
      "Result 1:\n",
      "Title: Climate change\n",
      "Description: Human-caused changes to climate on Earth\n",
      "Summary: Present-day climate change includes both global warming—the ongoing increase in global average temperature—and its wider effects on Earth's climate. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The current rise in global temperatures is driven by human activities, especially fossil fuel burning since the Industrial Revolution. Fossil fuel use, deforestation, and some agricultural and industrial practices release greenhouse gases. These gases absorb some of the heat that the Earth radiates after it warms from sunlight, warming the lower atmosphere. Carbon dioxide, the primary greenhouse gas driving global warming, has grown by about 50% and is at levels not seen for millions of years.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Change_in_Average_Temperature_With_Fahrenheit.svg/320px-Change_in_Average_Temperature_With_Fahrenheit.svg.png\n",
      "\n",
      "Result 2:\n",
      "Title: Climate change denial\n",
      "Description: Denial of the scientific consensus on climate change\n",
      "Summary: Climate change denial is a form of science denial characterized by rejecting, refusing to acknowledge, disputing, or fighting the scientific consensus on climate change. Those promoting denial commonly use rhetorical tactics to give the appearance of a scientific controversy where there is none. Climate change denial includes unreasonable doubts about the extent to which climate change is caused by humans, its effects on nature and human society, and the potential of adaptation to global warming by human actions. To a lesser extent, climate change denial can also be implicit when people accept the science but fail to reconcile it with their belief or action. Several studies have analyzed these positions as forms of denialism, pseudoscience, or propaganda.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change_denial\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Inhofe_holding_snowball.jpg/320px-Inhofe_holding_snowball.jpg\n",
      "\n",
      "Result 3:\n",
      "Title: Climate change mitigation\n",
      "Description: Actions to reduce net greenhouse gas emissions to limit climate change\n",
      "Summary: Climate change mitigation (or decarbonisation) is action to limit the greenhouse gases in the atmosphere that cause climate change. Climate change mitigation actions include conserving energy and replacing fossil fuels with clean energy sources. Secondary mitigation strategies include changes to land use and removing carbon dioxide (CO2) from the atmosphere. Current climate change mitigation policies are insufficient as they would still result in global warming of about 2.7 °C by 2100, significantly above the 2015 Paris Agreement's goal of limiting global warming to below 2 °C.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change_mitigation\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Westmill_Solar_2.jpg/320px-Westmill_Solar_2.jpg\n",
      "\n",
      "Result 4:\n",
      "Title: Effects of climate change\n",
      "Description: No Description Available.\n",
      "Summary: Effects of climate change are well documented and growing for Earth's natural environment and human societies. Changes to the climate system include an overall warming trend, changes to precipitation patterns, and more extreme weather. As the climate changes it impacts the natural environment with effects such as more intense forest fires, thawing permafrost, and desertification. These changes impact ecosystems and societies, and can become irreversible once tipping points are crossed. Climate activists are engaged in a range of activities around the world that seek to ameliorate these issues or prevent them from happening.\n",
      "Link: https://en.wikipedia.org/wiki/Effects_of_climate_change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/062821Yreka_Fire_CalFire_-2wiki.jpg/320px-062821Yreka_Fire_CalFire_-2wiki.jpg\n",
      "\n",
      "Result 5:\n",
      "Title: United Nations Framework Convention on Climate Change\n",
      "Description: International environmental treaty\n",
      "Summary: The United Nations Framework Convention on Climate Change (UNFCCC) is the UN process for negotiating an agreement to limit dangerous climate change. It is an international treaty among countries to combat \"dangerous human interference with the climate system\". The main way to do this is limiting the increase in greenhouse gases in the atmosphere. It was signed in 1992 by 154 states at the United Nations Conference on Environment and Development (UNCED), informally known as the Earth Summit, held in Rio de Janeiro. The treaty entered into force on 21 March 1994. \"UNFCCC\" is also the name of the Secretariat charged with supporting the operation of the convention, with offices on the UN Campus in Bonn, Germany.\n",
      "Link: https://en.wikipedia.org/wiki/United_Nations_Framework_Convention_on_Climate_Change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/UNFCCC_Annex_I_Parties%2C_OECD%2C_EU.svg/320px-UNFCCC_Annex_I_Parties%2C_OECD%2C_EU.svg.png\n"
     ]
    }
   ],
   "source": [
    "# Get external context from Wikipedia using the REST API\n",
    "def fetch_wikipedia_context(keywords):\n",
    "    search_term = \" \".join(keywords)\n",
    "    \n",
    "    # Step 1: Use the Action API to get the top 5 search results\n",
    "    search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    search_params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": search_term,\n",
    "        \"srlimit\": 5,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"GraphExplorationTool/1.0 (ksimon24@gwu.edu)\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        search_response = requests.get(search_url, params=search_params, headers=headers)\n",
    "        search_response.raise_for_status()\n",
    "        search_data = search_response.json()\n",
    "        \n",
    "        search_results = search_data.get(\"query\", {}).get(\"search\", [])\n",
    "        \n",
    "        if not search_results:\n",
    "            return None\n",
    "\n",
    "        # Step 2: Fetch summaries using the REST API for each search result\n",
    "        context_list = []\n",
    "        for result in search_results:\n",
    "            page_title = result.get(\"title\")\n",
    "            rest_url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{page_title.replace(' ', '_')}\"\n",
    "            \n",
    "            rest_response = requests.get(rest_url, headers=headers)\n",
    "            rest_response.raise_for_status()\n",
    "            rest_data = rest_response.json()\n",
    "            \n",
    "            # Extract relevant information\n",
    "            title = rest_data.get(\"title\", \"No Title\")\n",
    "            description = rest_data.get(\"description\", \"No Description Available.\")\n",
    "            summary = rest_data.get(\"extract\", \"No Summary Available.\")\n",
    "            link = rest_data.get(\"content_urls\", {}).get(\"desktop\", {}).get(\"page\", \"No Link Available.\")\n",
    "            thumbnail = rest_data.get(\"thumbnail\", {}).get(\"source\", None)\n",
    "            \n",
    "            context_entry = {\n",
    "                \"title\": title,\n",
    "                \"description\": description,\n",
    "                \"summary\": summary,\n",
    "                \"link\": link,\n",
    "                \"thumbnail\": thumbnail\n",
    "            }\n",
    "            context_list.append(context_entry)\n",
    "        \n",
    "        return context_list\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching Wikipedia context: {e}\")\n",
    "        return None\n",
    "    \n",
    "def display_wikipedia_context(context_list):\n",
    "    if not context_list:\n",
    "        print(\"\\nNo external context available from Wikipedia.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nWikipedia Context:\")\n",
    "    for i, context in enumerate(context_list, start=1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"Title: {context['title']}\")\n",
    "        print(f\"Description: {context['description']}\")\n",
    "        print(f\"Summary: {context['summary']}\")\n",
    "        print(f\"Link: {context['link']}\")\n",
    "        if context['thumbnail']:\n",
    "            print(f\"Thumbnail: {context['thumbnail']}\")\n",
    "\n",
    "wikipedia_context = fetch_wikipedia_context(keywords)\n",
    "display_wikipedia_context(wikipedia_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Keywords: ['climate', 'change']\n",
      "\n",
      "Found 50 relevant nodes:\n",
      "\n",
      "\n",
      "Wikipedia Context:\n",
      "\n",
      "Result 1:\n",
      "Title: Climate change\n",
      "Description: Human-caused changes to climate on Earth\n",
      "Summary: Present-day climate change includes both global warming—the ongoing increase in global average temperature—and its wider effects on Earth's climate. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The current rise in global temperatures is driven by human activities, especially fossil fuel burning since the Industrial Revolution. Fossil fuel use, deforestation, and some agricultural and industrial practices release greenhouse gases. These gases absorb some of the heat that the Earth radiates after it warms from sunlight, warming the lower atmosphere. Carbon dioxide, the primary greenhouse gas driving global warming, has grown by about 50% and is at levels not seen for millions of years.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Change_in_Average_Temperature_With_Fahrenheit.svg/320px-Change_in_Average_Temperature_With_Fahrenheit.svg.png\n",
      "\n",
      "Result 2:\n",
      "Title: Climate change denial\n",
      "Description: Denial of the scientific consensus on climate change\n",
      "Summary: Climate change denial is a form of science denial characterized by rejecting, refusing to acknowledge, disputing, or fighting the scientific consensus on climate change. Those promoting denial commonly use rhetorical tactics to give the appearance of a scientific controversy where there is none. Climate change denial includes unreasonable doubts about the extent to which climate change is caused by humans, its effects on nature and human society, and the potential of adaptation to global warming by human actions. To a lesser extent, climate change denial can also be implicit when people accept the science but fail to reconcile it with their belief or action. Several studies have analyzed these positions as forms of denialism, pseudoscience, or propaganda.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change_denial\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Inhofe_holding_snowball.jpg/320px-Inhofe_holding_snowball.jpg\n",
      "\n",
      "Result 3:\n",
      "Title: Climate change mitigation\n",
      "Description: Actions to reduce net greenhouse gas emissions to limit climate change\n",
      "Summary: Climate change mitigation (or decarbonisation) is action to limit the greenhouse gases in the atmosphere that cause climate change. Climate change mitigation actions include conserving energy and replacing fossil fuels with clean energy sources. Secondary mitigation strategies include changes to land use and removing carbon dioxide (CO2) from the atmosphere. Current climate change mitigation policies are insufficient as they would still result in global warming of about 2.7 °C by 2100, significantly above the 2015 Paris Agreement's goal of limiting global warming to below 2 °C.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change_mitigation\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Westmill_Solar_2.jpg/320px-Westmill_Solar_2.jpg\n",
      "\n",
      "Result 4:\n",
      "Title: Effects of climate change\n",
      "Description: No Description Available.\n",
      "Summary: Effects of climate change are well documented and growing for Earth's natural environment and human societies. Changes to the climate system include an overall warming trend, changes to precipitation patterns, and more extreme weather. As the climate changes it impacts the natural environment with effects such as more intense forest fires, thawing permafrost, and desertification. These changes impact ecosystems and societies, and can become irreversible once tipping points are crossed. Climate activists are engaged in a range of activities around the world that seek to ameliorate these issues or prevent them from happening.\n",
      "Link: https://en.wikipedia.org/wiki/Effects_of_climate_change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/062821Yreka_Fire_CalFire_-2wiki.jpg/320px-062821Yreka_Fire_CalFire_-2wiki.jpg\n",
      "\n",
      "Result 5:\n",
      "Title: United Nations Framework Convention on Climate Change\n",
      "Description: International environmental treaty\n",
      "Summary: The United Nations Framework Convention on Climate Change (UNFCCC) is the UN process for negotiating an agreement to limit dangerous climate change. It is an international treaty among countries to combat \"dangerous human interference with the climate system\". The main way to do this is limiting the increase in greenhouse gases in the atmosphere. It was signed in 1992 by 154 states at the United Nations Conference on Environment and Development (UNCED), informally known as the Earth Summit, held in Rio de Janeiro. The treaty entered into force on 21 March 1994. \"UNFCCC\" is also the name of the Secretariat charged with supporting the operation of the convention, with offices on the UN Campus in Bonn, Germany.\n",
      "Link: https://en.wikipedia.org/wiki/United_Nations_Framework_Convention_on_Climate_Change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/UNFCCC_Annex_I_Parties%2C_OECD%2C_EU.svg/320px-UNFCCC_Annex_I_Parties%2C_OECD%2C_EU.svg.png\n",
      "\n",
      "Exploring subgraph for node type: Publication\n",
      "Exploring subgraph for node indices: tensor([76606, 94077, 50763, 55702, 88548, 42689, 82757, 12919, 60848, 35890])\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 18 nodes and 8 edges.\n",
      "Edge Type: [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Dataset\n",
      "Exploring subgraph for node indices: tensor([4329, 2323,   97,   61,   65, 2131, 4180, 3752,  293, 4253])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Extracted subgraph with 2263 nodes and 16366 edges.\n",
      "Edge Type: [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: ScienceKeyword\n",
      "Exploring subgraph for node indices: tensor([ 94, 802, 888, 989, 139,  70,  72,  96, 729, 229])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 374 nodes and 738 edges.\n",
      "Edge Type: [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Project\n",
      "Exploring subgraph for node indices: tensor([109, 117,  26,  59,  89, 105, 108, 121,   0, 137])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Extracted subgraph with 197 nodes and 206 edges.\n",
      "Edge Type: [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "\n",
      "Exploring subgraph for node type: Platform\n",
      "Exploring subgraph for node indices: tensor([165, 166, 173, 178, 344,  68, 248, 307, 344, 409])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Extracted subgraph with 1953 nodes and 4553 edges.\n",
      "Edge Type: [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n"
     ]
    }
   ],
   "source": [
    "# Next Steps:\n",
    "# 1. improve graph search and rank results.\n",
    "# 2. improve subgraph exploration.\n",
    "# 3. improve external context retrieval (NASA API).\n",
    "\n",
    "# Updated search_graph function with TF-IDF scoring\n",
    "# TODO: make max_per_type specific to each node type\n",
    "def search_graph(data, keywords, node_types=['Dataset', 'Project', 'ScienceKeyword', 'Instrument', 'Platform', 'Publication'], max_results=50, max_per_type=10):\n",
    "    results = []\n",
    "    texts = []  # Collect text data for TF-IDF processing\n",
    "    metadata = []  # To store corresponding metadata (node type, index, key, value)\n",
    "\n",
    "    # Step 1: Collect all matching nodes and their text data\n",
    "    for node_type in node_types:\n",
    "        for key in data[node_type]:\n",
    "            if key == 'num_nodes':\n",
    "                continue\n",
    "            \n",
    "            values = data[node_type][key]\n",
    "            if isinstance(values, list):\n",
    "                for idx, value in enumerate(values):\n",
    "                    value_str = str(value).lower()\n",
    "                    if any(kw in value_str for kw in keywords):\n",
    "                        texts.append(value_str)\n",
    "                        metadata.append((node_type, idx, key, value))\n",
    "\n",
    "    if not texts:\n",
    "        return []\n",
    "\n",
    "    ############ Visualization of TF-IDF scoring:############\n",
    "    # Example scores output. Shape = (n_samples, n_keywords), and we sum along rows (i.e. sum across the columns)\n",
    "    # [[0.5 0.8]  # TF-IDF scores for \"climate change impacts ecosystems...\"\n",
    "    # [0.8 0.7]  # TF-IDF scores for \"mitigation strategies for climate change...\"\n",
    "    # [0.9 0.2]] # TF-IDF scores for \"climate change denial rejects...\"\n",
    "    # \n",
    "    # After summation:\n",
    "    # [1.3, 1.5, 1.1]  # Sum of TF-IDF scores for each text in `texts`\n",
    "    # and indices: [0, 1, 2]  # Indices of the summed scores\n",
    "    # \n",
    "    # After sorting in descending order:\n",
    "    # [1.5, 1.3, 1.1]  # Sorted TF-IDF scores\n",
    "    # and indices of the sorted scores in descending order: [2, 0, 1]\n",
    "    ########################################################\n",
    "\n",
    "    # Step 2: Compute TF-IDF scores for the collected texts\n",
    "    # NOTE: texts stores the properties of the nodes which contain the keywords\n",
    "    vectorizer = TfidfVectorizer(vocabulary=keywords)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    scores = tfidf_matrix.sum(axis=1).A1  # Sum the TF-IDF scores for each text\n",
    "\n",
    "    # Step 3: Sort the results by TF-IDF score in descending order\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    sorted_results = [metadata[i] for i in sorted_indices]\n",
    "    with open(\"sorted_results.txt\", \"w\") as file:\n",
    "        for result in sorted_results:\n",
    "            file.write(f\"{result}\\n\")\n",
    "\n",
    "    # Step 4: Limit the number of results overall and per node type\n",
    "    final_results = []\n",
    "    counts_per_type = {node_type: 0 for node_type in node_types}\n",
    "\n",
    "    for result in sorted_results:\n",
    "        node_type = result[0]\n",
    "        if len(final_results) >= max_results:\n",
    "            break\n",
    "        if counts_per_type[node_type] < max_per_type:\n",
    "            final_results.append(result)\n",
    "            counts_per_type[node_type] += 1\n",
    "\n",
    "    return final_results\n",
    "\n",
    "# Updated display_results function to trim long values\n",
    "def display_results(results, max_value_length=200):\n",
    "    if not results:\n",
    "        print(\"No relevant nodes found.\")\n",
    "        return\n",
    "\n",
    "    with open(\"query_results.txt\", \"w\") as file:\n",
    "        print(f\"\\nFound {len(results)} relevant nodes:\\n\")\n",
    "        for node_type, idx, key, value in results:\n",
    "            value_str = str(value)\n",
    "            if len(value_str) > max_value_length:\n",
    "                value_str = value_str[:max_value_length] + \"...\"\n",
    "            output_line = f\"Node Type: {node_type} | Index: {idx} | Property: {key} | Value: {value_str}\\n\"\n",
    "            file.write(output_line)\n",
    "\n",
    "# Get user query\n",
    "# query = input(\"Enter your query (e.g., 'Find datasets related to climate change projects'): \")\n",
    "query = \"climate change\" #TODO: remove hardcoded query\n",
    "keywords = extract_keywords(query)\n",
    "print(f\"\\nExtracted Keywords: {keywords}\")\n",
    "\n",
    "# Search the graph with TF-IDF ranking\n",
    "graph_results = search_graph(data, keywords)\n",
    "display_results(graph_results)\n",
    "\n",
    "# Fetch Wikipedia context\n",
    "wikipedia_context = fetch_wikipedia_context(keywords)\n",
    "display_wikipedia_context(wikipedia_context)\n",
    "\n",
    "# Explore subgraphs based on the results\n",
    "explore_subgraphs(data, graph_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ OpenAI API ############\n",
    "# import os\n",
    "# from openai import OpenAI\n",
    "# # Define the path to the text file containing the API key\n",
    "# file_path = \"/home/karlsimon/CSCI6365/final/api_key.txt\"\n",
    "# with open(file_path, \"r\") as file:\n",
    "#     api_key = file.read().strip()\n",
    "# print(api_key)\n",
    "\n",
    "# client = OpenAI(\n",
    "#     api_key=api_key,\n",
    "# )\n",
    "\n",
    "# chat_completion = client.chat.completions.create(\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Say this is a test\",\n",
    "#         }\n",
    "#     ],\n",
    "#     model=\"gpt-4o-mini\",  # Use 'gpt-4-turbo' if 'gpt-4o' isn't available\n",
    "# )\n",
    "\n",
    "# # Output 12.09: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Gemini API ############\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Define the path to the text file containing the API key\n",
    "file_path = \"/home/karlsimon/CSCI6365/final/gemini_api_key.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    api_key = file.read().strip()\n",
    "# print(api_key)\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Create a model instance (using Gemini 1.5 Flash in this case)\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM-Generated Summary:\n",
      "The provided text focuses on various research studies investigating the effects of climate change and its interaction with other factors.  Several studies analyze the impact of climate change on evapotranspiration (ET), finding a decrease in ET in some regions due to deforestation, despite increased temperatures potentially raising ET.  Other studies examine the link between climate change and forest fires (increased fire frequency in Yunnan, China, correlated with decreased precipitation and water storage), and the impact of changing prey quality on Chinook salmon due to climate change-induced shifts in fatty acid composition.  Additional research explores the interplay of climate change with stratospheric ozone, the equilibrium climate sensitivity of climate models, and the drivers of human migration in climate-vulnerable regions (economic factors outweighing environmental concerns).  Finally, studies address the impact of climate change on groundwater levels in Hungary, the effects on vegetation cycles and soil moisture in France using a land surface model, and the bivariate analysis of flood frequency and volume in the Ganjiang River basin.  A systematic review concludes that while many agricultural practices claim synergies between climate change mitigation and adaptation, empirical evidence is often lacking and the reported synergies may be overstated.  In summary, the research highlights the multifaceted and regionally varied impacts of climate change across various ecosystems and human activities, emphasizing the need for further research and robust data to support effective mitigation and adaptation strategies.\n",
      "\n",
      "\n",
      "Exploring subgraph for node type: Publication\n",
      "Exploring subgraph for node indices: tensor([76606, 94077, 50763, 55702, 88548, 42689, 82757, 12919, 60848, 35890])\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 18 nodes and 8 edges.\n",
      "Edge Type: [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Dataset\n",
      "Exploring subgraph for node indices: tensor([4329, 2323,   97,   61,   65, 2131, 4180, 3752,  293, 4253])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Extracted subgraph with 2263 nodes and 16366 edges.\n",
      "Edge Type: [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: ScienceKeyword\n",
      "Exploring subgraph for node indices: tensor([ 94, 802, 888, 989, 139,  70,  72,  96, 729, 229])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 374 nodes and 738 edges.\n",
      "Edge Type: [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Project\n",
      "Exploring subgraph for node indices: tensor([109, 117,  26,  59,  89, 105, 108, 121,   0, 137])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Extracted subgraph with 197 nodes and 206 edges.\n",
      "Edge Type: [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "\n",
      "Exploring subgraph for node type: Platform\n",
      "Exploring subgraph for node indices: tensor([165, 166, 173, 178, 344,  68, 248, 307, 344, 409])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Extracted subgraph with 1953 nodes and 4553 edges.\n",
      "Edge Type: [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "LLM-Generated Explanation for Node Type 'Publication':\n",
      "A subgraph containing one node of type 'Publication' and zero edges, despite having defined edge types involving 'Publication', signifies an isolated publication within the larger graph.  This isolation means:\n",
      "\n",
      "* **No citations:** The absence of edges of type ('Publication', 'CITES', 'Publication') indicates this publication doesn't cite any other publications in the dataset.  This could be a very recent publication, a self-contained work, or a data entry error.\n",
      "\n",
      "* **No associated research areas:** The lack of edges of type ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword') suggests this publication isn't linked to any specific research areas (keywords) in the dataset. Again, this could be due to incomplete data or the publication's nature.\n",
      "\n",
      "\n",
      "In essence, this subgraph highlights a data point that lacks connections within the context of the larger knowledge graph.  Further investigation is needed to determine whether this isolation reflects a genuine characteristic of the publication or a problem with data completeness or consistency.  The single node represents a \"data island\" – a piece of information disconnected from the broader network of relationships.\n",
      "\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "LLM-Generated Explanation for Node Type 'Publication':\n",
      "A subgraph containing one node of type 'Publication' and zero edges is significant because it represents an isolated publication within the larger graph.  The presence of zero edges means this publication has no known relationships to other publications (via 'CITES') or to any scientific keywords (via 'HAS_APPLIED_RESEARCH_AREA').\n",
      "\n",
      "This isolation could indicate several things:\n",
      "\n",
      "* **Data Incompleteness:** The most likely scenario is that the data about this publication is incomplete.  It might be missing citation information or keywords associated with its research area.  More data would be needed to fully integrate this publication into the knowledge graph.\n",
      "\n",
      "* **Truly Isolated Publication:** Although less probable, it's possible this publication is genuinely unique and doesn't cite other works or have explicitly defined research areas within the scope of the data being analyzed. This could represent a pioneering work or a publication outside the typical research patterns captured in the dataset.\n",
      "\n",
      "* **Data Cleaning Issue:** There might be an error in the data processing or loading that led to this node being disconnected.\n",
      "\n",
      "The information about the *possible* edge types, ('Publication', 'CITES', 'Publication') and ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword'), highlights what *should* be present but isn't.  It emphasizes the lack of connections and suggests a potential problem with the data's completeness or accuracy rather than reflecting a meaningful aspect of the research landscape.  Further investigation is required to determine the reason for the isolation.\n",
      "\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "LLM-Generated Explanation for Node Type 'Publication':\n",
      "A subgraph containing one node of type 'Publication' and zero edges, given the specified edge types, is significant because it represents an isolated publication.  The presence of the edge types ('Publication', 'CITES', 'Publication') and ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword') indicates that *potential* relationships exist between publications (citation links) and between publications and research areas.  However, the lack of *any* edges in this specific subgraph means this particular publication:\n",
      "\n",
      "* **Has not cited any other publications.**  It's either the earliest publication in a chain, a self-contained work, or its citation data is missing.\n",
      "* **Is not linked to any research areas (ScienceKeywords) via the 'HAS_APPLIED_RESEARCH_AREA' relationship.** This could mean the research area is unclassified, unknown, or the data for that relationship is absent.\n",
      "\n",
      "In essence, this isolated node highlights a data gap or a publication that stands apart from the network. It's important because it could:\n",
      "\n",
      "* **Represent a starting point for further investigation.** Researchers might need to manually check the publication's metadata for missing citation or research area data.\n",
      "* **Be an outlier.**  Its isolation might reveal interesting characteristics (e.g., very niche research, a foundational work before others built upon it, etc.).\n",
      "* **Point to data quality issues.**  Incomplete or inaccurate data entry could be responsible for the absence of edges.\n",
      "\n",
      "The significance, therefore, lies not just in the single node itself, but in the *context* of its absence from the expected relationships defined by the edge types.  It's a data point that warrants further scrutiny.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to summarize combined results using the LLM\n",
    "def summarize_results_with_llm(graph_results, wikipedia_context):\n",
    "    prompt = \"Summarize the following search results and Wikipedia context:\\n\\n\"\n",
    "\n",
    "    # Add graph results to the prompt\n",
    "    prompt += \"Graph Search Results:\\n\"\n",
    "    for node_type, idx, key, value in graph_results[:10]:  # Limit to top 5 results for brevity\n",
    "        prompt += f\"- Node Type: {node_type}, Property: {key}, Value: {str(value)[:2000]}...\\n\"\n",
    "\n",
    "    # Add Wikipedia context to the prompt\n",
    "    prompt += \"\\nWikipedia Context:\\n\"\n",
    "    for i, context in enumerate(wikipedia_context, start=1):\n",
    "        prompt += f\"{i}. Title: {context['title']}\\n\"\n",
    "        prompt += f\"   Summary: {context['summary'][:300]}...\\n\"\n",
    "    \n",
    "    with open(\"prompt_file.txt\", \"w\") as file:\n",
    "        file.write(f\"{prompt}\\n\")\n",
    "\n",
    "    # Call the Gemini model to generate the summary\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Function to generate explanations for subgraphs using the LLM\n",
    "def explain_subgraph_with_llm(node_type, edge_types, num_nodes, num_edges):\n",
    "    prompt = (\n",
    "        f\"Explain the significance of a subgraph extracted for node type '{node_type}'. \"\n",
    "        f\"The subgraph contains {num_nodes} nodes and {num_edges} edges. \"\n",
    "        f\"The edge types in this subgraph are: {', '.join([str(edge) for edge in edge_types])}.\"\n",
    "    )\n",
    "\n",
    "    # Call the Gemini model to generate the explanation\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Generate an LLM summary of the combined results\n",
    "summary = summarize_results_with_llm(graph_results, wikipedia_context)\n",
    "print(\"\\nLLM-Generated Summary:\")\n",
    "print(summary)\n",
    "\n",
    "# Explore subgraphs based on the results\n",
    "explore_subgraphs(data, graph_results)\n",
    "\n",
    "# Generate LLM explanations for each explored subgraph\n",
    "for node_type, idx, key, value in graph_results[:3]:  # Limit to 3 nodes for brevity\n",
    "    # Get subgraph for the current node type and indices\n",
    "    subset, edge_index, edge_types = get_subgraph(data, node_type, torch.tensor([idx]))\n",
    "    explanation = explain_subgraph_with_llm(node_type, edge_types, len(subset), edge_index.size(1))\n",
    "    print(f\"\\nLLM-Generated Explanation for Node Type '{node_type}':\")\n",
    "    print(explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
