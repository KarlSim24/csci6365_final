{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project: Query-Driven Retrieval-Augmented Graph Exploration Tool\n",
    "By Karl Simon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the dataset into PyG (PyTorch Geometric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes and Properties:\n",
      "\n",
      "Node Type: Dataset\n",
      "Number of Nodes: 6390\n",
      "  - temporalExtentStart: 6375 items (non-numeric)\n",
      "  - seCorner: 5330 items (non-numeric)\n",
      "  - cmrId: 6390 items (non-numeric)\n",
      "  - globalId: 6390 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([6390, 512])\n",
      "  - abstract: 6390 items (non-numeric)\n",
      "  - daac: 6131 items (non-numeric)\n",
      "  - nwCorner: 5330 items (non-numeric)\n",
      "  - temporalFrequency: 6390 items (non-numeric)\n",
      "  - pagerank_global: torch.Size([6390, 1])\n",
      "  - temporalExtentEnd: 3765 items (non-numeric)\n",
      "  - shortName: 6390 items (non-numeric)\n",
      "  - landingPageUrl: 3037 items (non-numeric)\n",
      "  - doi: 6390 items (non-numeric)\n",
      "  - longName: 6390 items (non-numeric)\n",
      "\n",
      "Node Type: DataCenter\n",
      "Number of Nodes: 184\n",
      "  - pagerank_global: torch.Size([184, 1])\n",
      "  - globalId: 184 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([184, 512])\n",
      "  - shortName: 184 items (non-numeric)\n",
      "  - url: 184 items (non-numeric)\n",
      "  - longName: 184 items (non-numeric)\n",
      "\n",
      "Node Type: Project\n",
      "Number of Nodes: 333\n",
      "  - pagerank_global: torch.Size([333, 1])\n",
      "  - globalId: 333 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([333, 512])\n",
      "  - shortName: 333 items (non-numeric)\n",
      "  - longName: 333 items (non-numeric)\n",
      "\n",
      "Node Type: Platform\n",
      "Number of Nodes: 442\n",
      "  - Type: 442 items (non-numeric)\n",
      "  - pagerank_global: torch.Size([442, 1])\n",
      "  - globalId: 442 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([442, 512])\n",
      "  - shortName: 442 items (non-numeric)\n",
      "  - longName: 442 items (non-numeric)\n",
      "\n",
      "Node Type: Instrument\n",
      "Number of Nodes: 867\n",
      "  - pagerank_global: torch.Size([867, 1])\n",
      "  - globalId: 867 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([867, 512])\n",
      "  - shortName: 867 items (non-numeric)\n",
      "  - longName: 867 items (non-numeric)\n",
      "\n",
      "Node Type: ScienceKeyword\n",
      "Number of Nodes: 1609\n",
      "  - pagerank_global: torch.Size([1609, 1])\n",
      "  - globalId: 1609 items (non-numeric)\n",
      "  - name: 1609 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([1609, 512])\n",
      "  - pagerank_publication_dataset: torch.Size([1609, 1])\n",
      "\n",
      "Node Type: Publication\n",
      "Number of Nodes: 125939\n",
      "  - year: 125939 items (non-numeric)\n",
      "  - pagerank_global: torch.Size([125939, 1])\n",
      "  - globalId: 125939 items (non-numeric)\n",
      "  - fastrp_embedding_with_labels: torch.Size([125939, 512])\n",
      "  - title: 125937 items (non-numeric)\n",
      "  - DOI: 125939 items (non-numeric)\n",
      "  - abstract: 99859 items (non-numeric)\n",
      "  - authors: 109975 items (non-numeric)\n",
      "\n",
      "Edges and Types:\n",
      "Edge Type: ('DataCenter', 'HAS_DATASET', 'Dataset') - Number of Edges: 9017 - Shape: torch.Size([2, 9017])\n",
      "Edge Type: ('Dataset', 'OF_PROJECT', 'Project') - Number of Edges: 6049 - Shape: torch.Size([2, 6049])\n",
      "Edge Type: ('Dataset', 'HAS_PLATFORM', 'Platform') - Number of Edges: 9884 - Shape: torch.Size([2, 9884])\n",
      "Edge Type: ('Platform', 'HAS_INSTRUMENT', 'Instrument') - Number of Edges: 2469 - Shape: torch.Size([2, 2469])\n",
      "Edge Type: ('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword') - Number of Edges: 1823 - Shape: torch.Size([2, 1823])\n",
      "Edge Type: ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword') - Number of Edges: 20436 - Shape: torch.Size([2, 20436])\n",
      "Edge Type: ('Publication', 'CITES', 'Publication') - Number of Edges: 208670 - Shape: torch.Size([2, 208670])\n",
      "Edge Type: ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword') - Number of Edges: 89039 - Shape: torch.Size([2, 89039])\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports for entire notebook\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "from torch_geometric.data import HeteroData\n",
    "from collections import defaultdict\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "# from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import random\n",
    "from scholarly import scholarly\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "MAX_VAL_LEN = 1000 # max text length for input to LLM from graph_results for each node\n",
    "\n",
    "\n",
    "# Load JSON data from file\n",
    "file_path = \"/home/karlsimon/CSCI6365/final/graph.json\"\n",
    "graph_data = []\n",
    "\n",
    "# Load data line by line to prevent memory overload\n",
    "with open(file_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            graph_data.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON line: {e}\")\n",
    "            continue\n",
    "\n",
    "# Initialize HeteroData object\n",
    "data = HeteroData()\n",
    "\n",
    "# Mapping for node indices per node type\n",
    "node_mappings = defaultdict(dict)\n",
    "\n",
    "# Temporary storage for properties\n",
    "node_properties = defaultdict(lambda: defaultdict(list))\n",
    "edge_indices = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# # Define limits for node subsets based on type\n",
    "# node_limits = {\n",
    "#     'Publication': 1000,\n",
    "#     'Dataset': 500,\n",
    "#     'ScienceKeyword': 300,\n",
    "#     'Instrument': 200,\n",
    "#     'Platform': 150,\n",
    "#     'Project': 100,\n",
    "#     'DataCenter': 50\n",
    "# }\n",
    "\n",
    "# Track the number of nodes added per type\n",
    "node_counts = defaultdict(int)\n",
    "\n",
    "# Process nodes with limits\n",
    "for item in graph_data:\n",
    "    if item['type'] == 'node':\n",
    "        node_type = item['labels'][0]\n",
    "        # if node_counts[node_type] >= node_limits.get(node_type, 50):\n",
    "        #     continue  # Skip nodes once the limit is reached\n",
    "\n",
    "        node_id = item['id']\n",
    "        properties = item['properties']\n",
    "\n",
    "        # Store the node index mapping\n",
    "        node_index = len(node_mappings[node_type])\n",
    "        node_mappings[node_type][node_id] = node_index\n",
    "        node_counts[node_type] += 1\n",
    "\n",
    "        # Store properties temporarily by type\n",
    "        for key, value in properties.items():\n",
    "            if isinstance(value, list) and all(isinstance(v, (int, float)) for v in value):\n",
    "                node_properties[node_type][key].append(torch.tensor(value, dtype=torch.float))\n",
    "            elif isinstance(value, (int, float)):\n",
    "                node_properties[node_type][key].append(torch.tensor([value], dtype=torch.float))\n",
    "            else:\n",
    "                node_properties[node_type][key].append(value)  # non-numeric properties as lists\n",
    "\n",
    "# # Define limits for relationships based on type\n",
    "# relationship_limits = {\n",
    "#     'CITES': 2000,\n",
    "#     'HAS_APPLIED_RESEARCH_AREA': 1000,\n",
    "#     'HAS_SCIENCEKEYWORD': 500,\n",
    "#     'HAS_PLATFORM': 500,\n",
    "#     'HAS_DATASET': 500,\n",
    "#     'OF_PROJECT': 300,\n",
    "#     'HAS_INSTRUMENT': 200\n",
    "# }\n",
    "\n",
    "# Track the number of relationships added per type\n",
    "relationship_counts = defaultdict(int)\n",
    "\n",
    "# Filter relationships to only include sampled nodes\n",
    "for item in graph_data:\n",
    "    if item['type'] == 'relationship':\n",
    "        start_type = item['start']['labels'][0]\n",
    "        end_type = item['end']['labels'][0]\n",
    "        start_id = item['start']['id']\n",
    "        end_id = item['end']['id']\n",
    "        edge_type = item['label']\n",
    "\n",
    "        # # Skip if relationship limit reached\n",
    "        # if relationship_counts[edge_type] >= relationship_limits.get(edge_type, 100):\n",
    "        #     continue\n",
    "\n",
    "        # Check if start and end nodes exist in the sampled nodes\n",
    "        if start_id in node_mappings[start_type] and end_id in node_mappings[end_type]:\n",
    "            start_idx = node_mappings[start_type][start_id]\n",
    "            end_idx = node_mappings[end_type][end_id]\n",
    "\n",
    "            # Append to edge list\n",
    "            edge_indices[(start_type, edge_type, end_type)]['start'].append(start_idx)\n",
    "            edge_indices[(start_type, edge_type, end_type)]['end'].append(end_idx)\n",
    "            relationship_counts[edge_type] += 1\n",
    "\n",
    "# Finalize node properties by batch processing\n",
    "for node_type, properties in node_properties.items():\n",
    "    data[node_type].num_nodes = len(node_mappings[node_type])\n",
    "    for key, values in properties.items():\n",
    "        if isinstance(values[0], torch.Tensor):\n",
    "            data[node_type][key] = torch.stack(values)\n",
    "        else:\n",
    "            data[node_type][key] = values  # Keep non-tensor properties as lists\n",
    "\n",
    "# Finalize edge indices in bulk\n",
    "for (start_type, edge_type, end_type), indices in edge_indices.items():\n",
    "    edge_index = torch.tensor([indices['start'], indices['end']], dtype=torch.long)\n",
    "    data[start_type, edge_type, end_type].edge_index = edge_index\n",
    "\n",
    "# Display statistics for verification\n",
    "print(\"Nodes and Properties:\")\n",
    "for node_type in data.node_types:\n",
    "    print(f\"\\nNode Type: {node_type}\")\n",
    "    print(f\"Number of Nodes: {data[node_type].num_nodes}\")\n",
    "    for key, value in data[node_type].items():\n",
    "        if key != 'num_nodes':\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                print(f\"  - {key}: {value.shape}\")\n",
    "            else:\n",
    "                print(f\"  - {key}: {len(value)} items (non-numeric)\")\n",
    "\n",
    "print(\"\\nEdges and Types:\")\n",
    "for edge_type in data.edge_types:\n",
    "    edge_index = data[edge_type].edge_index\n",
    "    print(f\"Edge Type: {edge_type} - Number of Edges: {edge_index.size(1)} - Shape: {edge_index.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1 : Search Graph for nodes based on user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Steps:\n",
    "# 1. improve graph search and rank results.\n",
    "# 2. improve subgraph exploration.\n",
    "# 3. improve external context retrieval (NASA API).\n",
    "\n",
    "# Functions definitions for keywords, search and display used in next cell\n",
    "def extract_keywords(query):\n",
    "    keywords = re.findall(r'\\b\\w+\\b', query)\n",
    "    return [kw.lower() for kw in keywords]\n",
    "\n",
    "# Updated search_graph function with TF-IDF scoring\n",
    "# TODO: make max_per_type specific to each node type\n",
    "def search_graph(data, keywords, node_types=['Dataset', 'Project', 'ScienceKeyword', 'Instrument', 'Platform', 'Publication'], max_results=50, max_per_type=10):\n",
    "    results = []\n",
    "    texts = []  # Collect text data for TF-IDF processing\n",
    "    metadata = []  # To store corresponding metadata (node type, index, key, value)\n",
    "\n",
    "    # Step 1: Collect all matching nodes and their text data\n",
    "    for node_type in node_types:\n",
    "        for key in data[node_type]:\n",
    "            if key == 'num_nodes':\n",
    "                continue\n",
    "            \n",
    "            values = data[node_type][key]\n",
    "            if isinstance(values, list):\n",
    "                for idx, value in enumerate(values):\n",
    "                    value_str = str(value).lower()\n",
    "                    if any(kw in value_str for kw in keywords):\n",
    "                        texts.append(value_str)\n",
    "                        metadata.append((node_type, idx, key, value))\n",
    "\n",
    "    if not texts:\n",
    "        return []\n",
    "\n",
    "    # Step 2: Compute TF-IDF scores for the collected texts\n",
    "    # NOTE: texts stores the properties of the nodes which contain the keywords\n",
    "    vectorizer = TfidfVectorizer(vocabulary=keywords)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    scores = tfidf_matrix.sum(axis=1).A1  # Sum the TF-IDF scores for each text\n",
    "\n",
    "    # Step 3: Sort the results by TF-IDF score in descending order\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    sorted_results = [metadata[i] for i in sorted_indices]\n",
    "    # with open(\"sorted_results.txt\", \"w\") as file:\n",
    "    #     for result in sorted_results:\n",
    "    #         file.write(f\"{result}\\n\")\n",
    "\n",
    "    # Step 4: Limit the number of results overall and per node type\n",
    "    final_results = []\n",
    "    counts_per_type = {node_type: 0 for node_type in node_types}\n",
    "\n",
    "    for result in sorted_results:\n",
    "        node_type = result[0]\n",
    "        if len(final_results) >= max_results:\n",
    "            break\n",
    "        if counts_per_type[node_type] < max_per_type:\n",
    "            final_results.append(result)\n",
    "            counts_per_type[node_type] += 1\n",
    "\n",
    "    # write the 50 final_results to a file\n",
    "    print(\"Writing 50 final_results to file\")\n",
    "    with open(\"final_results.txt\", \"w\") as file:\n",
    "        for result in final_results:\n",
    "            file.write(f\"{result}\\n\")\n",
    "\n",
    "    return final_results\n",
    "\n",
    "# Updated display_results function to trim long values\n",
    "def display_results(results, max_value_length=MAX_VAL_LEN):\n",
    "    if not results:\n",
    "        print(\"No relevant nodes found.\")\n",
    "        return\n",
    "\n",
    "    with open(\"query_results.txt\", \"w\") as file:\n",
    "        print(f\"\\nFound {len(results)} relevant nodes:\\n\")\n",
    "        for node_type, idx, key, value in results:\n",
    "            value_str = str(value)\n",
    "            if len(value_str) > max_value_length:\n",
    "                value_str = value_str[:max_value_length] + \"...\"\n",
    "            output_line = f\"Node Type: {node_type} | Index: {idx} | Property: {key} | Value: {value_str}\\n\"\n",
    "            file.write(output_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given query, extract keywords, search the graph for relevant nodes, and display the results\n",
    "# NOTE: currently only searches for exact keyword matches in node properties\n",
    "\n",
    "def get_subgraph(data, node_type, node_indices, num_hops=2):\n",
    "    # Find all edge types where the node_type is either the source or target\n",
    "    relevant_edges = [\n",
    "        (src, rel, dst) for (src, rel, dst) in data.edge_types if src == node_type or dst == node_type\n",
    "    ]\n",
    "    \n",
    "    print(\"relevant_edges = \", relevant_edges)\n",
    "\n",
    "    if not relevant_edges:\n",
    "        print(f\"No edges found for node type '{node_type}'\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Combine edge indices from all relevant edge types\n",
    "    combined_edge_index = []\n",
    "    combined_edge_types = []\n",
    "\n",
    "    for edge_type in relevant_edges:\n",
    "        edge_index = data[edge_type].edge_index\n",
    "        combined_edge_index.append(edge_index)\n",
    "        combined_edge_types.append(edge_type)\n",
    "\n",
    "    # Stack all edge indices into a single tensor\n",
    "    combined_edge_index = torch.cat(combined_edge_index, dim=1)\n",
    "\n",
    "    # Extract the subgraph using the combined edge index\n",
    "    subset, edge_index, _, _ = k_hop_subgraph(node_idx=node_indices, num_hops=num_hops, edge_index=combined_edge_index)\n",
    "    return subset, edge_index, combined_edge_types\n",
    "\n",
    "\n",
    "# Explore subgraphs based on the search results.\n",
    "def explore_subgraphs(data, results, num_hops=2):\n",
    "    if not results:\n",
    "        print(\"No nodes to explore for subgraphs.\")\n",
    "        return\n",
    "\n",
    "    # Group the results by node type\n",
    "    nodes_by_type = defaultdict(list)\n",
    "    for node_type, idx, _, _ in results:\n",
    "        nodes_by_type[node_type].append(idx)\n",
    "\n",
    "    # Extract and display subgraphs for each node type\n",
    "    for node_type, indices in nodes_by_type.items():\n",
    "        print(f\"\\nExploring subgraph for node type: {node_type}\")\n",
    "        # print(f\"Number of nodes: {len(indices)}\") #10 nodes\n",
    "        # Get the valid range for node indices\n",
    "        num_nodes = data[node_type].num_nodes\n",
    "        valid_indices = [idx for idx in indices if idx < num_nodes]\n",
    "\n",
    "        if not valid_indices:\n",
    "            print(f\"No valid indices for node type '{node_type}'.\")\n",
    "            continue\n",
    "\n",
    "        node_indices = torch.tensor(valid_indices[:10])  # Limit to 10 nodes (only using 10 per node_type anyways for now)\n",
    "        print(f\"Exploring subgraph for node indices: {node_indices}\") # may not be sequential due to search results ordering \n",
    "        subset, edge_index, edge_type = get_subgraph(data, node_type, node_indices, num_hops=num_hops)\n",
    "\n",
    "        if subset is not None and edge_index is not None:\n",
    "            print(f\"Extracted subgraph with {len(subset)} nodes and {edge_index.size(1)} edges.\")\n",
    "            print(f\"Edge Type: {edge_type}\")\n",
    "        else:\n",
    "            print(f\"Could not extract subgraph for node type: {node_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 : Use APIs Wikipedia for external information based on user query\n",
    "- Question: should API be queries based on keywords or the extracted graph nodes from keywords?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get external context from Wikipedia using the REST API\n",
    "def fetch_wikipedia_context(keywords):\n",
    "    search_term = \" \".join(keywords)\n",
    "    \n",
    "    # Step 1: Use the Action API to get the top 5 search results\n",
    "    search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    search_params = {\n",
    "        \"action\": \"query\",\n",
    "        \"list\": \"search\",\n",
    "        \"srsearch\": search_term,\n",
    "        \"srlimit\": 5,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"GraphExplorationTool/1.0 (ksimon24@gwu.edu)\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        search_response = requests.get(search_url, params=search_params, headers=headers)\n",
    "        search_response.raise_for_status()\n",
    "        search_data = search_response.json()\n",
    "        \n",
    "        search_results = search_data.get(\"query\", {}).get(\"search\", [])\n",
    "        \n",
    "        if not search_results:\n",
    "            return None\n",
    "\n",
    "        # Step 2: Fetch summaries using the REST API for each search result\n",
    "        context_list = []\n",
    "        for result in search_results:\n",
    "            print(\"result = \", result)\n",
    "            page_title = result.get(\"title\")\n",
    "            rest_url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{page_title.replace(' ', '_')}\"\n",
    "            \n",
    "            rest_response = requests.get(rest_url, headers=headers)\n",
    "            rest_response.raise_for_status()\n",
    "            rest_data = rest_response.json()\n",
    "            \n",
    "            # Extract relevant information\n",
    "            title = rest_data.get(\"title\", \"No Title\")\n",
    "            description = rest_data.get(\"description\", \"No Description Available.\")\n",
    "            summary = rest_data.get(\"extract\", \"No Summary Available.\")\n",
    "            link = rest_data.get(\"content_urls\", {}).get(\"desktop\", {}).get(\"page\", \"No Link Available.\")\n",
    "            thumbnail = rest_data.get(\"thumbnail\", {}).get(\"source\", None)\n",
    "            \n",
    "            context_entry = {\n",
    "                \"title\": title,\n",
    "                \"description\": description,\n",
    "                \"summary\": summary,\n",
    "                \"link\": link,\n",
    "                \"thumbnail\": thumbnail\n",
    "            }\n",
    "            context_list.append(context_entry)\n",
    "        \n",
    "        return context_list\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching Wikipedia context: {e}\")\n",
    "        return None\n",
    "    \n",
    "def display_wikipedia_context(context_list):\n",
    "    if not context_list:\n",
    "        print(\"\\nNo external context available from Wikipedia.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nWikipedia Context:\")\n",
    "    for i, context in enumerate(context_list, start=1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"Title: {context['title']}\")\n",
    "        print(f\"Description: {context['description']}\")\n",
    "        print(f\"Summary: {context['summary']}\")\n",
    "        print(f\"Link: {context['link']}\")\n",
    "        if context['thumbnail']:\n",
    "            print(f\"Thumbnail: {context['thumbnail']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted Keywords: ['climate', 'change']\n",
      "Writing 50 final_results to file\n",
      "\n",
      "Found 50 relevant nodes:\n",
      "\n",
      "result =  {'ns': 0, 'title': 'Climate change', 'pageid': 5042951, 'size': 317341, 'wordcount': 27919, 'snippet': 'Present-day <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span> includes both global warming—the ongoing increase in global average temperature—and its wider effects on Earth\\'s <span class=\"searchmatch\">climate</span>. <span class=\"searchmatch\">Climate</span> change', 'timestamp': '2024-12-10T03:05:32Z'}\n",
      "result =  {'ns': 0, 'title': 'Climate change denial', 'pageid': 12474403, 'size': 237127, 'wordcount': 22133, 'snippet': '<span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">change</span> denial (also global warming denial) is a form of science denial characterized by rejecting, refusing to acknowledge, disputing, or fighting', 'timestamp': '2024-12-04T00:54:58Z'}\n",
      "result =  {'ns': 0, 'title': 'Climate change mitigation', 'pageid': 2119179, 'size': 229408, 'wordcount': 22636, 'snippet': '<span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">change</span> mitigation (or decarbonisation) is action to limit the greenhouse gases in the atmosphere that cause <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span>. <span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">change</span> mitigation', 'timestamp': '2024-12-09T04:31:50Z'}\n",
      "result =  {'ns': 0, 'title': 'Effects of climate change', 'pageid': 2119174, 'size': 180605, 'wordcount': 20335, 'snippet': 'Effects of <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span> are well documented and growing for Earth\\'s natural environment and human societies. <span class=\"searchmatch\">Changes</span> to the <span class=\"searchmatch\">climate</span> system include an', 'timestamp': '2024-12-08T12:02:41Z'}\n",
      "result =  {'ns': 0, 'title': 'United Nations Framework Convention on Climate Change', 'pageid': 31898, 'size': 95299, 'wordcount': 9711, 'snippet': 'Framework Convention on <span class=\"searchmatch\">Climate</span> <span class=\"searchmatch\">Change</span> (UNFCCC) is the UN process for negotiating an agreement to limit dangerous <span class=\"searchmatch\">climate</span> <span class=\"searchmatch\">change</span>. It is an international', 'timestamp': '2024-11-18T10:15:56Z'}\n",
      "\n",
      "Wikipedia Context:\n",
      "\n",
      "Result 1:\n",
      "Title: Climate change\n",
      "Description: Human-caused changes to climate on Earth\n",
      "Summary: Present-day climate change includes both global warming—the ongoing increase in global average temperature—and its wider effects on Earth's climate. Climate change in a broader sense also includes previous long-term changes to Earth's climate. The current rise in global temperatures is driven by human activities, especially fossil fuel burning since the Industrial Revolution. Fossil fuel use, deforestation, and some agricultural and industrial practices release greenhouse gases. These gases absorb some of the heat that the Earth radiates after it warms from sunlight, warming the lower atmosphere. Carbon dioxide, the primary greenhouse gas driving global warming, has grown by about 50% and is at levels not seen for millions of years.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Change_in_Average_Temperature_With_Fahrenheit.svg/320px-Change_in_Average_Temperature_With_Fahrenheit.svg.png\n",
      "\n",
      "Result 2:\n",
      "Title: Climate change denial\n",
      "Description: Denial of the scientific consensus on climate change\n",
      "Summary: Climate change denial is a form of science denial characterized by rejecting, refusing to acknowledge, disputing, or fighting the scientific consensus on climate change. Those promoting denial commonly use rhetorical tactics to give the appearance of a scientific controversy where there is none. Climate change denial includes unreasonable doubts about the extent to which climate change is caused by humans, its effects on nature and human society, and the potential of adaptation to global warming by human actions. To a lesser extent, climate change denial can also be implicit when people accept the science but fail to reconcile it with their belief or action. Several studies have analyzed these positions as forms of denialism, pseudoscience, or propaganda.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change_denial\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Inhofe_holding_snowball.jpg/320px-Inhofe_holding_snowball.jpg\n",
      "\n",
      "Result 3:\n",
      "Title: Climate change mitigation\n",
      "Description: Actions to reduce net greenhouse gas emissions to limit climate change\n",
      "Summary: Climate change mitigation (or decarbonisation) is action to limit the greenhouse gases in the atmosphere that cause climate change. Climate change mitigation actions include conserving energy and replacing fossil fuels with clean energy sources. Secondary mitigation strategies include changes to land use and removing carbon dioxide (CO2) from the atmosphere. Current climate change mitigation policies are insufficient as they would still result in global warming of about 2.7 °C by 2100, significantly above the 2015 Paris Agreement's goal of limiting global warming to below 2 °C.\n",
      "Link: https://en.wikipedia.org/wiki/Climate_change_mitigation\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/8/8f/Westmill_Solar_2.jpg/320px-Westmill_Solar_2.jpg\n",
      "\n",
      "Result 4:\n",
      "Title: Effects of climate change\n",
      "Description: No Description Available.\n",
      "Summary: Effects of climate change are well documented and growing for Earth's natural environment and human societies. Changes to the climate system include an overall warming trend, changes to precipitation patterns, and more extreme weather. As the climate changes it impacts the natural environment with effects such as more intense forest fires, thawing permafrost, and desertification. These changes impact ecosystems and societies, and can become irreversible once tipping points are crossed. Climate activists are engaged in a range of activities around the world that seek to ameliorate these issues or prevent them from happening.\n",
      "Link: https://en.wikipedia.org/wiki/Effects_of_climate_change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/062821Yreka_Fire_CalFire_-2wiki.jpg/320px-062821Yreka_Fire_CalFire_-2wiki.jpg\n",
      "\n",
      "Result 5:\n",
      "Title: United Nations Framework Convention on Climate Change\n",
      "Description: International environmental treaty\n",
      "Summary: The United Nations Framework Convention on Climate Change (UNFCCC) is the UN process for negotiating an agreement to limit dangerous climate change. It is an international treaty among countries to combat \"dangerous human interference with the climate system\". The main way to do this is limiting the increase in greenhouse gases in the atmosphere. It was signed in 1992 by 154 states at the United Nations Conference on Environment and Development (UNCED), informally known as the Earth Summit, held in Rio de Janeiro. The treaty entered into force on 21 March 1994. \"UNFCCC\" is also the name of the Secretariat charged with supporting the operation of the convention, with offices on the UN Campus in Bonn, Germany.\n",
      "Link: https://en.wikipedia.org/wiki/United_Nations_Framework_Convention_on_Climate_Change\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/UNFCCC_Annex_I_Parties%2C_OECD%2C_EU.svg/320px-UNFCCC_Annex_I_Parties%2C_OECD%2C_EU.svg.png\n",
      "\n",
      "Exploring subgraph for node type: Publication\n",
      "Exploring subgraph for node indices: tensor([76606, 94077, 50763, 55702, 88548, 42689, 82757, 12919, 60848, 35890])\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 18 nodes and 8 edges.\n",
      "Edge Type: [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Dataset\n",
      "Exploring subgraph for node indices: tensor([4329, 2323,   97,   61,   65, 2131, 4180, 3752,  293, 4253])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Extracted subgraph with 2263 nodes and 16366 edges.\n",
      "Edge Type: [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: ScienceKeyword\n",
      "Exploring subgraph for node indices: tensor([ 94, 802, 888, 989, 139,  70,  72,  96, 729, 229])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 374 nodes and 738 edges.\n",
      "Edge Type: [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Project\n",
      "Exploring subgraph for node indices: tensor([109, 117,  26,  59,  89, 105, 108, 121,   0, 137])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Extracted subgraph with 197 nodes and 206 edges.\n",
      "Edge Type: [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "\n",
      "Exploring subgraph for node type: Platform\n",
      "Exploring subgraph for node indices: tensor([165, 166, 173, 178, 344,  68, 248, 307, 344, 409])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Extracted subgraph with 1953 nodes and 4553 edges.\n",
      "Edge Type: [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n"
     ]
    }
   ],
   "source": [
    "# ################ Pre-LLM Steps ################\n",
    "# query = input(\"Enter your query (e.g., 'Find datasets related to climate change projects'): \")\n",
    "query = \"climate change\" #TODO: remove hardcoded query\n",
    "keywords = extract_keywords(query)\n",
    "print(f\"\\nExtracted Keywords: {keywords}\")\n",
    "\n",
    "# Search the graph with TF-IDF ranking\n",
    "graph_results = search_graph(data, keywords)\n",
    "display_results(graph_results) # 50 results\n",
    "\n",
    "# Fetch Wikipedia context\n",
    "wikipedia_context = fetch_wikipedia_context(keywords)\n",
    "display_wikipedia_context(wikipedia_context)\n",
    "\n",
    "# Explore subgraphs based on the results\n",
    "# TODO: save these subgraphs\n",
    "explore_subgraphs(data, graph_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin the RAG Pipeline with LLM Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Gemini API ############\n",
    "# Define the path to the text file containing the API key\n",
    "file_path = \"/home/karlsimon/CSCI6365/final/gemini_api_key.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    api_key = file.read().strip()\n",
    "# print(api_key)\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Create a model instance (using Gemini 1.5 Flash in this case)\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LLM-Generated Summary:\n",
      "The provided text comprises abstracts from several research papers investigating diverse aspects of climate change and its impacts.  The studies cover a wide range of topics including:\n",
      "\n",
      "* **Impact on Evapotranspiration (ET):** One study assesses the spatial and temporal variations in ET in the Narmada river basin (India) using SEBAL and predicts future changes based on land use and climate change models (ACCESS1-0 and Markov Chain).\n",
      "\n",
      "* **Climate Change and Forest Fires:** Another study examines the relationship between climate change (using GRACE data) and forest fires in Yunnan province, China, analyzing the spatiotemporal distribution of fires and their correlation with hydrological and climatic factors.\n",
      "\n",
      "* **Climate Change Impacts on Marine Ecosystems:**  Research investigates the effect of climate change-induced alterations in prey quality (fatty acid composition) on juvenile Chinook salmon, focusing on their nutritional condition and growth.\n",
      "\n",
      "* **Stratospheric Ozone and Climate Change Interactions:** A study discusses the complex interplay between stratospheric ozone depletion, recovery (due to the Montreal Protocol), and climate change, highlighting the uncertainties involved.\n",
      "\n",
      "* **Climate Model Sensitivity:**  Analysis focuses on the increased equilibrium climate sensitivity (ECS) in the CNRM-CM6-1 climate model compared to its predecessor, attributing the increase to changes in atmospheric components, particularly cloud radiative responses.\n",
      "\n",
      "* **Climate Change and Human Migration:** Research explores the individual-level decision-making processes behind climate-induced migration, emphasizing the interplay of economic, social, environmental, and cultural factors in a vulnerable region.\n",
      "\n",
      "* **Climate Change and Groundwater:** A study investigates the impact of climate change on groundwater table fluctuations in Hungary's Great Hungarian Plain, utilizing hydrological models and climate projections to predict future groundwater levels.\n",
      "\n",
      "* **Climate Change Impacts on Vegetation:** Research uses a Land Surface Model (SURFEX) to simulate the impact of climate change on the biomass and soil water content of various vegetation types (cereals, grasslands, forests) in France.\n",
      "\n",
      "* **Climate Change and Flood Frequency:** A study assesses the implications of climate change for the bivariate quantiles of flood peak and volume in the Ganjiang River basin (China), using climate model outputs and hydrological modeling.\n",
      "\n",
      "* **Climate Change Mitigation and Adaptation in Agriculture:**  A systematic review analyzes the evidence for synergies and trade-offs between climate change mitigation and adaptation strategies in agriculture, revealing that claims of synergies may be overstated.\n",
      "\n",
      "\n",
      "The Wikipedia context provides background information on climate change, encompassing its causes, consequences, mitigation efforts, and the scientific consensus surrounding it, as well as the issue of climate change denial.  The research papers largely contribute to a deeper understanding of the multifaceted impacts of climate change across various geographical locations and ecological systems.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to summarize combined results using the LLM\n",
    "def summarize_results_with_llm(graph_results, wikipedia_context):\n",
    "    prompt = \"Summarize the following search results and Wikipedia context:\\n\\n\"\n",
    "\n",
    "    # Add graph results to the prompt\n",
    "    prompt += \"Graph Search Results:\\n\"\n",
    "    for node_type, idx, key, value in graph_results[:10]:  # Limit to top 10 results\n",
    "        prompt += f\"- Node Type: {node_type}, Property: {key}, Value: {str(value)[:MAX_VAL_LEN]}...\\n\"\n",
    "\n",
    "    # Add Wikipedia context to the prompt\n",
    "    prompt += \"\\nWikipedia Context:\\n\"\n",
    "    for i, context in enumerate(wikipedia_context, start=1):\n",
    "        prompt += f\"{i}. Title: {context['title']}\\n\"\n",
    "        prompt += f\"   Summary: {context['summary'][:MAX_VAL_LEN]}...\\n\"\n",
    "    \n",
    "    with open(\"prompt_file.txt\", \"w\") as file:\n",
    "        file.write(f\"{prompt}\\n\")\n",
    "\n",
    "    # Call the Gemini model to generate the summary\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Generate an LLM summary of the combined results\n",
    "summary = summarize_results_with_llm(graph_results, wikipedia_context)\n",
    "print(\"\\nLLM-Generated Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Use subgraph for additional information and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploring subgraph for node type: Publication\n",
      "Exploring subgraph for node indices: tensor([76606, 94077, 50763, 55702, 88548, 42689, 82757, 12919, 60848, 35890])\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 18 nodes and 8 edges.\n",
      "Edge Type: [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Dataset\n",
      "Exploring subgraph for node indices: tensor([4329, 2323,   97,   61,   65, 2131, 4180, 3752,  293, 4253])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Extracted subgraph with 2263 nodes and 16366 edges.\n",
      "Edge Type: [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: ScienceKeyword\n",
      "Exploring subgraph for node indices: tensor([ 94, 802, 888, 989, 139,  70,  72,  96, 729, 229])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Extracted subgraph with 374 nodes and 738 edges.\n",
      "Edge Type: [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "\n",
      "Exploring subgraph for node type: Project\n",
      "Exploring subgraph for node indices: tensor([109, 117,  26,  59,  89, 105, 108, 121,   0, 137])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Extracted subgraph with 197 nodes and 206 edges.\n",
      "Edge Type: [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "\n",
      "Exploring subgraph for node type: Platform\n",
      "Exploring subgraph for node indices: tensor([165, 166, 173, 178, 344,  68, 248, 307, 344, 409])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Extracted subgraph with 1953 nodes and 4553 edges.\n",
      "Edge Type: [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "subset =  tensor([   6,    8,   46,  478,  482,  486,  629,  716,  739,  871,  881,  886,\n",
      "         897,  924,  936,  939,  993, 1000, 1005, 1032, 1102, 2323]) , edge_index =  tensor([[   6,    8,   46,   46,  629,  716,  739,  871,  881,  886,  897,  924,\n",
      "          936,  939,  993, 1000, 1005, 1032, 1102,    6,  478,  482,  486,  886,\n",
      "          886],\n",
      "        [  46,   46, 2323,    6,   46,   46,   46,   46,   46,   46,   46,   46,\n",
      "           46,   46,   46,   46,   46,   46,   46,    6,   46,   46,   46,    8,\n",
      "            6]]) , edge_types =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')] and node_type =  Dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Explore subgraphs based on the results (NOT LLM dependent, just for output to terminal)\n",
    "explore_subgraphs(data, graph_results)\n",
    "\n",
    "# TODO: change metrics to evaluate the LLM generated explanations\n",
    "# Generate LLM explanations for each explored subgraph\n",
    "\n",
    "# print(\"length of graph_results[:3]: \", len(graph_results)) # len(graph_results) = 50, and len(graph_results[:3]) = 3\n",
    "# print(\"graph_results[:3]: \", graph_results[:3])\n",
    "\n",
    "for node_type, idx, key, value in graph_results[11:12]:  # Limit to 3 nodes for brevity\n",
    "    # Get subgraph for the current node type and indices\n",
    "    subset, edge_index, edge_types = get_subgraph(data, node_type, torch.tensor([idx]))\n",
    "    print(f\"subset = \", subset, \", edge_index = \", edge_index, \", edge_types = \", edge_types, \"and node_type = \", node_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# specify the priorities to use in value selection\n",
    "def get_priority_properties():\n",
    "    priority_properties = {\n",
    "        'Dataset': ['longName', 'abstract', 'shortName'],\n",
    "        'Publication': ['title', 'abstract'],\n",
    "        'ScienceKeyword': ['name'],\n",
    "        'Instrument': ['longName', 'shortName'],\n",
    "        'Platform': ['longName', 'shortName'],\n",
    "        'Project': ['longName', 'shortName'],\n",
    "        'DataCenter': ['longName', 'shortName']\n",
    "    }\n",
    "    return priority_properties\n",
    "\n",
    "def create_nodes_of_interest(graph_results, max_per_type=3):\n",
    "    nodes_by_type = defaultdict(list)\n",
    "    for node_type, idx, key, value in graph_results:\n",
    "        nodes_by_type[node_type].append((idx, node_type, key, value))\n",
    "\n",
    "    # Select up to max_per_type nodes for each type\n",
    "    nodes_of_interest = []\n",
    "    for node_type, nodes in nodes_by_type.items():\n",
    "        nodes_of_interest.extend(random.sample(nodes, min(max_per_type, len(nodes))))\n",
    "\n",
    "    return nodes_of_interest\n",
    "\n",
    "def explore_subgraph_nodes(data, node_type, node_id, num_hops=2, max_per_type=3):\n",
    "    priority_properties = get_priority_properties()\n",
    "    subset, edge_index, edge_types = get_subgraph(data, node_type, torch.tensor([node_id]), num_hops=num_hops)\n",
    "\n",
    "    if subset is None:\n",
    "        return []\n",
    "\n",
    "    # Map node indices to their types and values\n",
    "    subgraph_nodes = []\n",
    "    print(f\"Number of nodes in subgraph for node_id: {node_id} = {len(subset)} | subset = {subset}\")\n",
    "\n",
    "    for sub_id in subset.tolist():\n",
    "        for sub_node_type in data.node_types:\n",
    "            num_nodes = data[sub_node_type].num_nodes\n",
    "            if sub_id < num_nodes:\n",
    "                # Attempt to find a meaningful property\n",
    "                value = None\n",
    "                for prop in priority_properties.get(sub_node_type, []):\n",
    "                    if prop in data[sub_node_type] and len(data[sub_node_type][prop]) > sub_id:\n",
    "                        value = data[sub_node_type][prop][sub_id]\n",
    "                        break\n",
    "                if value is None:  # Fallback to globalId or indicate no value\n",
    "                    value = data[sub_node_type].get('globalId', ['No value'])[sub_id] if 'globalId' in data[sub_node_type] else 'No value'\n",
    "                \n",
    "                subgraph_nodes.append((sub_id, sub_node_type, value))\n",
    "\n",
    "    # Group by node type and select a random subset of up to max_per_type nodes\n",
    "    nodes_by_type = defaultdict(list)\n",
    "    for node_id, node_type, value in subgraph_nodes:\n",
    "        nodes_by_type[node_type].append((node_id, node_type, value))\n",
    "\n",
    "    exploration_list = []\n",
    "    for node_type, nodes in nodes_by_type.items():\n",
    "        exploration_list.extend(random.sample(nodes, min(max_per_type, len(nodes))))\n",
    "\n",
    "    return exploration_list\n",
    "\n",
    "def write_exploration_to_file(data, graph_results, filename=\"graph_exploration.txt\"):\n",
    "    nodes_of_interest = create_nodes_of_interest(graph_results)\n",
    "\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(\"=== Nodes of Interest ===\\n\")\n",
    "        for idx, node_type, key, value in nodes_of_interest:\n",
    "            value_str = str(value)\n",
    "            display_value = value_str[:MAX_VAL_LEN] + (\"...\" if len(value_str) > MAX_VAL_LEN else \"\")\n",
    "            file.write(f\"ID: {idx}, Type: {node_type}, Key: {key}, Value: {display_value}\\n\")\n",
    "\n",
    "        file.write(\"\\n=== Subgraph Exploration ===\\n\")\n",
    "        for idx, node_type, key, value in nodes_of_interest:\n",
    "            file.write(f\"\\nExploring Subgraph for Node ID: {idx} (Type: {node_type})\\n\")\n",
    "            subgraph_nodes = explore_subgraph_nodes(data, node_type, idx)\n",
    "            for sub_id, sub_node_type, sub_value in subgraph_nodes:\n",
    "                sub_value_str = str(sub_value)\n",
    "                display_sub_value = sub_value_str[:MAX_VAL_LEN] + (\"...\" if len(sub_value_str) > MAX_VAL_LEN else \"\")\n",
    "                file.write(f\"  - ID: {sub_id}, Type: {sub_node_type}, Value: {display_sub_value}\\n\")\n",
    "\n",
    "    print(f\"\\nExploration results written to '{filename}'.\")\n",
    "\n",
    "\n",
    "def interactive_exploration(data):\n",
    "    priority_properties = get_priority_properties()\n",
    "    while True:\n",
    "        choice = input(\"\\nEnter a Node ID to explore further (or 'q' to quit): \")\n",
    "        if choice.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            node_id = int(choice)\n",
    "            node_type = input(\"Enter the Node Type (e.g., Dataset, ScienceKeyword, Instrument): \").strip()\n",
    "\n",
    "            # Validate the node type\n",
    "            if node_type not in data.node_types:\n",
    "                print(f\"Invalid node type '{node_type}'. Available types: {data.node_types}\")\n",
    "                continue\n",
    "\n",
    "            num_nodes = data[node_type].num_nodes\n",
    "            if node_id >= num_nodes:\n",
    "                print(f\"No node with ID: {node_id} in type '{node_type}'.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nSelected Node ID: {node_id} (Type: {node_type})\")\n",
    "            action = input(\"Enter 'wiki' to fetch Wikipedia context or 'subgraph' to explore subgraph of node: \").lower()\n",
    "\n",
    "            if action == 'wiki':\n",
    "                # Select a meaningful property using priority_properties\n",
    "                value = None\n",
    "                for prop in priority_properties.get(node_type, []):\n",
    "                    if prop in data[node_type] and len(data[node_type][prop]) > node_id:\n",
    "                        value = data[node_type][prop][node_id]\n",
    "                        break\n",
    "\n",
    "                if value is None:\n",
    "                    value = 'No value'\n",
    "\n",
    "                wikipedia_context = fetch_wikipedia_context([str(value)])\n",
    "                print(\"The prompt used for the Wikipedia context =\", str(value))\n",
    "                display_wikipedia_context(wikipedia_context)\n",
    "\n",
    "            elif action == 'subgraph':\n",
    "                subgraph_nodes = explore_subgraph_nodes(data, node_type, node_id)\n",
    "                print(f\"\\nSubgraph for Node ID: {node_id} (Type: {node_type})\")\n",
    "                for sub_id, sub_node_type, sub_value in subgraph_nodes:\n",
    "                    sub_value_str = str(sub_value)\n",
    "                    display_sub_value = sub_value_str[:MAX_VAL_LEN] + (\"...\" if len(sub_value_str) > MAX_VAL_LEN else \"\")\n",
    "                    print(f\"  - ID: {sub_id}, Type: {sub_node_type}, Value: {display_sub_value}\")\n",
    "\n",
    "            else:\n",
    "                print(\"Invalid action. Please enter 'wiki' or 'subgraph'.\")\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"Invalid Node ID. Please enter a valid number.\")\n",
    "\n",
    "def run_exploration_tool(data, graph_results):\n",
    "    # Write initial exploration to file\n",
    "    write_exploration_to_file(data, graph_results)\n",
    "\n",
    "    # Start interactive exploration\n",
    "    interactive_exploration(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 60848 = 1 | subset = tensor([60848])\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 76606 = 1 | subset = tensor([76606])\n",
      "relevant_edges =  [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 42689 = 1 | subset = tensor([42689])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 3752 = 10 | subset = tensor([   6,   54,  492,  497,  500,  778,  940,  967, 1107, 3752])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 4253 = 10 | subset = tensor([   6,   54,  492,  497,  500,  778,  940,  967, 1107, 4253])\n",
      "relevant_edges =  [('DataCenter', 'HAS_DATASET', 'Dataset'), ('Dataset', 'OF_PROJECT', 'Project'), ('Dataset', 'HAS_PLATFORM', 'Platform'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 4329 = 10 | subset = tensor([   6,   54,  492,  497,  500,  778,  940,  967, 1107, 4329])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 888 = 3 | subset = tensor([803, 887, 888])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 802 = 279 | subset = tensor([  70,   75,   90,  108,  123,  128,  129,  164,  174,  176,  182,  184,\n",
      "         226,  250,  271,  275,  281,  343,  459,  466,  802,  803,  887,  992,\n",
      "         996, 1182, 1704, 1707, 1714, 1736, 1772, 1775, 1786, 1820, 1831, 1838,\n",
      "        1846, 1858, 1860, 1905, 1920, 1922, 1939, 1957, 1998, 2010, 2013, 2020,\n",
      "        2098, 2101, 2117, 2137, 2168, 2184, 2208, 2229, 2245, 2250, 2258, 2275,\n",
      "        2293, 2299, 2303, 2323, 2344, 2345, 2347, 2361, 2391, 2409, 2424, 2431,\n",
      "        2520, 2522, 2526, 2559, 2591, 2599, 2607, 2612, 2632, 2647, 2670, 2685,\n",
      "        2689, 2700, 2703, 2705, 2732, 2739, 2759, 2779, 2803, 2806, 2844, 2854,\n",
      "        2865, 2874, 2875, 2877, 2912, 2922, 2946, 2953, 2960, 2978, 2983, 2985,\n",
      "        2998, 3005, 3011, 3017, 3023, 3026, 3033, 3036, 3221, 3276, 3335, 3362,\n",
      "        3383, 3409, 3415, 3430, 3628, 3680, 4149, 5080, 5088, 5089, 5094, 5095,\n",
      "        5096, 5101, 5105, 5106, 5108, 5111, 5120, 5121, 5129, 5133, 5140, 5141,\n",
      "        5146, 5148, 5153, 5156, 5161, 5165, 5167, 5171, 5197, 5198, 5208, 5233,\n",
      "        5238, 5243, 5244, 5247, 5252, 5253, 5259, 5272, 5273, 5293, 5295, 5302,\n",
      "        5303, 5319, 5330, 5342, 5348, 5355, 5361, 5366, 5367, 5371, 5372, 5373,\n",
      "        5376, 5381, 5389, 5392, 5393, 5394, 5404, 5421, 5426, 5430, 5432, 5437,\n",
      "        5442, 5449, 5458, 5459, 5461, 5470, 5473, 5477, 5478, 5480, 5483, 5494,\n",
      "        5497, 5498, 5506, 5511, 5517, 5530, 5537, 5538, 5541, 5546, 5557, 5566,\n",
      "        5567, 5571, 5574, 5576, 5580, 5598, 5604, 5607, 5611, 5625, 5632, 5640,\n",
      "        5651, 5656, 5665, 5666, 5674, 5675, 5676, 5678, 5679, 5681, 5686, 5695,\n",
      "        5696, 5699, 5702, 5704, 5705, 5711, 5713, 5728, 5729, 5735, 5736, 5741,\n",
      "        5742, 5744, 5755, 5760, 5765, 5767, 5783, 5798, 5803, 5816, 5827, 5830,\n",
      "        5834, 5837, 5843, 5846, 5848, 5851, 5852, 5867, 5868, 5872, 5873, 5874,\n",
      "        5898, 5906, 5907])\n",
      "relevant_edges =  [('ScienceKeyword', 'HAS_SUBCATEGORY', 'ScienceKeyword'), ('Dataset', 'HAS_SCIENCEKEYWORD', 'ScienceKeyword'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n",
      "Number of nodes in subgraph for node_id: 96 = 82 | subset = tensor([  90,   96, 3220, 3222, 3224, 3225, 3227, 3230, 3231, 3246, 3248, 3262,\n",
      "        3264, 3270, 3286, 3301, 3305, 3307, 3310, 3312, 3313, 3315, 3318, 3327,\n",
      "        3330, 3347, 3350, 3358, 3372, 3375, 3377, 3379, 3382, 3389, 3393, 3397,\n",
      "        3399, 3400, 3410, 3414, 3416, 3418, 3422, 3428, 3432, 3433, 3440, 3442,\n",
      "        3449, 3452, 3462, 3476, 3481, 3485, 3492, 3493, 3494, 3496, 3504, 3511,\n",
      "        3512, 3517, 3522, 3529, 3531, 3532, 3546, 3561, 3564, 3568, 3575, 3576,\n",
      "        3577, 3578, 5102, 5118, 5188, 5347, 5405, 5410, 5712, 5789])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Number of nodes in subgraph for node_id: 105 = 7 | subset = tensor([ 105, 2245, 2347, 2431, 2689, 2844, 2877])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Number of nodes in subgraph for node_id: 121 = 26 | subset = tensor([ 121, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205,\n",
      "        3206, 3207, 3209, 3210, 3362, 3374, 3409, 3415, 3514, 3521, 5916, 5917,\n",
      "        5918, 5919])\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Number of nodes in subgraph for node_id: 108 = 3 | subset = tensor([ 108, 2323, 2424])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Number of nodes in subgraph for node_id: 166 = 28 | subset = tensor([  69,   81,   85,  166,  335,  548,  554,  607,  673,  690,  697,  721,\n",
      "         735,  751,  752,  819,  834,  904,  946,  979,  994, 1095, 1723, 1742,\n",
      "        1779, 2029, 2467, 2760])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Number of nodes in subgraph for node_id: 409 = 576 | subset = tensor([  42,   69,   71,   76,  113,  152,  173,  180,  227,  291,  330,  409,\n",
      "         532,  533,  537,  541,  547,  549,  557,  559,  560,  561,  563,  566,\n",
      "         567,  568,  571,  575,  581,  582,  586,  588,  592,  595,  596,  597,\n",
      "         603,  604,  606,  608,  609,  612,  615,  617,  619,  623,  625,  627,\n",
      "         628,  631,  634,  637,  640,  641,  643,  646,  649,  651,  653,  655,\n",
      "         656,  659,  664,  665,  666,  668,  671,  674,  677,  678,  680,  684,\n",
      "         685,  689,  692,  693,  696,  709,  712,  714,  730,  732,  734,  735,\n",
      "         737,  741,  742,  743,  745,  755,  760,  762,  769,  771,  774,  776,\n",
      "         777,  779,  780,  787,  788,  789,  790,  795,  797,  799,  801,  804,\n",
      "         805,  808,  809,  811,  815,  817,  821,  822,  823,  828,  830,  832,\n",
      "         833,  836,  838,  841,  842,  843,  844,  846,  848,  851,  852,  854,\n",
      "         858,  859,  860,  861,  867,  868,  870,  874,  875,  878,  880,  888,\n",
      "         889,  895,  896,  899,  909,  921,  927,  932,  933,  934,  937,  938,\n",
      "         939,  942,  943,  952,  953,  955,  957,  958,  960,  964,  965,  970,\n",
      "         971,  976,  977,  985,  986,  992,  995,  998, 1001, 1002, 1004, 1006,\n",
      "        1007, 1009, 1011, 1012, 1013, 1014, 1015, 1019, 1022, 1024, 1025, 1034,\n",
      "        1043, 1044, 1045, 1046, 1048, 1049, 1051, 1054, 1055, 1062, 1063, 1064,\n",
      "        1068, 1072, 1074, 1078, 1079, 1085, 1088, 1090, 1091, 1092, 1093, 1097,\n",
      "        1098, 1099, 1100, 1103, 1104, 1106, 1109, 1110, 1111, 1115, 1118, 1122,\n",
      "        1124, 1126, 1129, 1130, 1134, 1135, 1139, 3610, 3611, 3614, 3617, 3620,\n",
      "        3632, 3635, 3636, 3646, 3650, 3657, 3658, 3659, 3670, 3672, 3674, 3675,\n",
      "        3677, 3682, 3694, 3695, 3706, 3721, 3722, 3723, 3734, 3745, 3746, 3776,\n",
      "        3777, 3783, 3784, 3791, 3793, 3795, 3799, 3804, 3810, 3821, 3829, 3830,\n",
      "        3835, 3847, 3855, 3860, 3864, 3868, 3872, 3874, 3877, 3883, 3891, 3899,\n",
      "        3902, 3906, 3916, 3934, 3940, 3943, 3947, 3949, 3950, 3953, 3954, 3973,\n",
      "        3975, 3978, 3985, 3998, 4002, 4012, 4014, 4019, 4026, 4027, 4032, 4043,\n",
      "        4045, 4054, 4055, 4058, 4068, 4072, 4081, 4099, 4123, 4136, 4157, 4161,\n",
      "        4165, 4167, 4168, 4170, 4171, 4181, 4199, 4200, 4205, 4208, 4211, 4213,\n",
      "        4221, 4235, 4251, 4252, 4260, 4262, 4273, 4288, 4292, 4295, 4301, 4302,\n",
      "        4307, 4311, 4315, 4319, 4329, 4340, 4347, 4350, 4352, 4368, 4371, 4372,\n",
      "        4380, 4388, 4406, 4410, 4414, 4415, 4421, 4434, 4438, 4459, 4461, 4468,\n",
      "        4470, 4480, 4485, 4496, 4499, 4500, 4501, 4511, 4515, 4529, 4530, 4533,\n",
      "        4543, 4548, 4549, 4556, 4579, 4582, 4586, 4587, 4590, 4595, 4596, 4615,\n",
      "        4621, 4625, 4627, 4629, 4635, 4643, 4656, 4665, 4671, 4694, 4695, 4697,\n",
      "        4717, 4722, 4740, 4749, 4752, 4774, 4784, 4788, 4792, 4799, 4802, 4809,\n",
      "        4816, 4821, 4843, 4847, 4858, 4859, 5130, 5145, 5232, 5235, 5244, 5252,\n",
      "        5278, 5286, 5288, 5295, 5304, 5314, 5376, 5393, 5511, 5594, 5625, 5656,\n",
      "        5681, 5729, 5768, 5830, 5890, 5937, 5938, 5939, 5940, 5941, 5942, 5943,\n",
      "        5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951, 5952, 5953, 5954, 5955,\n",
      "        5956, 5957, 5958, 5960, 5961, 5962, 5963, 5965, 5966, 5967, 5968, 5969,\n",
      "        5970, 5971, 5973, 5975, 5976, 5977, 5978, 5979, 5980, 5981, 5982, 5983,\n",
      "        5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991, 5992, 5993, 5994, 5995,\n",
      "        5996, 5997, 5999, 6000, 6001, 6002, 6003, 6004, 6005, 6006, 6007, 6008,\n",
      "        6009, 6010, 6011, 6013, 6014, 6015, 6016, 6017, 6018, 6019, 6020, 6021,\n",
      "        6022, 6023, 6026, 6027, 6028, 6030, 6031, 6032, 6033, 6034, 6035, 6036,\n",
      "        6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044, 6045, 6046, 6047, 6048,\n",
      "        6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056, 6057, 6058, 6060, 6061,\n",
      "        6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069, 6071, 6072, 6073, 6287])\n",
      "relevant_edges =  [('Dataset', 'HAS_PLATFORM', 'Platform'), ('Platform', 'HAS_INSTRUMENT', 'Instrument')]\n",
      "Number of nodes in subgraph for node_id: 173 = 580 | subset = tensor([  42,   69,   71,   76,  113,  152,  173,  532,  533,  537,  541,  547,\n",
      "         549,  557,  559,  560,  561,  563,  566,  567,  568,  571,  575,  581,\n",
      "         582,  586,  588,  592,  595,  596,  597,  603,  604,  606,  608,  609,\n",
      "         612,  615,  617,  619,  623,  625,  627,  628,  631,  634,  637,  640,\n",
      "         641,  643,  646,  649,  651,  653,  655,  656,  659,  664,  665,  666,\n",
      "         668,  671,  674,  677,  678,  680,  684,  685,  689,  692,  693,  696,\n",
      "         709,  712,  714,  730,  732,  734,  735,  737,  741,  742,  743,  745,\n",
      "         755,  760,  762,  769,  771,  774,  776,  777,  779,  780,  787,  788,\n",
      "         789,  790,  795,  797,  799,  801,  804,  805,  808,  809,  811,  815,\n",
      "         817,  821,  822,  823,  828,  830,  832,  833,  836,  838,  841,  842,\n",
      "         843,  844,  846,  848,  851,  852,  854,  858,  859,  860,  861,  867,\n",
      "         868,  870,  874,  875,  878,  880,  888,  889,  895,  896,  899,  909,\n",
      "         921,  927,  932,  933,  934,  937,  938,  939,  942,  943,  952,  953,\n",
      "         955,  957,  958,  960,  964,  965,  970,  971,  976,  977,  985,  986,\n",
      "         992,  995,  998, 1001, 1002, 1004, 1006, 1007, 1009, 1011, 1012, 1013,\n",
      "        1014, 1015, 1019, 1022, 1024, 1025, 1034, 1043, 1044, 1045, 1046, 1048,\n",
      "        1049, 1051, 1054, 1055, 1062, 1063, 1064, 1068, 1072, 1074, 1078, 1079,\n",
      "        1085, 1088, 1090, 1091, 1092, 1093, 1097, 1098, 1099, 1100, 1103, 1104,\n",
      "        1106, 1109, 1110, 1111, 1115, 1118, 1122, 1124, 1126, 1129, 1130, 1134,\n",
      "        1135, 1139, 1739, 1867, 1954, 1968, 2068, 2087, 2159, 2222, 2363, 2444,\n",
      "        2613, 2644, 2695, 2787, 2863, 2955, 3610, 3611, 3614, 3617, 3620, 3632,\n",
      "        3635, 3636, 3650, 3657, 3658, 3659, 3670, 3672, 3674, 3675, 3677, 3682,\n",
      "        3694, 3695, 3706, 3721, 3722, 3723, 3734, 3745, 3776, 3777, 3783, 3784,\n",
      "        3791, 3793, 3795, 3799, 3804, 3810, 3821, 3829, 3830, 3835, 3847, 3855,\n",
      "        3860, 3864, 3872, 3874, 3877, 3883, 3891, 3899, 3902, 3906, 3916, 3934,\n",
      "        3940, 3943, 3947, 3949, 3950, 3953, 3954, 3973, 3975, 3978, 3985, 3998,\n",
      "        4014, 4019, 4026, 4027, 4032, 4043, 4054, 4055, 4058, 4068, 4072, 4081,\n",
      "        4099, 4123, 4136, 4157, 4161, 4165, 4167, 4168, 4170, 4171, 4181, 4199,\n",
      "        4200, 4205, 4208, 4211, 4213, 4221, 4235, 4251, 4252, 4260, 4262, 4273,\n",
      "        4288, 4292, 4295, 4301, 4302, 4307, 4311, 4315, 4319, 4329, 4340, 4347,\n",
      "        4350, 4352, 4368, 4371, 4372, 4380, 4388, 4406, 4410, 4414, 4415, 4421,\n",
      "        4434, 4438, 4459, 4461, 4468, 4470, 4480, 4485, 4496, 4499, 4500, 4501,\n",
      "        4511, 4515, 4529, 4530, 4533, 4543, 4548, 4549, 4556, 4579, 4582, 4586,\n",
      "        4587, 4590, 4595, 4596, 4615, 4621, 4625, 4627, 4629, 4635, 4643, 4656,\n",
      "        4665, 4671, 4694, 4695, 4697, 4717, 4722, 4740, 4749, 4752, 4774, 4784,\n",
      "        4788, 4792, 4799, 4802, 4809, 4816, 4821, 4843, 4847, 4858, 4859, 5130,\n",
      "        5145, 5232, 5235, 5244, 5252, 5278, 5286, 5288, 5295, 5304, 5314, 5376,\n",
      "        5393, 5511, 5625, 5656, 5681, 5729, 5768, 5830, 5890, 5937, 5938, 5939,\n",
      "        5940, 5941, 5942, 5943, 5944, 5945, 5946, 5947, 5948, 5949, 5950, 5951,\n",
      "        5952, 5953, 5954, 5955, 5956, 5957, 5958, 5960, 5961, 5962, 5963, 5965,\n",
      "        5966, 5967, 5968, 5969, 5970, 5971, 5973, 5975, 5976, 5977, 5978, 5979,\n",
      "        5980, 5981, 5982, 5983, 5984, 5985, 5986, 5987, 5988, 5989, 5990, 5991,\n",
      "        5992, 5993, 5994, 5995, 5996, 5997, 5999, 6000, 6001, 6002, 6003, 6004,\n",
      "        6005, 6006, 6007, 6008, 6009, 6010, 6011, 6013, 6014, 6015, 6016, 6017,\n",
      "        6018, 6019, 6020, 6021, 6022, 6023, 6026, 6027, 6028, 6030, 6031, 6032,\n",
      "        6033, 6034, 6035, 6036, 6037, 6038, 6039, 6040, 6041, 6042, 6043, 6044,\n",
      "        6045, 6046, 6047, 6048, 6049, 6050, 6051, 6052, 6053, 6054, 6055, 6056,\n",
      "        6057, 6058, 6060, 6061, 6062, 6063, 6064, 6065, 6066, 6067, 6068, 6069,\n",
      "        6071, 6072, 6073, 6287])\n",
      "\n",
      "Exploration results written to 'graph_exploration.txt'.\n",
      "\n",
      "Selected Node ID: 6 (Type: Project)\n",
      "result =  {'ns': 0, 'title': 'Earth Observing System', 'pageid': 471278, 'size': 29782, 'wordcount': 2019, 'snippet': 'The <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) is a program of NASA comprising a series of artificial satellite missions and scientific instruments in <span class=\"searchmatch\">Earth</span> orbit designed', 'timestamp': '2024-11-01T14:37:17Z'}\n",
      "result =  {'ns': 0, 'title': 'Aqua (satellite)', 'pageid': 830227, 'size': 14120, 'wordcount': 1300, 'snippet': 'component of the <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) preceded by Terra (launched 1999) and followed by Aura (launched 2004). The name &quot;<span class=\"searchmatch\">Aqua</span>&quot; comes from the Latin', 'timestamp': '2024-12-09T23:30:06Z'}\n",
      "result =  {'ns': 0, 'title': 'Terra (satellite)', 'pageid': 470910, 'size': 12209, 'wordcount': 1013, 'snippet': ' It is the flagship of the <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) and the first satellite of the <span class=\"searchmatch\">system</span> which was followed by <span class=\"searchmatch\">Aqua</span> (launched in 2002) and Aura', 'timestamp': '2024-12-09T23:31:38Z'}\n",
      "result =  {'ns': 0, 'title': 'List of Earth observation satellites', 'pageid': 1934667, 'size': 38842, 'wordcount': 1190, 'snippet': 'IAU. 12 August 2018. Retrieved 25 September 2021. &quot;qua <span class=\"searchmatch\">Earth</span>-<span class=\"searchmatch\">observing</span> satellite mission&quot;. <span class=\"searchmatch\">Aqua</span>.nasa.gov. Retrieved 10 August 2017. &quot;The Aura Mission&quot;', 'timestamp': '2024-12-05T03:30:45Z'}\n",
      "result =  {'ns': 0, 'title': 'Aura (satellite)', 'pageid': 827963, 'size': 14249, 'wordcount': 1240, 'snippet': 'It is the third major component of the <span class=\"searchmatch\">Earth</span> <span class=\"searchmatch\">Observing</span> <span class=\"searchmatch\">System</span> (<span class=\"searchmatch\">EOS</span>) following on Terra (launched 1999) and <span class=\"searchmatch\">Aqua</span> (launched 2002). Aura follows on from the', 'timestamp': '2024-07-29T23:05:19Z'}\n",
      "The prompt used for the Wikipedia context = Earth Observing System (EOS), Aqua\n",
      "\n",
      "Wikipedia Context:\n",
      "\n",
      "Result 1:\n",
      "Title: Earth Observing System\n",
      "Description: NASA program involving satellites\n",
      "Summary: The Earth Observing System (EOS) is a program of NASA comprising a series of artificial satellite missions and scientific instruments in Earth orbit designed for long-term global observations of the land surface, biosphere, atmosphere, and oceans. Since the early 1970s, NASA has been developing its Earth Observing System, launching a series of Landsat satellites in the decade. Some of the first included passive microwave imaging in 1972 through the Nimbus 5 satellite. Following the launch of various satellite missions, the conception of the program began in the late 1980s and expanded rapidly through the 1990s. Since the inception of the program, it has continued to develop, including; land, sea, radiation and atmosphere. Collected in a system known as EOSDIS, NASA uses this data in order to study the progression and changes in the biosphere of Earth. The main focus of this data collection surrounds climatic science. The program is the centrepiece of NASA's Earth Science Enterprise.\n",
      "Link: https://en.wikipedia.org/wiki/Earth_Observing_System\n",
      "\n",
      "Result 2:\n",
      "Title: Aqua (satellite)\n",
      "Description: NASA scientific research satellite (2002–Present)\n",
      "Summary: Aqua is a NASA scientific research satellite in orbit around the Earth, studying the precipitation, evaporation, and cycling of water. It is the second major component of the Earth Observing System (EOS) preceded by Terra and followed by Aura.\n",
      "Link: https://en.wikipedia.org/wiki/Aqua_(satellite)\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Aqua_spacecraft_model.png/320px-Aqua_spacecraft_model.png\n",
      "\n",
      "Result 3:\n",
      "Title: Terra (satellite)\n",
      "Description: NASA climate research satellite (1999–Present)\n",
      "Summary: Terra is a multi-national scientific research satellite operated by NASA in a Sun-synchronous orbit around the Earth. It takes simultaneous measurements of Earth's atmosphere, land, and water to understand how Earth is changing and to identify the consequences for life on Earth. It is the flagship of the Earth Observing System (EOS) and the first satellite of the system which was followed by Aqua and Aura. Terra was launched in 1999.\n",
      "Link: https://en.wikipedia.org/wiki/Terra_(satellite)\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/9/92/Terra_spacecraft_model.png/320px-Terra_spacecraft_model.png\n",
      "\n",
      "Result 4:\n",
      "Title: List of Earth observation satellites\n",
      "Description: No Description Available.\n",
      "Summary: Earth observation satellites are Earth-orbiting spacecraft with sensors used to collect imagery and measurements of the surface of the earth. These satellites are used to monitor short-term weather, long-term climate change, natural disasters. Earth observations satellites provide information for research subjects that benefit from looking at Earth’s surface from above. Types of sensors on these satellites include passive and active remote sensors. Sensors on Earth observation satellites often take measurements of emitted energy over some portion of the electromagnetic spectrum.\n",
      "Link: https://en.wikipedia.org/wiki/List_of_Earth_observation_satellites\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/Earth_from_Space.jpg/320px-Earth_from_Space.jpg\n",
      "\n",
      "Result 5:\n",
      "Title: Aura (satellite)\n",
      "Description: NASA Earth observation satellite (2004–Present)\n",
      "Summary: Aura is a multi-national NASA scientific research satellite in orbit around the Earth, studying the Earth's ozone layer, air quality and climate. It is the third major component of the Earth Observing System (EOS) following on Terra and Aqua. Aura follows on from the Upper Atmosphere Research Satellite (UARS). Aura is a joint mission between NASA, the Netherlands, Finland, and the U.K. The Aura spacecraft is healthy and is expected to operate until at least 2023, likely beyond.\n",
      "Link: https://en.wikipedia.org/wiki/Aura_(satellite)\n",
      "Thumbnail: https://upload.wikimedia.org/wikipedia/commons/8/88/Aura_spacecraft.png\n",
      "\n",
      "Selected Node ID: 6 (Type: Project)\n",
      "relevant_edges =  [('Dataset', 'OF_PROJECT', 'Project')]\n",
      "Number of nodes in subgraph for node_id: 6 = 1728 | subset = tensor([   6,   11,   12,  ..., 6346, 6347, 6353])\n",
      "\n",
      "Subgraph for Node ID: 6 (Type: Project)\n",
      "  - ID: 2499, Type: Dataset, Value: MLS/Aura Level 2 Temperature V005 (ML2T) at GES DISC\n",
      "  - ID: 1222, Type: Dataset, Value: MODIS/Aqua Calibrated Radiances 5-Min L1B Swath 250m - NRT\n",
      "  - ID: 2204, Type: Dataset, Value: HIRDLS/Aura Level 3 Chlorine Nitrate (ClONO2) 1deg Lat Zonal Fourier Coefficients V007 (H3ZFCCLONO2) at GES DISC\n",
      "  - ID: 46, Type: DataCenter, Value: Goddard Earth Sciences Data and Information Services Center (formerly Goddard DAAC), Global Change Data Center, Earth Sciences Division, Science and Exploration Directorate, Goddard Space Flight Center, NASA\n",
      "  - ID: 164, Type: DataCenter, Value: N/A\n",
      "  - ID: 33, Type: DataCenter, Value: Cyclone Global Navigation Satellite System Mission, NASA Earth Science System Pathfinder, University of Michigan\n",
      "  - ID: 109, Type: Project, Value: Gravity Recovery and Climate Experiment Data Assimilation for Drought Monitoring\n",
      "  - ID: 190, Type: Project, Value: Alpha Jet Atmospheric eXperiment\n",
      "  - ID: 199, Type: Project, Value: Clouds, Aerosol and Monsoon Processes-Philippines Experiment\n",
      "  - ID: 22, Type: Platform, Value: Digital Elevation Model\n",
      "  - ID: 384, Type: Platform, Value: TanDEM-X\n",
      "  - ID: 100, Type: Platform, Value: \n",
      "  - ID: 208, Type: Instrument, Value: Advanced Microwave Sounding Unit-A\n",
      "  - ID: 189, Type: Instrument, Value: Second Generation Airborne Precipitation Radar\n",
      "  - ID: 227, Type: Instrument, Value: Optical Disdrometer\n",
      "  - ID: 6, Type: ScienceKeyword, Value: GLOBAL POSITIONING SYSTEMS\n",
      "  - ID: 82, Type: ScienceKeyword, Value: DISASTER RESPONSE\n",
      "  - ID: 20, Type: ScienceKeyword, Value: SUBSETTING/SUPERSETTING\n",
      "  - ID: 4724, Type: Publication, Value: Human density impacts Nubian Flapshell turtle survival in Sub-Saharan Africa: Future conservation strategies\n",
      "  - ID: 2682, Type: Publication, Value: Coupling End-Member Mixing Analysis and Isotope Mass Balancing (222-Rn) for Differentiation of Fresh and Recirculated Submarine Groundwater Discharge Into Knysna Estuary, South Africa\n",
      "  - ID: 1699, Type: Publication, Value: The underappreciated role of anthropogenic sources in atmospheric soluble iron flux to the Southern Ocean\n"
     ]
    }
   ],
   "source": [
    "# Assumes `graph_results` contains the 50 search results\n",
    "run_exploration_tool(data, graph_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connections for node 76606 of type 'Publication': [('Publication', 'CITES', 'Publication'), ('Publication', 'HAS_APPLIED_RESEARCH_AREA', 'ScienceKeyword')]\n"
     ]
    }
   ],
   "source": [
    "# See what kinds of edge types exist for a given node\n",
    "def check_node_connections(data, node_type, node_idx):\n",
    "    connections = []\n",
    "    for edge_type in data.edge_types:\n",
    "        edge_index = data[edge_type].edge_index\n",
    "        if node_idx in edge_index[0] or node_idx in edge_index[1]:\n",
    "            connections.append(edge_type)\n",
    "    return connections\n",
    "\n",
    "node_idx = 76606\n",
    "node_type = 'Publication'\n",
    "connections = check_node_connections(data, node_type, node_idx)\n",
    "print(f\"Connections for node {node_idx} of type '{node_type}': {connections}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The Technique Analysis of CO2 in Troposphere using AIRS\n",
      "Abstract: CO2 pollutants (in this study on troposphere layer) and the data used are derived AIRS which  is  The result from the analysis is CO2 profile obtained from AIRS/Aqua L3 Monthly CO2\n",
      "URL: http://sunankalijaga.org/prosiding/index.php/icse/article/view/282\n",
      "\n",
      "Title: Seven years of observations of mid-tropospheric CO2 from the Atmospheric Infrared Sounder\n",
      "Abstract: 1, is a hyperspectral infrared instrument on the EOS Aqua Spacecraft, launched on May 4,   We are finding that the AIRS mid-tropospheric CO 2 is a good indicator of vertical motion in\n",
      "URL: https://www.sciencedirect.com/science/article/pii/S0094576511001457\n",
      "\n",
      "Title: Midtropospheric CO2 concentration retrieval from AIRS observations in the tropics\n",
      "Abstract: Atmospheric Infrared Sounder (AIRS), launched onboard the NASA's Aqua platform in May   sensitive to CO 2 and well covering the mid-to-high troposphere. Also flying onboard Aqua,\n",
      "URL: https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2004GL020141\n",
      "\n",
      "Title: The contribution of AIRS data to the estimation of CO2 sources and sinks\n",
      "Abstract: (AIRS) has been providing the first global maps of CO 2 concentrations in the cloud-free  upper troposphere This paper explores the usefulness of this data for the estimation of CO 2\n",
      "URL: https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2005GL024229\n",
      "\n",
      "Title: AIRS channel selection for CO2 and other trace‐gas retrievals\n",
      "Abstract: Space Administration’s Aqua satellite Atmospheric Infrared Sounder (AIRS) or the European   in the two CO2 bands. Indeed, for the lower troposphere, an increase of CO2 decreases the\n",
      "URL: https://rmets.onlinelibrary.wiley.com/doi/abs/10.1256/qj.02.180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test scholarly PiPy package\n",
    "from scholarly import scholarly\n",
    "\n",
    "def fetch_scholar_info(query):\n",
    "    try:\n",
    "        search_query = scholarly.search_pubs(query)\n",
    "        for i in range(5):  # Get top 3 results\n",
    "            paper = next(search_query)\n",
    "            print(f\"Title: {paper['bib']['title']}\")\n",
    "            print(f\"Abstract: {paper['bib'].get('abstract', 'No abstract available')}\")\n",
    "            print(f\"URL: {paper.get('pub_url', 'No URL available')}\\n\")\n",
    "    except StopIteration:\n",
    "        print(\"No results found on Google Scholar.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data from Google Scholar: {e}\")\n",
    "\n",
    "# Example usage\n",
    "query = \"AIRS Aqua CO2 free troposphere\"\n",
    "fetch_scholar_info(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "#### Known Issues:\n",
    "- Duplicates in the dataset are not manually removed\n",
    "- Using WIKIPEDIA as external resource. NASA APIs are very specific and not generalizeable to the specifc user queries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
